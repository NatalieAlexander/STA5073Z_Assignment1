---
title: "Supervised Learning: Assignment 2"
author: "Natalie Alexander"
date: "2023-04-24"
output:
  pdf_document: default
  html_document: default
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r, libraries, include = FALSE, eval=TRUE}

library(stats)
library(tree)
library(ROCR)
library(rpart)
library(caret)
library(knitr)
library(glmnet)

```


```{css, include = FALSE, eval=FALSE}

/* Whole document: */
body{
  font-family: Courier;
  font-size: 12pt;
}
/* Headers */
{
  font-size: 24pt;
}

```


# **Section A**

## **1. Question A: Splitting the data**
```{r, splitdata, include = FALSE, eval=TRUE}

# read csv data
my_data <- read.csv("heart_failure_clinical_records_dataset.csv", header=TRUE)

# convert categorical variables to factors
my_data$anaemia <- as.factor(my_data$anaemia)
my_data$high_blood_pressure <- as.factor(my_data$high_blood_pressure)
my_data$diabetes <- as.factor(my_data$diabetes)
my_data$sex <- as.factor(my_data$sex)
my_data$smoking <- as.factor(my_data$smoking)
my_data$DEATH_EVENT <- as.factor(my_data$DEATH_EVENT)

# check dimensions of data before missing values are removed
dim(my_data)

# remove missing values
my_data <- na.omit(my_data)

# check dimensions of data after missing values are removed
dim(my_data)

# set seed
set.seed(24)

# randomly split data 
train_ind <- sample(1:nrow(my_data), 0.8*nrow(my_data))
train <- my_data[train_ind,]
test <- my_data[-train_ind, ]

# check train and test set dimensions
dim(train)
dim(test)

```

The csv file was read into r. Subsequently, all categorical variables such as boolean and binary variables were converted to factors. These features include anaemia, high blood pressure, diabetes, sex, smoking and the target variable, death event. These categorical variables are represented by dummy variables, where 1 indicates a "Yes" and 0 indicates a "No". For sex, however, female is denoted as 0 and male as 1. Missing values were then omitted and the dimensions of the data were assessed. There were no missing values present in the data. 299 rows and 13 columns were available before and after removing missing values.

The seed was set to 24 to obtain reproducible results. The full data set was then randomly split into 80% training and 20% test sets. Eighty-percent of row indices from the full data set were used to construct the training set. All row indices in the full data set that were not found in the training set, were used to construct the test set. The dimensions of the training (239 rows by 13 columns) and test sets (60 rows by 13 columns) were assessed to confirm correctness of the split.


\newpage

## **2. Question b: Model fit**

The logistic regression model had an overall better model performance relative to the decision tree when considering performance metrics on the test set (**table 4**). 

```{r logisregresmodel, include = FALSE, eval=TRUE}

options(scipen = 999) #decimals not scientific notation
options(digits=3) #no. of decimal digits

# “vanilla” logistic regression model
model1 <- glm(DEATH_EVENT ~., train, family=binomial)
model1
summary(model1)

# variable coefficients and p-values
model1_coef_p <- data.frame(coefficients = round(model1$coefficients, 3), "odds effect"=round(exp(coef(model1)),3), "p-value" = round(coef(summary(model1))[,'Pr(>|z|)'], 3))

###############################Predictions######################################

# predicted probabilities on test set
pi_hat_2 <- predict(model1, test[,-13], type='response')

# predict target variable on test set 
## threshold of 0.5
y_hat2 <- ifelse(pi_hat_2 >= 0.5, '1', '0')

############################metrics############################################

# accuracy
N <- length(test$DEATH_EVENT)
class_accuracy2 <- 1 - (sum(y_hat2 != test$DEATH_EVENT)/N)

# confusion matrix
conf_mat2 <- table(y_hat2, test$DEATH_EVENT, dnn=c("predicted", "observed"))

# true positive rate or recall = true positives/total positives
recall2 <- conf_mat2[2, 2]/(conf_mat2[1,2]+conf_mat2[2, 2])

# true negative rate or specificity = true negatives/total negatives
specificity2 <- conf_mat2[1,1]/(conf_mat2[1,1]+conf_mat2[2,1])

#positive predictive value = true positives/(true positives+false positives)
ppv2 <- conf_mat2[2, 2]/(conf_mat2[2, 2]+conf_mat2[2, 1])

# F1 score = 2*(PPV*TPR/(PPV+TPR))
f1_2 <- 2*((ppv2*recall2)/(ppv2+recall2))

# ROC AUC
pred_2 <- prediction(pi_hat_2, test$DEATH_EVENT)
perf_2  <- performance(pred_2, 'tpr', 'fpr')
roc_auc2 <- performance(pred_2, measure = 'auc')@y.values[[1]]

# FPR = 1 - TNR
FPR2 <- 1 - specificity2

#FNR = 1 - TPR
FNR2 <- 1 - recall2

# Matthew's correlation coefficient
## MCC = (TP x TN - FP x FN) / sqrt((TP + FP)(TP + FN)(TN + FP)(TN + FN))
MCC2 <- ((specificity2*recall2)-(FNR2*FPR2))/sqrt((recall2+FPR2)*(recall2+FNR2)*(specificity2+FPR2)*(specificity2+FNR2))

```

### **2.1 Logistic regression model**

**i. Logistic regression model construction**

The logistic regression model was constructed using the training set. The model was built on an underlying binomial distribution and all features were included in the model. Predictions and performance metrics were evaluated on the test set. A threshold of 0.5 was used, where an outcome of 1 was determined by a predicted probability equal to or greater than the threshold.  An outcome of 0 was determined by a predicted probability less than the threshold. 

**ii. Logistic regression model outcome**

The logistic regression model took a total of 5 Fisher scoring iterations to complete. The AIC measure was 205.94 and may be compared to multiple models. The model with the smallest AIC (and fewer penalized parameters) will be the preferred model.

We assess the deviance as a measure of goodness of model fit. The null deviance was 306.07 on 238  degrees of freedom and the residual deviance was 179.94 on 226  degrees of freedom. The null deviance is larger than the residual deviance which suggests that the null model does not do well in correctly predicting the response variable. The null model only includes the intercept. We see an improvement in the residual deviance (when the 12 features are included in the model). This suggests that this model makes better predictions than the null model. However, we can further reduce the residual deviance for a better model fit.

**Table 1** shows that the y-intercept of 11.155 is not significant (p-value of 0.066 > alpha of 0.05). We also see that the following features are not significant (p-values > alpha of 0.05): anaemia, creatinine phosphokinase, diabetes, high blood pressure, platelets, serum_sodium, sex and smoking. The model fit may improve by excluding these non-significant features.

Age has a regression coefficient of 0.036	and is statistically significant with a p-value of 0.040 < alpha of 0.05. The positive regression coefficient suggests a positive relationship between age and the likelihood of the target variable = 1, where a 1 unit increase in age is associated with an average change of 0.036 in the log odds of the response variable (death event) taking on a value of 1.

Ejection fraction has a regression coefficient of -0.078	and is statistically significant with a p-value of ~0 < alpha of 0.05. The negative regression coefficient suggests a negative relationship between ejection fraction and the likelihood of the target variable = 1, where a 1 unit increase in ejection fraction is associated with an average change of -0.078 in the log odds of the response variable (death event) taking on a value of 1.

Serum creatinine has a regression coefficient of 0.588	and is statistically significant with a p-value of 0.001 < alpha of 0.05. The positive regression coefficient suggests a positive relationship between serum creatinine and the likelihood of the target variable = 1, where a 1 unit increase in serum creatinine is associated with an average change of 0.588 in the log odds of the response variable (death event) taking on a value of 1.

Time has a regression coefficient of -0.021	and is statistically significant with a p-value of ~0 < alpha of 0.05. The negative regression coefficient suggests a negative relationship between time and the likelihood of the target variable = 1, where a 1 unit increase in time is associated with an average change of -0.021 in the log odds of the response variable (death event) taking on a value of 1.

\newpage

`r kable(model1_coef_p, caption="Logistic regression model outcome")`

**iii. Performance metrics on test set**

The confusion matrix observed in **table 2** was used to calculate the performance metrics on the test set (**table 4**). We see class imbalance, where the majority class is 0. 

The **classification accuracy** of 0.867 in **table 4** is quite high, suggesting good model performance. The model correctly predicts 86.7% of actual observations as true positives and true negatives. However, we see class imbalance in the confusion matrix (**table 2**). Thus, accuracy is biased towards the majority class (0). Predictions of the majority class will have higher accuracy relative to the minority class. 

`r kable(conf_mat2, caption="Confusion matrix for logistic regression")`

The **recall** in **table 4** is moderately high (0.667), which suggests that the model correctly classifies 66.7% of its actual positives as true positives. The model has a moderate cost associated with the false negative rate (0.333). A small false negative rate is ideal, as we do not want to incorrectly predict a death event (1) as no death event (0).

The **specificity** in **table 4** is large (0.933), which suggests that the model correctly classifies 93.3% of its actual negatives as true negatives. A minuscule cost is associated with the false positive rate (0.067). This is ideal, as we do not want to incorrectly predict no death event (0) as a death event (1). 

The **F1-score** of 0.714 in **table 4** is closer to 1 than to 0, so we assume relatively okay model performance. The model has moderate performance when making accurate positive predictions while also reducing false positives and false negatives. However, we can further improve the model to strike a balance between precision and recall.

The **ROC AUC** of 0.883 **table 4** is close to 1, suggesting a good measure of separability. The model does a good job distinguishing between classes. 

Unlike the F1-score, the **MCC metric** in **table 4** is not influenced by which class is labelled as positive. The MCC metric of 0.623 suggests a positive correlation and good agreement between actual observations and predicted y-values. The metric is closer to 1 than to 0, so the classifier is closer to a perfect classifier than a random classifier.

**iv. What is the Matthews Correlation Coefficient (MCC)?**

**MCC = [ (TN X TP) - (FN x FP) ] / [ sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN)) ]**

The MCC is a metric used to measure the differences between actual observed values and their respective predicted values. The MCC is similar to the chi-square statistics of 2 by 2 contingency tables. Thus, the MCC summarizes and considers all information in the confusion matrix. Because this is a correlation metric, the MCC ranges from -1 to +1. An MCC score of -1 suggests perfect negative correlation between predicted and actual y-values. An MCC score of 0 suggests random predictions and no agreement between actual and predicted y-values. An MCC score of +1 suggests perfect positive correlation and the best possible agreement between predicted and actual y-values. In a nutshell, a large MCC score indicates that the model generally returns predictions with good true negative-, true positive-, false negative- and false positive-rates.

**v. What is the motivation for using the MCC?**

The MCC is a balanced and informed measure because it takes into account true positives, true negatives, false positives, and false negatives. As a result, it is not biased towards a particular class as in the case of the F1-score. MCC is thus also suitable for imbalanced data sets. MCC is the preferred metric in instances where the cost of false positives and false negatives are different. 
Another advantage is that the MCC is not affected by the threshold, and so MCC can be used to compare models which use different classification thresholds.

### **2.2 Decision tree model**
```{r treemodel, include = FALSE, eval=TRUE}

options(digits=5)

#classification tree with with default splitting & stopping criteria
tree1 <- tree(DEATH_EVENT ~ ., data = train, split = "deviance")
tree1
summary(tree1)

############################metrics#############################################

# test model on test set
y_hat1 <- predict(tree1, test, type="class")

# confusion matrix
conf_mat1 <- table(y_hat1, test$DEATH_EVENT, dnn=c("predicted", "observed"))

# classification accuracy
class_accuracy1 <- sum(diag(conf_mat1))/nrow(test)  

# true positive rate or recall = true positives/total positives
recall1 <- conf_mat1[2, 2]/(conf_mat1[1,2]+conf_mat1[2, 2])

# true negative rate or specificity = true negatives/total negatives
specificity1 <- conf_mat1[1,1]/(conf_mat1[1,1]+conf_mat1[2,1])

# positive predictive value = true positives/(true positives+false positives)
ppv1 <- conf_mat1[2, 2]/(conf_mat1[2, 2]+conf_mat1[2, 1])

# F1 score = 2*(PPV*TPR/(PPV+TPR))
f1_1 <- 2*((ppv1*recall1)/(ppv1+recall1))

# ROC AUC
pi_hat_1 <- predict(tree1, test[,-13], type='vector')[,2]
pred_1 <- prediction(pi_hat_1, test$DEATH_EVENT)
perf_1  <- performance(pred_1, 'tpr', 'fpr')
roc_auc1 <- performance(pred_1, measure = 'auc')@y.values[[1]]

# FPR = 1 - TNR
FPR1 <- 1 - specificity1

#FNR = 1 - TPR
FNR1 <- 1 - recall1

# Matthew's correlation coefficient
MCC1 <- ((specificity1*recall1)-(FNR1*FPR1))/sqrt((recall1+FPR1)*(recall1+FNR1)*(specificity1+FPR1)*(specificity1+FNR1))

```


**i. Decision tree model construction**

The tree package in r was used to build the decision tree model. Deviance, which is the default splitting criteria was used for tree construction. Default stopping criteria was also used, where mincut = 5, minsize = 10 and mindev = 0.01. **Figure 1** shows that the best tree has 17 terminal nodes. 

**ii. Decision tree model outcome**

The features used in the tree were: time, serum sodium, creatinine phosphokinase, age, platelets, ejection fraction, serum creatinine and sex. The total residual deviance was 88.2 on 222 degrees of freedom. The residual mean deviance was 0.397, which suggests minimal error remaining in the tree after construction. The misclassification error rate was 0.0921. In other words, there were 22 misclassifications out of a total of 239 classifications.

```{r treeplot, echo=FALSE, fig.cap="Decision tree split on deviance"}

par(mfrow = c(1,1), mar = c(1, 1, 1, 1), cex = 0.6)
plot(tree1)
text(tree1, cex = 0.8, pretty=0, font=2, col="darkblue")
legend("topright", title="Key", legend=c("0 : No or 'female' for sex", "1 : Yes or 'male' for sex"), title.col="darkblue")

```


**iii. Performance metrics on test set**

The confusion matrix observed in the **table 3** was used to calculate the performance metrics (**table 4**) on the test set. Again, we see class imbalance, where the majority class is 0. 
 
`r kable(conf_mat1, caption="Confusion matrix for tree model")`

The **classification accuracy** of 0.8 in **table 4** is quite high, suggesting good model performance. The model correctly classifies 80% of the observations as true positives and true negatives. However, we see class imbalance in the confusion matrix (**table 3**). Thus, accuracy is biased towards the majority class (0). Predictions of the majority class will have higher accuracy relative to the minority class. 

The **recall** of 0.533 in **table 4**, suggests that the model only correctly predicts 53.3% of its actual positives as true positives. The model has high cost associated with the false negative rate (0.467). We do not want to incorrectly predict a death event (1) as no death event (0).

**Specificity** in **table 4** is quite high (0.889), which suggests that the model correctly predicts 88.9% of its actual negatives as true negatives. There is a low cost associated with the false positive rate (0.111). 

The **F1-score** of 0.571 in **table 4** is closer to 1 than to 0, so we assume relatively okay model performance. The model has moderate performance when making accurate positive predictions while also reducing false positives and false negatives. We can improve the model to strike a balance between precision and recall.

The **ROC AUC** of 0.724 in **table 4** is close to 1, suggesting a good measure of separability. The model does a good job distinguishing between classes. 

The **MCC metric** of 0.452 in **table 4** suggests positive correlation, but minor agreement between actual observations and predicted values. The metric is closer to 0 than to 1, so the classifier is closer to a random classifier than a perfect classifier.

```{r metrics, include = FALSE, eval=TRUE}

# compare metrics for both models in a single table
metrics <- data.frame("Accuracy"= round(c(class_accuracy1, class_accuracy2), 3), "Recall"= round(c(recall1, recall2), 3), "False negative rate"= round(c(FNR1, FNR2), 3), "Specificity"= round(c(specificity1, specificity2), 3),
                    "False positive rate"= round(c(FPR1, FPR2), 3),
                      "F1-score" = round(c(f1_1, f1_2), 3), "ROC AUC" = round(c(roc_auc1, roc_auc2), 3),
                      "MCC"= round(c(MCC1, MCC2), 3))
    
# row names for table                  
rownames(metrics) <- c("Tree model", "Logistic regression model")

# transform table 
metrics <- t(metrics)

```

`r kable(metrics, caption="Performance metrics for decision tree & logistic regression model")`


\newpage

## **3. Question c: Repetitions**

```{r bootstrap1, include = FALSE, eval=TRUE}

options(digits=3) #set  decimal digits to 3

# set seed
set.seed(24)

# empty tables for performance metrics
tree_metrics <- data.frame("Accuracy" = rep(NA, 100), "Recall" = rep(NA, 100), 
           "Specificity" = rep(NA, 100), "F1-score" = rep(NA, 100),
           "AUC" = rep(NA, 100), "MCC" = rep(NA, 100))

log_regr_metrics <- data.frame("Accuracy" = rep(NA, 100), "Recall" = rep(NA, 100), 
           "Specificity" = rep(NA, 100), "F1-score" = rep(NA, 100),
           "AUC" = rep(NA, 100), "MCC" = rep(NA, 100))

##############################100 iterations####################################

# for loop to generate 100 iterations
for (i in 1:100) {

# randomly split data 
train_ind <- sample(1:nrow(my_data), 0.8*nrow(my_data))
train <- my_data[train_ind,]
test <- my_data[-train_ind, ]

##############################models############################################

# “vanilla” logistic regression model
model1 <- glm(DEATH_EVENT ~., train, family=binomial)

# classification tree with with default splitting & stopping criteria
tree1 <- tree(DEATH_EVENT ~ ., data = train, split = "deviance")

#############################tree model predictions#############################

# test model on test set
y_hat1 <- predict(tree1, test, type="class")

# confusion matrix
conf_mat1 <- table(y_hat1, test$DEATH_EVENT, dnn=c("predicted", "observed"))

##########################tree model metrics####################################

# classification accuracy
class_accuracy1 <- sum(diag(conf_mat1))/nrow(test)
tree_metrics[i, 1] <- class_accuracy1

# true positive rate or recall = true positives/total positives
recall1 <- conf_mat1[2, 2]/(conf_mat1[1,2]+conf_mat1[2, 2])
tree_metrics[i, 2] <- recall1

# true negative rate or specificity = true negatives/total negatives
specificity1 <- conf_mat1[1,1]/(conf_mat1[1,1]+conf_mat1[2,1])
tree_metrics[i, 3] <- specificity1

# positive predictive value = true positives/(true positives+false positives)
ppv1 <- conf_mat1[2, 2]/(conf_mat1[2, 2]+conf_mat1[2, 1])

# F1 score = 2*(PPV*TPR/(PPV+TPR))
f1_1 <- 2*((ppv1*recall1)/(ppv1+recall1))
tree_metrics[i, 4] <- f1_1

# ROC AUC
pi_hat_1 <- predict(tree1, test[,-13], type='vector')[,2]
pred_1 <- prediction(pi_hat_1, test$DEATH_EVENT)
roc_auc1 <- performance(pred_1, measure = 'auc')@y.values[[1]]
tree_metrics[i, 5] <- roc_auc1

# FPR = 1 - TNR
FPR1 <- 1 - specificity1

#FNR = 1 - TPR
FNR1 <- 1 - recall1

# Matthew's correlation coefficient
MCC1 <- ((specificity1*recall1)-(FNR1*FPR1))/sqrt((recall1+FPR1)*(recall1+FNR1)*(specificity1+FPR1)*(specificity1+FNR1))
tree_metrics[i, 6] <- MCC1

##########################Logistic regression model predictions#################

pi_hat_2 <- predict(model1, test[,-13], type='response')
y_hat2 <- ifelse(pi_hat_2 >= 0.5, '1', '0')

# confusion matrix
conf_mat2 <- table(y_hat2, test$DEATH_EVENT, dnn=c("predicted", "observed"))

####################Logistic regression model metrics###########################

# accuracy
N <- length(test$DEATH_EVENT)
class_accuracy2 <- 1 - sum(y_hat2 != test$DEATH_EVENT)/N
log_regr_metrics[i, 1] <- class_accuracy2
  
# true positive rate or recall = true positives/total positives
recall2 <- conf_mat2[2, 2]/(conf_mat2[1,2]+conf_mat2[2, 2])
log_regr_metrics[i, 2] <- recall2

# true negative rate or specificity = true negatives/total negatives
specificity2 <- conf_mat2[1,1]/(conf_mat2[1,1]+conf_mat2[2,1])
log_regr_metrics[i, 3] <- specificity2

#positive predictive value = true positives/(true positives+false positives)
ppv2 <- conf_mat2[2, 2]/(conf_mat2[2, 2]+conf_mat2[2, 1])

# F1 score = 2*(PPV*TPR/(PPV+TPR))
f1_2 <- 2*((ppv2*recall2)/(ppv2+recall2))
log_regr_metrics[i, 4] <- f1_2

# ROC AUC
pred_2 <- prediction(pi_hat_2, test$DEATH_EVENT)
roc_auc2 <- performance(pred_2, measure = 'auc')@y.values[[1]]
log_regr_metrics[i, 5] <- roc_auc2

# FPR = 1 - TNR
FPR2 <- 1 - specificity2

#FNR = 1 - TPR
FNR2 <- 1 - recall2

# Matthew's correlation coefficient
MCC2 <- ((specificity2*recall2)-(FNR2*FPR2))/sqrt((recall2+FPR2)*(recall2+FNR2)*(specificity2+FPR2)*(specificity2+FNR2))
log_regr_metrics[i, 6] <- MCC2
}

##########################Tree mean metrics#####################################

# mean of each metric in the tree model
tree_mean_metrics <- data.frame("Accuracy" = NA, "Recall" = NA, 
           "Specificity" = NA, "F1-score" = NA,
           "ROC AUC" = NA, "MCC" = NA)

i = 1
for (col in tree_metrics) {
  tree_mean_metrics[i] <- mean(col)
  i = i + 1
  
}

#########################Logistic regression model metrics######################

# mean of each metric in the logistic regression model
logregr_mean_metrics <- data.frame("Accuracy" = NA, "Recall" = NA, 
           "Specificity" = NA, "F1-score" = NA,
           "ROC AUC" = NA, "MCC" = NA)

i = 1
for (col in log_regr_metrics) {
  logregr_mean_metrics[i] <- mean(col)
  i = i + 1
  
}

####################mean metrics of tree and logistic regression################

# table of mean metrics
mean_metrics1 <- rbind(tree_mean_metrics, logregr_mean_metrics)
rownames(mean_metrics1) <- c("Tree model", "Logistic regression model")

```

### **3.1 Model comparisons**

**i. Average model comparisons**

**Table 5** below shows the mean metrics for 100 iterations of each model. As before in the single models, we once again see that the logistic regression model had a better model performance relative to the decision tree.

`r kable(mean_metrics1, caption="Average performance metrics for decision tree & logistic regression (100 iterations)")`

**ii. Average model comparison to single models**

The tree models had better average performance metrics for recall, F1-score, ROC AUC and MCC (**table 5**), when compared to the single decision tree in **table 4**. However, the single tree model had better performance metrics for classification accuracy and specificity. 
The logistic regression models only had a better average performance for recall (**table 5**) when compared to the single logistic regression model in **table 4**. However, the single logistic regression model had better performance metrics for classification accuracy, specificity, F1-score, ROC AUC and MCC.  

### **3.2 Decision tree results**

The models had an average **classification accuracy** of 0.788 (**table 5**), suggesting that on average, 78.8% of observations were correctly predicted as true positives and true negatives.

The average **recall** of 0.656 (**table 5**), suggests that the models on average correctly predict 65.6% of its actual positives as true positives. The models had a modest average cost associated with the false negative rate (0.344). 

Average **specificity** is quite high (0.855, **table 5**), which suggests that the models on average correctly predict 85.5% of its actual negatives as true negatives. There is a low average cost associated with the false positive rate (0.145). 

The average **F1-score** of 0.662 (**table 5**) is closer to 1 than to 0, so we assume moderately good average model performance. The models do a good job making correct positive predictions while also reducing false positives and false negatives.

The average **ROC AUC** of 0.829 (**table 5**) is close to 1, suggesting a good measure of separability. On average the classifiers do a good job in distinguishing between classes. 

The average **MCC metric** of 0.526 (**table 5**) suggests positive correlation and moderate agreement between actual observations and predicted values. The metric is closer to 1 than to 0, so on average the classifiers are closer to a perfect classifier than a random classifier.

### **3.3 Logistic regression results**

The models have an average **classification accuracy** of 0.827 (**table 5**), suggesting that on average, 82.7% of observations were correctly predicted as true positives and true negatives.

The average **recall** of 0.674 (**table 5**), suggests that the models on average correctly predict 67.4% of its actual positives as true positives. The models have a modest average cost associated with the false negative rate (0.326). 

Average **specificity** is quite high (0.902, **table 5**), which suggests that the models on average correctly predict 90.2% of its actual negatives as true negatives. There is a low average cost associated with the false positive rate (0.098). 

The average **F1-score** of 0.711 (**table 5**) is closer to 1 than to 0, so we assume relatively good average model performance. The models do a good job making accurate positive predictions while also reducing false positives and false negatives.

The average **ROC AUC** of 0.875 (**table 5**) is close to 1, suggesting a good measure of separability. On average the models do a good job in distinguishing between classes. 

The average **MCC metric** of 0.595 (**table 5**) suggests positive correlation and moderate agreement between actual observations and predicted values. The metric is closer to 1 than to 0, so on average the classifiers are closer to a perfect classifier than a random classifier.

### **3.4 Boxplots**

Looking at the box plots in **figures 2 and 3**, we see variation in the sample metrics across all iterations. This is because each iteration uses a different training set to build the model. Each iteration also uses a different test set to test model performance. This repetition prevents overfitting and ensures that the average model is generalized to new unseen data. In this manner, we are able to identify sources of variability and bias, and estimate uncertainty in the population.

As the number of iterations increase, the distribution of the metrics start to follow the true sampling distribution of the statistic. The distribution then tends towards the normal distribution, where mean = median. In most cases, we observe this approximate normal distribution in the boxplots, where the mean (dashed red line) is equal to the median and the box plots are symmetric in shape. There are slight deviations to the expected symmetrical shape of the normal distribution. These deviations may be due to outliers.

**i. Decision trees' boxplots**

There are slight deviations where mean does not equal to the median (**figure 2**). **Classification accuracy** is slightly right skewed, where the upper tail is longer than the lower tail, and the median is closer to quartile 1 than quartile 3. **Recall** has an almost perfect symmetrical shape where mean = median and tails are of equal length. **Specificity** has a slight left skewed distribution where the median is close to quartile 3 but the tails are symmetrical. The **F1-score** has mean = median, but we see some right skewness because the right tail is longer than the left tail. We also observe an outlier at the lower tail. **ROC AUC** has mean = median, but we see some left skewness, where the lower tail is longer than the upper tail. We also observe an outlier at the bottom tail. For **MCC** we see that mean = median, but there is slight right-skewness, where the upper tail is longer  than the lower tail. We also observe an outlier at the upper tail.

**ii. Logistic regression models' boxplots**

The mean (red dashed line) is equivalent to median in most cases (**figure 3**). **Classification accuracy** has a perfect symmetric shape, but an outlier exists at the bottom tail. **Recall** is almost perfectly symmetrical where mean = median. However, we see an outlier at the bottom tail. **Specificity** is left skewed, where the lower tail is slightly longer than the upper tail and an outlier exists at the bottom tail. The **F1-score** has a slight left skew, where the median is close to quartile 1 and the bottom tail is longer than the upper tail. An outlier also exists at the bottom tail. **ROC AUC** has mean = median, but a slight left skew exists where the lower tail is slightly longer than the upper tail and an outlier exists at the bottom tail. **MCC** is perfectly symmetric, with an outlier at the bottom tail.

```{r, echo=FALSE, fig.cap="Boxplots showing mean performance metrics for decision trees (100 iterations)"}

# title labels
labels <- c("Classification Accuracy", "Recall", "Specificity", "F1-score", "ROC AUC", "MCC")

# colours
cols <- c("navajowhite", "cadetblue2", "coral2", "plum2", "darkseagreen2", "lightpink1")

# boxplots for each metric
par(mfrow=c(2,3)) # set plot dimensions
i = 1
for (col in tree_metrics) {
  boxplot(col, main=labels[i], col=cols[i],
          cex.axis = 0.8, font.axis = 2,
          col.axis = "darkblue", lwd=1.5,
          las=1)
  text(y = round(boxplot.stats(col)$stats, 3), 
       labels = round(boxplot.stats(col)$stats, 3),
       x = 1.35, cex = 0.8)
  abline(h = tree_mean_metrics[i], col = "red",lwd=2, lty = 2)
  box(col="darkblue", lwd=1.5)
  
  i = i + 1
}

```


```{r, echo=FALSE, fig.cap="Boxplots showsing mean performance metrics for logistic regression models (100 iterations)"}

# title labels
labels <- c("Classification Accuracy", "Recall", "Specificity", "F1-score", "ROC AUC", "MCC")

# colours
cols <- c("navajowhite", "cadetblue2", "coral2", "plum2", "darkseagreen2", "lightpink1")

# boxplots for each metric
par(mfrow=c(2,3)) # set plot dimensions
i = 1
for (col in log_regr_metrics) {
  boxplot(col, main=labels[i], col=cols[i],
          cex.axis = 0.8, font.axis = 2,
          col.axis = "darkblue", lwd=1.5,
          las=1)
    text(y = round(boxplot.stats(col)$stats, 3), 
       labels = round(boxplot.stats(col)$stats, 3),
       x = 1.35, cex = 0.8)
   abline(h = logregr_mean_metrics[i], col = "red",lwd=2, lty = 2)
  box(col="darkblue", lwd=1.5)
  i = i + 1
}

```

 
\newpage

## **4. Question d: AUC curves**

**i. ROC curve analysis**

In **figure 4A**, for the decision tree, we see that the best false-positive rate vs true-positive rate at a threshold of 0.5 is 0.12 and 0.65, respectively. However, this co-ordinate does not lie on the ROC AUC curve which suggests that the classifier's classification accuracy and performance is not optimal at a threshold of 0.5.

In figure 4B, for the logistic regression model we see that the best false-positive rate vs true-positive rate at a threshold of 0.5 is 0.03 and 0.6, respectively. 

**ii. Construction of ROC curve for classification tree**

For the classification tree in **figure 4A**, the ROC curve is constructed as follows:

1. First set the seed. In this case the seed is set to set.seed(24)

2. Next read in the data and load the libraries: tree(), ROCR().

3. Categorical variables should use dummy variables. Ensure that all categorical (boolean and binary) predictor variables and the target variable are set to factors.

4. Randomly split the data into (80%) training and (20%) test sets. Use the sample() function to randomly sample 80% of total row indices from the data set. All row indices in the total data set that are not found in the training set, will be used for the test set.

5. Fit a classification tree model on the training set. We use the tree() function to fit the tree model. Set the target and predictor variables in the function. Hyperparameters such as the splitting and stopping criteria must be specified. The default splitting criteria is deviance, however other splitting criteria such as the Gini-index may be used. The default stopping criteria is mincut = 5, minsize = 10 and mindev = 0.01. 

6. Make predictions on the test set. Use the training model to predict the probability of obtaining a positive outcome in the test set. We use the predict() function, where we include the model, the test set's x-variable data and we also specify the format of the outcome. In this case we use type = 'vector', as we are interested in the class probabilities of the outcome variable. We extract the second column because we are interested in the probabilities of predicting a positive outcome.

7. Determine the labels of the outcome. Use a threshold (e.g., default = 0.5) to compare the predicted probabilities to. A predicted probability that is greater than or equal to the threshold will be labelled as the positive class. A predicted probability that is smaller than the threshold will be labelled as the negative class, assuming a binary classifier.

8. Construct a confusion matrix which contains the number of true positives, false positives, true negatives, and false negatives. 
In this step we use the prediction() function of the ROCR package, which requires the class probabilities of the positive class. The second parameter is the test set's actual target variable labels. The prediction() function then stores information such as the predicted probabilities of outcome, the predicted outcome and the actual outcome. We can then calculate the number of true positives, false positives, true negatives, and false negatives. 

9. Using the confusion matrix (in 8), we can calculate the true positive rate and false positive rate of the classifier. Most importantly, we calculate the true positive rate and false positive rate for each possible threshold specified beforehand. The true positive rate is the proportion of cases that are correctly predicted as positive out of the total actual positive cases. The false positive rate is the proportion of cases that are incorrectly classified as positive out of the total number of actual negative cases.
The performance() function is called using the information stored by the prediction() function. We specifically ask for the true positive rates and false positive rates for each threshold.

10. Next we construct the ROC curve by plotting the false positive rate along the x-axis and the true positive rate along y-axis, where each point is associated with a distinct threshold. Join the co-ordinates and calculate the ROC AUC as the area under the curve.

```{r, echo=FALSE, fig.cap="ROC curves. A. Decision tree. B. Logistic regression model"}

par(mfrow=c(2, 2), mar=c(4, 5, 4, 5)) #set figure dimensions

#ROC curve for classification tree model on test set
plot(perf_1, colorize = FALSE, lwd=2, col.lab="darkblue", main="A.")
points(recall1 ~ FPR1, col = 'red', pch = 16)
text(round(FPR1, 2), round(recall1, 2), 
     labels = paste("(", round(FPR1, 2), ";", round(recall1, 2), ")", 
                    sep = ""), pos = 3, cex=0.7, col="red")
box(col="darkblue", lwd=2)

#ROC curve for logistic regression model on test set
plot(perf_2, colorize = FALSE, lwd=2, col.lab="darkblue", main="B.")
points(recall2 ~ FPR2, col = 'red', pch = 16)
text(round(FPR2, 2), round(recall2, 2), 
     labels = paste("(", round(FPR2, 2), ";", round(recall2, 2), ")", 
                    sep = ""), pos = 4, cex=0.7, col="red")
box(col="darkblue", lwd=2)

plot.new()
legend("topleft", title ="Key", title.col="darkblue", legend=c("Optimal FPR, TPR", "at threshold = 0.5"), col=c("red", NA), pch=c(16, NA), cex=0.8)

```


\newpage

## **5. Question e : LASSO regularization and pruning**

### **5.1 Logistic regression: LASSO regularization**

```{r lasso, include = FALSE, eval=TRUE}

# empty matrix to insert logistic regression metrics at each iteration
# cv min model
log_regr_metrics_cv_min <- data.frame("Accuracy" = rep(NA, 100), "Recall" = rep(NA, 100), 
           "Specificity" = rep(NA, 100), "F1-score" = rep(NA, 100),
           "ROC AUC" = rep(NA, 100), "MCC" = rep(NA, 100))

# cv 1se model
log_regr_metrics_cv_1se <- data.frame("Accuracy" = rep(NA, 100), "Recall" = rep(NA, 100), 
           "Specificity" = rep(NA, 100), "F1-score" = rep(NA, 100),
           "ROC AUC" = rep(NA, 100), "MCC" = rep(NA, 100))

#################################LASSO##########################################

# set seed
set.seed(24)

# for loop to generate 100 iterations
for (i in 1:100) {

# randomly split data 
train_ind <- sample(1:nrow(my_data), 0.8*nrow(my_data))
train <- my_data[train_ind,]
test <- my_data[-train_ind,]

###############################Model############################################

# “vanilla” logistic regression model
model1 <- glm(DEATH_EVENT ~., train, family=binomial)

###############################X and Y variables################################

# matrix of x variables
X_train <- model.matrix(DEATH_EVENT ~ .,train)[,-1] #remove y-intercept
X_test <- model.matrix(DEATH_EVENT ~ .,test)[,-1] #remove y-intercept

# vector of y variable
Y_train <- train$DEATH_EVENT 
Y_test <- test$DEATH_EVENT

###########################LASSO regularization#################################

# LASSO model
lasso_model1 <- glmnet(X_train, Y_train, alpha = 1, standardize = TRUE, family = 'binomial')

# apply 10 fold cross validation with lasso penalty
cv_model1 <- cv.glmnet(X_train, Y_train, alpha = 1, nfolds = 10, type.measure = 'class', standardize = TRUE, family = 'binomial') 

# Look at the minimum and 1se lambda values
cv_model1$lambda
cv_model1$lambda.min
cv_model1$lambda.1se 

#############################Predictions on test set###########################

# LASSO with lambda chosen by CV:
pi_hat_cv_min <- predict(lasso_model1, X_test, s = cv_model1$lambda.min, type = 'response')
pi_hat_cv_1se <- predict(lasso_model1, X_test, s = cv_model1$lambda.1se, type = 'response')

# Get predicted Y for test set
Y_hat_cv_min <- ifelse(pi_hat_cv_min >= 0.5, '1', '0')
Y_hat_cv_1se <- ifelse(pi_hat_cv_1se >= 0.5, '1', '0')

# confusion matrix
conf_mat_cv_min <- table(Y_hat_cv_min, test$DEATH_EVENT, dnn=c("predicted", "observed"))
conf_mat_cv_1se <- table(Y_hat_cv_1se, test$DEATH_EVENT, dnn=c("predicted", "observed"))

##################################Metrics#######################################

#accuracy
N <- length(test$DEATH_EVENT)
accuracy_cv_min <- 1 - sum(Y_hat_cv_min != Y_test)/N
log_regr_metrics_cv_min[i,1] <- accuracy_cv_min
accuracy_cv_1se <- 1 - sum(Y_hat_cv_1se != Y_test)/N
log_regr_metrics_cv_1se[i,1] <- accuracy_cv_1se

# true positive rate or recall = true positives/total positives
recall_cv_min <- conf_mat_cv_min[2, 2]/(conf_mat_cv_min[1,2]+conf_mat_cv_min[2, 2])
log_regr_metrics_cv_min[i,2] <- recall_cv_min
recall_cv_1se <- conf_mat_cv_1se[2, 2]/(conf_mat_cv_1se[1,2]+conf_mat_cv_1se[2, 2])
log_regr_metrics_cv_1se[i, 2] <- recall_cv_1se

# true negative rate or specificity = true negatives/total negatives
specificity_cv_min <- conf_mat_cv_min[1,1]/(conf_mat_cv_min[1,1]+conf_mat_cv_min[2,1])
log_regr_metrics_cv_min[i, 3] <- specificity_cv_min
specificity_cv_1se <- conf_mat_cv_1se[1,1]/(conf_mat_cv_1se[1,1]+conf_mat_cv_1se[2,1])
log_regr_metrics_cv_1se[i, 3] <- specificity_cv_1se

#positive predictive value = true positives/(true positives+false positives)
ppv_cv_min <- conf_mat_cv_min[2, 2]/(conf_mat_cv_min[2, 2]+conf_mat_cv_min[2, 1])
ppv_cv_1se <- conf_mat_cv_1se[2, 2]/(conf_mat_cv_1se[2, 2]+conf_mat_cv_1se[2, 1])

# F1 score = 2*(PPV*TPR/(PPV+TPR))
f1_cv_min <- 2*((ppv_cv_min*recall_cv_min)/(ppv_cv_min+recall_cv_min))
log_regr_metrics_cv_min[i, 4] <- f1_cv_min
f1_cv_1se <- 2*((ppv_cv_1se*recall_cv_1se)/(ppv_cv_1se+recall_cv_1se))
log_regr_metrics_cv_1se[i, 4] <- f1_cv_1se

# ROC AUC
pred_cv_min <- prediction(pi_hat_cv_min, test$DEATH_EVENT)
roc_auc_cv_min <- performance(pred_cv_min, measure = 'auc')@y.values[[1]]
log_regr_metrics_cv_min[i, 5] <- roc_auc_cv_min

pred_cv_1se <- prediction(pi_hat_cv_1se, test$DEATH_EVENT)
roc_auc_cv_1se <- performance(pred_cv_1se, measure = 'auc')@y.values[[1]]
log_regr_metrics_cv_1se[i, 5] <- roc_auc_cv_1se

# FPR = 1 - TNR
FPR_cv_min <- 1 - specificity_cv_min
FPR_cv_1se <- 1 - specificity_cv_1se

#FNR = 1 - TPR
FNR_cv_min <- 1 - recall_cv_min
FNR_cv_1se <- 1 - recall_cv_1se

# Matthew's correlation coefficient
# MCC = (TN*TP-FN*FP)/sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))
MCC_cv_min <- ((specificity_cv_min*recall_cv_min)-(FNR_cv_min*FPR_cv_min))/sqrt((recall_cv_min+FPR_cv_min)*(recall_cv_min+FNR_cv_min)*(specificity_cv_min+FPR_cv_min)*(specificity_cv_min+FNR_cv_min))
log_regr_metrics_cv_min[i, 6] <- MCC_cv_min

MCC_cv_1se <- ((specificity_cv_1se*recall_cv_1se)-(FNR_cv_1se*FPR_cv_1se))/sqrt((recall_cv_1se+FPR_cv_1se)*(recall_cv_1se+FNR_cv_1se)*(specificity_cv_1se+FPR_cv_1se)*(specificity_cv_1se+FNR_cv_1se))
log_regr_metrics_cv_1se[i, 6] <- MCC_cv_1se
}


###########################MEAN LOGISTIC REGRESSION METRICS#####################

# mean of each metric in the logistic regression model
## min lambda
logregr_mean_lasso_metrics_min <- data.frame("Accuracy" = NA, "Recall" = NA, 
           "Specificity" = NA, "F1-score" = NA,
           "ROC AUC" = NA, "MCC" = NA)

i = 1
for (col in log_regr_metrics_cv_min) {
  logregr_mean_lasso_metrics_min[i] <- mean(col)
  i = i + 1
  
}

# mean of each metric in the logistic regression model
# 1se lambda
logregr_mean_lasso_metrics_1se <- data.frame("Accuracy" = NA, "Recall" = NA, 
           "Specificity" = NA, "F1-score" = NA,
           "ROC AUC" = NA, "MCC" = NA)

i = 1
for (col in log_regr_metrics_cv_1se) {
  logregr_mean_lasso_metrics_1se[i] <- mean(col)
  i = i + 1
  
}

################################################################################

```

The x-variables were standardized before model construction. The scaling of the x-variables, ensure model convergence and interpretability. 10 fold cross-validation was used in the LASSO penalized models. Cross-validation generated both lambda 1se and lambda min values which were used in separate models. The 1se model is preferable when prediction is the aim, while the min model is preferable when interpretation of variable relationships are required. However, it is worth while investigating both of these models. Finally, a threshold of 0.5 was used to make class predictions based on test set.

### **5.2 Decision tree: pruning and Gini-index as splitting criteria**

```{r pruning, include = FALSE, eval=TRUE}

# empty matrix to insert tree metrics at each iteration
tree_metrics <- data.frame("Accuracy" = rep(NA, 100), "Recall" = rep(NA, 100), 
           "Specificity" = rep(NA, 100), "F1-score" = rep(NA, 100),
           "ROC AUC" = rep(NA, 100), "MCC" = rep(NA, 100))

###########################################################################

# set seed
set.seed(24)

# for loop to generate 100 iterations
for (i in 1:100) {

# randomly split data 
train_ind <- sample(1:nrow(my_data), 0.8*nrow(my_data))
train <- my_data[train_ind,]
test <- my_data[-train_ind,]

########################Decision tree Model#####################################

# classification tree with with different splitting criteria
tree2 <- tree(DEATH_EVENT ~ ., data = train, split = "gini")

############################Cross Validation####################################

# Pruning
cv_tree2 <- cv.tree(tree2, FUN = prune.misclass) 

# set inf to 0
cv_tree2$k[1] <- 0

# tree size of min cv-error
min_cv <- cv_tree2$size[which.min(cv_tree2$dev)] 

# Pruned tree
tree2_pruned <- prune.misclass(tree2, best = min_cv)

#########################Predictions on test set################################

# Predictions on the test set
Y_hat <- predict(tree2_pruned, test[,-13], type = 'class') 

##################################Metrics#######################################

#accuracy
N <- length(test$DEATH_EVENT)
accuracy_tree2 <- 1 - sum(Y_hat != test$DEATH_EVENT)/N
tree_metrics[i,1] <- accuracy_tree2

# confusion matrix
conf_mat <- table(Y_hat, test$DEATH_EVENT, dnn=c("predicted", "observed"))

# true positive rate or recall = true positives/total positives
recall <- conf_mat[2, 2]/(conf_mat[1,2]+conf_mat[2, 2])
tree_metrics[i,2] <- recall

# true negative rate or specificity = true negatives/total negatives
specificity <- conf_mat[1,1]/(conf_mat[1,1]+conf_mat[2,1])
tree_metrics[i, 3] <- specificity

#positive predictive value = true positives/(true positives+false positives)
ppv <- conf_mat[2, 2]/(conf_mat[2, 2]+conf_mat[2, 1])
tree_metrics[i, 4] <- ppv

# F1 score = 2*(PPV*TPR/(PPV+TPR))
f1 <- 2*((ppv*recall)/(ppv+recall))

# ROC AUC
pi_hat <- predict(tree2, test[,-13], type='vector')[,2]
pred <- prediction(pi_hat, test$DEATH_EVENT)
roc_auc <- performance(pred, measure = 'auc')@y.values[[1]]
tree_metrics[i, 5] <- roc_auc

# FPR = 1 - TNR
FPR <- 1 - specificity

#FNR = 1 - TPR
FNR <- 1 - recall

# Matthew's correlation coefficient
# MCC = (TN*TP-FN*FP)/sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))
MCC <- ((specificity*recall)-(FNR*FPR))/sqrt((recall+FPR)*(recall+FNR)*(specificity+FPR)*(specificity+FNR))
tree_metrics[i, 6] <- MCC
}

##############################MEAN TREE METRICS#################################

# mean of each metric in the tree model
pruned_tree_mean_metrics <- data.frame("Accuracy" = NA, "Recall" = NA,
           "Specificity" = NA, "F1-score" = NA,
           "ROC AUC" = NA, "MCC" = NA)

i = 1
for (col in tree_metrics) {
  pruned_tree_mean_metrics[i] <- mean(col)
  i = i + 1

}

```

The Gini-index was used as the splitting criteria for the decision trees. Default stopping criteria were selected as mentioned in the single decision tree **2.2 (i)**. 10-fold cross-validation was applied to be consistent the LASSO regression models. The misclassification error rate was used as the pruning criteria. In each iteration, the tree is pruned to the tree size (and corresponding number of terminal nodes) with the minimum cross-validation misclassification error rate.

```{r combinedmetrics,include = FALSE, eval=TRUE}

# table of mean metrics
mean_metrics2 <- rbind(tree_mean_metrics, pruned_tree_mean_metrics, logregr_mean_metrics, logregr_mean_lasso_metrics_min, logregr_mean_lasso_metrics_1se)
rownames(mean_metrics2) <- c("Tree models", "Pruned tree models", "Logistic regression models", "LASSO lambda min models", "LASSO lambda 1se models")

```


### **5.3 Summary of model outcomes**

Upon assessment of the average performance metrics in **table 6**, the pruned tree model (which uses the Gini-index as a splitting criteria) outperformed the original tree models (which uses deviance as a spitting criteria). We see that the original logistic regression model outperformed the LASSO models for most metrics, except accuracy and specificity. The inflated specificity is due to the class imbalance, where the majority class is the negative class (0). Assessment of the most reliable metric, the MCC, suggests that the original logistic regression model is the best performing model among all models. To summarize, it seems as if the original logistic regression model generalizes the best to unseen, new data. 

`r kable(mean_metrics2, caption="Average performance metrics across all models (100 iterations)")`

### **5.4 Model comparison**

**i. Accuracy**

In **table 6** we see that the pruned tree model has an improved accuracy of 0.825 relative to the original tree model with an accuracy of 0.788. In fact, the original tree model performed the worst among all models in terms of accuracy. However, both of these decision trees had a larger accuracy rate relative to the 0.737 observed by the decision tree model of Chicco and Jurman (2020).
The LASSO lambda min model had a larger accuracy (0.829) relative to the original logistic regression (0.827) and LASSO lambda 1se (0.822) models. In fact, the LASSO lambda min model performed the best among all other models in terms of accuracy. The LASSO and logistic regression models had a larger accuracy rate relative to the 0.730 observed by the linear regression model of Chicco and Jurman (2020).
Overall, the various models in **table 6** outperformed the models by Chicco and Jurman (2020), when considering accuracy. 

**ii. Recall**

The pruned tree model had a slight improvement in performance with a recall of 0.659 relative to the original tree model with a recall of 0.656 (**table 6**). These models had a better performance relative to the decision tree of Chicco and Jurman (2020) with a recall of 0.532.
The logistic regression model had the best recall value of 0.674 relative to the LASSO models (and all other models). The LASSO lambda min model had a larger recall of 0.606 relative to the LASSO lambda 1se model with a recall of 0.532. The LASSO and logistic regression models had a larger recall relative to the linear regression model of Chicco and Jurman (2020) which had a recall rate of 0.394.
Overall, the various models in **table 6** outperformed the models by Chicco and Jurman (2020), when considering recall.

**iii. Specificity**

The pruned tree model had a specificity of 0.905, which outperformed the original tree model with a specificity of 0.855 (**table 6**). These models outperformed the decision tree of Chicco and Jurman (2020) with a specificity of 0.831. 
The LASSO lamda 1se model had the best specificity (0.962) relative to both the original logistic regression model (0.902) and the LASSO lambda min model (0.936). The LASSO and logistic regression models performed better than the linear regression model of Chicco and Jurman (2020) (0.892).

**iv. F1-score**

The pruned tree model had an improved performance in terms of F1-score (0.777) compared to the original tree model with an F1-score of 0.662 (**table 6**). These models performed better than the decision tree of Chicco and Jurman (2020) with an F1-score of 0.554.
The original logistic regression model performed better (F1-score=0.711) than both LASSO models. We do not see an improvement in the LASSO lambda 1se (F1-score=0.652) model relative to the LASSO lambda min model (F1-score=0.690). Overall, the logistic regression and LASSO models had a larger F1-score relative to the linear regression model of Chicco and Jurman (2020) (F1-score = 0.475).
Overall, the various models in **table 6** outperformed the models by Chicco and Jurman (2020), when considering the F1-score.

**v. ROC AUC**

We see an improvement in the ROC AUC for the pruned tree model (ROC AUC = 0.853), relative to the original tree model (ROC AUC = 0.829) (**table 6**). These models performed better than the decision tree of Chicco and Jurman (2020) with a ROC AUC of 0.681.
The original logistic regression model (ROC AUC = 0.875) performed better than the LASSO models in terms of ROC AUC. The performance deteriorates in the LASSO lambda 1se model (ROC AUC = 0.861) relative to the LASSO lambda min model (ROC AUC = 0.869). These models performed better than the linear regression model of Chicco and Jurman (2020) with a ROC AUC of 0.643.
Overall, the various models in **table 6** outperformed the models by Chicco and Jurman (2020), when considering ROC AUC.

**vi. MCC**

We see improvement in the MCC metric of the pruned tree model (MCC = +0.588) relative to the original tree model (MCC = +0.526) (**table 6**). These models performed better than the decision tree of Chicco and Jurman (2020) with an MCC of +0.376. However, we still observe positive correlation in both instances.
The original logistic regression model performed better than the LASSO models, where the logistic regression model had an MCC of +0.595. The LASSO lambda min model (MCC = +0.578) performed better than the LASSO lambda 1se model (MCC = +0.548). The logistic regression and LASSO models performed better than the linear regression model of Chicco and Jurman (2020) (MCC = +0.332).
Overall, the various models in **table 6** outperformed the models by Chicco and Jurman (2020), when considering the MCC metric.


\newpage

# **Section B**

## **1. Question A : Distribution**

```{r seouldata, include = FALSE, eval=TRUE}

# read csv data
## ensure categorical variables are factors
seoul_data <- read.csv("SeoulBikeData.csv", header=TRUE, stringsAsFactors = TRUE, fileEncoding="windows-1252") #file encoding

# Change column names to show special symbols
colnames(seoul_data) <- c("Date", "Rented Bike Count", "Hour", "Temperature(degrees Celsius)", "Humidity(%)", "Wind speed(m/s)", "Visibility(10m)", "Dew point temperature(degrees Celsius)", "Solar Radiation(MJ/m2)", "Rainfall(mm)", "Snowfall(cm)", "Seasons", "Holiday", "Functioning Day")

# check column data types
# check that categorical variables are factors
str(seoul_data)

# check dimensions of data before missing values are removed
dim(seoul_data)

# remove missing values
seoul_data <- na.omit(seoul_data)

# check dimensions of data after missing values are removed
dim(seoul_data) #no change in dimensions

```

**i. Preparing the data**

The csv file was read into r. The correct file encoding was set to windows-1252 and special characters were included in the column names. All categorical variables were converted to factors. These categorical variables include: Date, Seasons, Holiday, Functioning Day. The dimensions of the data were checked before and after removing missing values. There were no missing values to remove. As a result the dimensions of the data remained the same before and after removing missing values: 8760 rows by 14 columns. 

**ii. Histogram of target variable (Rented Bike Count)**

The histogram in **figure 5A** illustrates the frequency of the rented bike counts, where the rented bike count is grouped into bins. The x-axis shows the bike counts rented per hour and the y-axis shows the total number of hours that had a specific bike count. We see that there are more occurrences of fewer bike rentals per hour. We also see a trend where the frequency of total hours decrease as the frequency of bike rentals increase. In other words, it is less common that a large amount of bikes are rented per hour. 

The density plot in **figure 5B** shows that the distribution of the rented bike count is skewed to the right. A right skewed distribution infers that the mean may be greater than the median or the right tail is longer than the left tail.

```{r, echo=FALSE, fig.cap=" (a) Histogram and (b) density plot of the target variable"}

par(mfrow=c(1, 2), mar=c(4.1, 4, 4.1, 4))

# Histogram
hist(seoul_data$`Rented Bike Count`, col="darkseagreen2", main="A",
     xlab="Rented bike count (per hour)", ylab="Frequency (total hours)", ylim=c(0, 2500))

# denisty plot
hist(seoul_data$`Rented Bike Count`, col="darkseagreen2", main="B",
     xlab="Rented bike count (per hour)", ylab="Density", freq=FALSE)
lines(density(seoul_data$`Rented Bike Count`), col="red", lwd=3)
legend("topright", title="Key", title.col="darkgreen", legend="Skewness", lwd=3, col= "red", lty=1, cex=0.7)

```


**iii. Target variable distribution across all features**

Looking at **figure 6**, we see the distribution of the target variable across the different features. We do not see a clear pattern for date. We see that rented bike count is skewed to the left for the following features: hour, temperature, visibility, and dew point temperature. We see that rented bike count is skewed to the right for the following features: wind speed, rainfall, and snowfall. We see clustering at the centre and no apparent pattern for humidity vs rented bike count. Solar radiation vs rented bike count has lots of noise and so the pattern is not clear. We see that rented bike count is skewed to the right for the seasons (autumn, spring and summer), holiday and functioning day (yes). 

The categorical variables seasons, holiday and functioning day stand out as good predictors because there is variation in the rented bike count between each of the classes of a respective feature. In addition, temperature, dew point temperature, rainfall and snowfall seem to be good predictors of the rented bike count. The target variable is strongly skewed to the left for temperature and dew point temperature, suggesting a positive linear relationship with the target variable. The target variable is strongly skewed to the right for rainfall and snowfall. Rainfall and snowfall seem to have a non-linear relationship with the target variable. 


```{r , echo=FALSE}

par(mfrow=c(3, 3), mar=c(4, 4, 4, 4))

# Pairwise plot of columns 1:10
plot(`Rented Bike Count` ~ ., seoul_data[,1:10], col = "coral2", cex.axis=0.7, pch=16, cex=0.3)

```


```{r , echo=FALSE, fig.cap="Distribution of target variable across features", fig.pos="bottom"}

par(mfrow=c(2, 2), mar=c(4, 4, 4, 4))

# Pairwise plot of columns 2 and 11:14
plot(`Rented Bike Count` ~ ., seoul_data[,c(2, 11:14)], col = "coral2", cex.axis=0.7, pch=16, cex=0.5)

```


\newpage

## **2. Question B: Models**

### **2.1 Splitting the data**

The data was randomly split into an 80% training and 20% test set. The training set was used to build models, while the test set was used to evaluate model performance. 

```{r, sdrmse, include = FALSE, eval=TRUE}

# empty data frame for test RMSE
rmse_seoul <- data.frame("Random forest" = NA, "GBM" = NA, "XGBoost" = NA, "Test sd" = NA)

```


### **2.2 Random Forest Model**

```{r, seoulmodels, include = FALSE, eval=TRUE}

##############################Random Forest#####################################

# Change column names to be compatible with random forest function
colnames(seoul_data) <- c("Date", "Rented_Bike_Count", "Hour", "Temperature", "Humidity", "Wind_speed", "Visibility", "Dew_point_temperature", "Solar_Radiation", "Rainfall", "Snowfall", "Seasons", "Holiday", "Functioning_Day")

# set seed
set.seed(24)

# split data set into train and test set
ind <- sample(1:nrow(seoul_data), 0.8*nrow(seoul_data))
train_seoul <- seoul_data[ind,]
test_seoul <- seoul_data[-ind, ]

# remove date column since this has more than 53 levels which is not compatible with random forest function
train_seoul <- train_seoul[,-1]
test_seoul <- test_seoul[, -1]

# standard deviation
seoul_sd <- round(sd(test_seoul$Rented_Bike_Count), 3)
rmse_seoul[,4] <- seoul_sd
#############################Random Forest######################################

# load library
library(randomForest)

## random forest model on training set
# set.seed(24)
# rf <- randomForest(Rented_Bike_Count ~ ., data = train_seoul,
#                            ntree = 500, 
#                            importance = TRUE, 
#                            na.action = na.exclude,
#                            do.trace = 25)
# save(rf, file = 'rf_assignment2.Rdata')
load('rf_assignment2.Rdata')
rf
################################Predictions#####################################

# Predictions
rf_pred <- predict(rf, newdata = test_seoul[,-1]) #remove target variable column

# Prediction accuracy
## RMSE = sqrt(mean((actual - predicted)^2))
rf_rmse <- sqrt(mean((test_seoul[,1] - rf_pred)^2)) #remove target variable column
rmse_seoul[,1] <- round(rf_rmse, 3)


```

**i. Random Forest model construction**

The randomForest package was used to construct the random forest model. The model was constructed using the rented bike count as the target variable. All features except date were included in the model. Date was excluded, since randomForest is not compatible with variables with more than 53 levels. Furthermore, we did not see a relationship between the target variable and date in the scatter plot (**figure 6**). The number of trees to grow (ntree), was set to 500 which is the default value in the randomForest package. 

**ii. Model outcome**

There were 4 variables tried at each split. The mean of the squared residuals was 52786. The R-squared value (0.874) suggests that the random forest model explains 87.4% of the variation observed in the target variable, rented bike count. 

**iii. Performance on test set**

The root mean-squared error (RMSE) on the test set is shown in **table 7**. The test RMSE for the random forest model is 221 which is smaller than the standard deviation of the test set's target variable (test sd = 630). For this reason, we assume the random forest model is doing a good job in making accurate predictions, specifically on new unseen data. The random forest model generalizes well to new unseen data. In addition, the model captures most of the variability observed in the data. 

### **2.2 Gradient Boosted trees (GBM)**

```{r gbm, include = FALSE, eval=TRUE}

#load libraries
library(gbm)
library(caret) 
library(doParallel)

# use all cores
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# grid of hyperparams
gbm_grid <- expand.grid(n.trees = c(100, 1000, 2000),
                        interaction.depth = c(1, 3, 6),
                        shrinkage = c(0.01, 0.005, 0.001),
                        n.minobsinnode = c(1, 5, 10))


# 10 fold cross validation
ctrl <- trainControl(method = "cv", number = 10)

## train model on grid
# set.seed(24)
# gbm_gridsearch <- train(Rented_Bike_Count ~ ., data = train_seoul, 
#                         method = 'gbm', 
#                         distribution = 'gaussian', #regression problem
#                         trControl = ctrl, 
#                         verbose = F, 
#                         bag.fraction = 1,  #straight forward gbm without bagging
#                         tuneGrid = gbm_grid)
# 
stopCluster(cl) #exit cluster

# save(gbm_gridsearch, file = 'gbm_assignment2.Rdata')
load('gbm_assignment2.Rdata')

# best hyperparams
gbm_gridsearch$bestTune

# hyperparams of all models
gbm_gridsearch$results

###################################Predictions#################################

# y-hats
gbm_pred <- predict(gbm_gridsearch, newdata = test_seoul[,-1]) #remove target variable column

# Prediction accuracy
## RMSE = sqrt(mean((actual - predicted)^2))
gbm_rmse <- sqrt(mean((test_seoul[,1] - gbm_pred)^2)) #remove target variable column
rmse_seoul[,2] <- round(gbm_rmse, 3)

```

**i. Gradient boosted tree (GBM) model construction**

The gbm and caret package was used to build the gbm model. All features except date were included in the model, to be consistent with the random forest model. The gaussian distribution was selected since the target variable is quantitative. Hyperparameter tuning was performed by means of a grid search, where (1) interaction depth = 1, 3, 6, (2) number of trees = 100, 1000, 2000, (3) shrinkage =  0.001, 0.005, 0.01, and (4) n.minobsinnode = 1, 5, 10. Ten-fold cross validation was used to estimate the lowest cross-validation RMSE.

**ii. Model outcome** 

The best model had the minimum train RMSE of 235 and the largest R-squared value of 0.869. The R-squared value is less than that obtained from the random forest (0.874). The optimal model used the following hyperparameters: (1) ntrees = 2000, (2) interaction depth = 6, (3) shrinkage = 0.01, and (4) n.minobsinnode = 5.

**iii. Performance on test set**

The test RMSE is 226 (**table 7**) which is larger than the test RMSE for the random forest model (test RMSE = 221). However, the test RMSE of the GBM is still smaller than the standard deviation of the test set's target variable (test sd = 630). This suggests that the GBM model is doing a relatively good job in making accurate predictions, specifically on new unseen data. The GBM model also generalizes well to new unseen data. Again, we see that the model captures most of the variability observed in the data. 


### **2.3 Extreme gradient boosting (XGBoost)**

```{r xgboost, include = FALSE, eval=TRUE}

library(xgboost) #load library

# use all cores
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# grid of hyperparams
xgb_gridsearch <- expand.grid(nrounds = c(1000, 10000, 50000),
                        max_depth = c(1, 6),     
                        eta = c(0.1, 0.01), 
                        gamma = 0.001,
                        colsample_bytree = 1,
                        min_child_weight = 1,     
                        subsample = 1)

# 10-fold cross validation
ctrl <-  trainControl(method = 'cv', number = 5, verboseIter = T)

# train xgboost model
# set.seed(24)
# system.time(xgb <- train(Rented_Bike_Count ~ ., data = train_seoul,
#                      method = 'xgbTree',
#                      trControl = ctrl,
#                      verbose = F,
#                      tuneGrid = xgb_gridsearch,
#                      nthread = 1)
# )
stopCluster(cl) #exit cluster

#save(xgb, file = 'xgb_assignment2.Rdata')
load('xgb_assignment2.Rdata')

# best xgboost model
xgb
xgb$results
xgb$bestTune

###################################Predictions#################################

# y-hats
xgb_pred <- predict(xgb, newdata = test_seoul[,-1]) #remove target variable column

# Prediction accuracy
## RMSE = sqrt(mean((actual - predicted)^2))
xgb_rmse <- sqrt(mean((test_seoul[,1] - xgb_pred)^2)) #remove target variable column
rmse_seoul[,3] <- round(xgb_rmse, 3)

```

**i. Extreme gradient boosting (XGBoost) model construction **

The xgboost and caret package was used to build the XGBoost model. All features except date were included in the model, to be consistent with the random forest and GBM models. Hyperparameter tuning was performed by means of a grid search, where (1) nrounds = 1000, 10000, 50000, (2) max depth = 1, 6, (3) eta = 0.1, 0.01, (4) gamma = 0.001, (5) colsample bytree = 1, (6) min child weight = 1,  and (7) subsample = 1 . Five-fold cross validation was used to estimate the lowest cross-validation RMSE. The reason for five-fold cross-validation is to reduce computational cost. Ten-fold cross-validation was not computationally feasible. 
                        
**ii. Model outcome**

The best model had a minimum train RMSE of 232 and the largest R-squared value of 0.872. The R-squared value of 0.872 is larger that the GBM model (R-squared = 0.869), but marginally smaller than that of the random forest (R-squared = 0.874). The following hyperparameters were used in the model: (1) nrounds = 1000, (2) max depth = 4, (3) eta = 0.01, (4) gamma = 0.001, (5) colsample by tree = 1, (6) min child weight = 1, and (7) subsample = 1.

**iii. Performance on test set**

The test RMSE is 216 (**table 7**) which is smaller than the test RMSE for the random forest model (test RMSE = 221) and the GBM model (test RSME=226). The test RMSE of the XGBoost model is also smaller than the standard deviation of the test set's target variable (test sd = 630). This suggests that the XGBoost model is doing the best at making accurate predictions, specifically on new unseen data. The XGBoost model is the most generalized to new, unseen data (relative to all other the models). This model captures most of the variability observed in the data, relative to all other models.

`r kable(rmse_seoul, caption="Test RMSE and standard deviation")`

\newpage

## **3. Question c : Feature analysis**

### **3.1 Variable importance**

**Figures 7, 8, and 9** show the variable importance of the random forest, GBM and XGBoost models, respectively. The variable importance plots use the reduction in the node impurity, namely the Gini-index as a criteria for variable importance. For the GBM and XGBoost models, the categorical variables are split into their respective classes.

**i. Random Forest**

The variable importance plot in **figure 7** suggests that the random forest model had hour, temperature, seasons and humidity as the top four important variables. 

**ii. Gradient boosted tree **

The GBM model in **figure 8** has hour, temperature, functioning day (Yes) and humidity as the top four important variables. As in the case of the random forest model, hour and temperature are still the top two most important variables. Humidity is still the fourth most important variable. However,  seasons are replaced with functioning day (yes) as the third most important variable. Seasons (Winter) is now the sixth most important variable. Seasons (spring)  and seasons(Summer) are the ninth and thirteenth most important variables, respectively.

**iii. Extreme gradient boosting**

The XGBoost model in **figure 9** has temperature, hour, functioning day (Yes), and solar radiation as the top four most important variables. Similar to the random forest and GBM model, temperature, and hour are still the top two most important variables. However, hour is now the second most important variable and temperature is the overall most important variable. In parallel to the GBM model, functioning day (Yes) is also the third most important variable. However, solar radiation has moved up from fifth position in the random forest and GBM models to the fourth position in the XGBoost model. Humidity is now the fifth most important variable. Seasons (Winter), seasons (Spring) and seasons (Summer) are now at positions 8, 9, and 14, respectively. 


```{r RFvarimp, echo=FALSE, fig.cap="Variable importance of random forest"}

par(mar=c(6, 8, 6, 8))

par(mar=c(4,4,4,4))
varImpPlot(rf, type = 2, pch=16, main="") #variable importance plot of random forest

```


```{r GBMvarimp, echo=FALSE, fig.cap="Variable importance of GBM"}

# variable imprtance of GBM
gbm_varimp <- varImp(gbm_gridsearch, type="Gini", algorithm="gbm")
plot(gbm_varimp, col="darkgreen", main="") 

```


```{r XGBvarimp, echo=FALSE, fig.cap="Variable importance of XGBoost"}

# variable importance of XGBoost
xgb_varimp <- varImp(xgb, type="Gini", algorithm="xgbTree")
plot(xgb_varimp, col="coral2", main="")

```

\newpage

### **3.2 Partial dependence**

### **3.2.1 Random Forest**

**i. Predicted bike rental count vs hour**

In **figure 10**, we see a non-linear relationship between hour and the predicted bike rental count. There is fluctuation in the number of bikes rented across the various hours.
Intuitively, it makes sense that the most bikes are rented between 7:00-8:00 in the morning when people wake up (to commute to work or school) and between 17:00-18:00 in the afternoon (when people travel home). 

**ii. Predicted bike rental count vs temperature**

In **figure 10**, we see an "S" shaped curve, suggesting a non-linear relationship between temperature and the predicted bike rental count. However, there is a positive and almost linear relationship relationship between temperature and the target variable from 0- to 25- degrees Celsius. This suggests that as temperature increases, the number of bikes rented increase, until an optimal value is reached at 25 degrees Celsius. Beyond 25 degrees Celsius, the number of bikes rented begin to decrease. Whereas, below 0 degrees Celsius, the number of bikes rented remain constant at a count below 500. 

**iii. Predicted bike rental count vs seasons**

In **figure 10**, there is variation in the number of bikes rented across the seasons. Autumn has the largest number of bikes rented followed by summer and spring, while winter has the least number of bikes rented. This suggests, that hot to moderate weather increases the number of bikes rented. 

**iv. Predicted bike rental count vs humidity**

In **figure 10**, there is a non-linear relationship between humidity and the predicted bike rental count. The number of bike rentals gradually decrease as humidity increases, however, beyond a humidity of 75%, there is a steep descent in the number of bike rentals.

```{r RFpdp, echo=FALSE, fig.cap="Random forest partial dependence plot", warning=FALSE}

options(warn=-1) #suppress library messages 

# load library
library(pdp)
library(ggplot2)
library(gridExtra)
library(dplyr)

# partial dependence plots
pdp1 <- partial(rf, pred.var="Hour", plot="TRUE", plot.engine="ggplot2")
pdp2 <- partial(rf, pred.var="Temperature", plot="TRUE", plot.engine="ggplot2")
pdp3 <- partial(rf, pred.var="Seasons", plot="TRUE", plot.engine="ggplot2")
pdp4 <- partial(rf, pred.var="Humidity", plot="TRUE", plot.engine="ggplot2")

grid.arrange(pdp1, pdp2, pdp3, pdp4, nrow=2, ncol=2) #plot grid of plots

```

\newpage

### **3.2.2 GBM**

**i. Predicted bike rental count vs Hour**

In **figure 11**, we see a similar pattern to the random forest. We see the same non-linear relationship between hour and the predicted bike rental count (see explanation in random forest **3.2.1 i**). The curve also follows the same shape as before. 

**ii. Predicted bike rental count vs Temperature**

In **figure 11**, we see a similar pattern to the random forest. We see the same non-linear relationship between temperature and the predicted bike rental count (see explanation in random forest **3.2.1 ii**). The shape of the curve is similar as before. The only difference is now we see a steeper decrease in the predicted bike rental count  beyond a temperature of 25 degrees Celsius. 

**iii. Predicted bike rental count vs Functioning day**

In **figure 11**, we see clear variation in the bike rental count vs functioning day. Intuitively, there are more bikes rented during functional hours than non-functional hours.

**iv. Predicted bike rental count vs Humidity**

In **figure 11**, we see a similar pattern to the random forest. We see a similar non-linear relationship between humidity and the predicted bike rental count (see explanation in random forest **3.2.1 iv**). The only difference is that we now see a steep descent in the number of bike rentals below a humidity of 75%. The shape of the curve is also very similar to that of the random forest.

```{r GBMpdp, echo=FALSE, fig.cap="GBM partial dependence plot"}

# partial dependence plots
pdp5 <- partial(gbm_gridsearch, pred.var="Hour", plot="TRUE", plot.engine="ggplot2")
pdp6 <- partial(gbm_gridsearch, pred.var="Temperature", plot="TRUE", plot.engine="ggplot2")
pdp7<- partial(gbm_gridsearch, pred.var="Functioning_Day", plot="TRUE", plot.engine="ggplot2")
pdp8 <- partial(gbm_gridsearch, pred.var="Humidity", plot="TRUE", plot.engine="ggplot2")

grid.arrange(pdp5, pdp6, pdp7, pdp8, nrow=2, ncol=2) #plot grid of plots

```


### **3.2.3 XGBoost**

**i. Predicted bike rental count vs Temperature**

In **figure 12**, we see a similar pattern to the random forest and the GBM. We see the same non-linear relationship between temperature and the number of predicted bikes (see explanation in random forest **3.2.1 ii**). The shape of the curve is also similar as before.

**ii. Predicted bike rental count vs Hour**

In **figure 12**, we see a similar pattern to the random forest and GBM. We see the same non-linear relationship between hour and number of predicted bikes (see explanation in random forest **3.2.1 i**). The curve also follows the same shape as before. 

**iii. Predicted bike rental count vs Functioning Day**

In **figure 12**, we see a similar pattern to the GBM model. We see the same variation in the predicted bike rental count between the classes of functioning day. As before, we see a maximum number of bikes rented during the functional hours of the day compared to the non-functional hours of the day. 

**iv. Predicted bike rental count vs Solar Radiation**

In **figure 12**, we see a non-linear relationship between solar radiation and the predicted bike rental count. There is a steep increase in the number of bikes rented as solar radiation increases from 0(MJ/m2) to ~0.5(MJ/m2). Beyond this point, there is a gradual increase in the number of bikes rented as solar radiation increases. Overall, we can say that there is a positive non-linear relationship between this feature and the target variable. 


```{r XGBoostpdp, echo=FALSE, fig.cap="XGBoost partial dependence plot"}

# partial dependence plots
pdp9 <- partial(xgb, pred.var="Temperature", plot="TRUE", plot.engine="ggplot2")
pdp10 <- partial(xgb, pred.var="Hour", plot="TRUE", plot.engine="ggplot2")
pdp11 <- partial(xgb, pred.var="Functioning_Day", plot="TRUE", plot.engine="ggplot2")
pdp12 <- partial(xgb, pred.var="Solar_Radiation", plot="TRUE", plot.engine="ggplot2")

grid.arrange(pdp9, pdp10, pdp11, pdp12, nrow=2, ncol=2) #plot grid of plots

```

**End**
