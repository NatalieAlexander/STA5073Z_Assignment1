---
title: "Data Science for Industry (STA5073Z)" 
subtitle: "Assignment 1"
author: "Natalie Bianca Alexander"
format: 
  html:
    embed-resources: true
    toc: true
    toc-location: left
    number-sections: true
    number-depth: 3
    theme: journal
execute:
  echo: false
  cache: true
---

```{r setup, include=FALSE, eval=T}

knitr::opts_chunk$set(echo =F, warnings=F, message=F)

```

```{r libraries, include=F, eval=T}

#list of libraries to load
packages <- c("stringr", "tidytext", "tidyr", "tidyverse", "dplyr", "ggplot2",
              "readr", "lubridate", "caret", "wordcloud", "rpart", "tree",
              "rpart.plot", "e1071", "randomForest",
              "gbm", "MLmetrics", "xgboost", "gridExtra")

##check if package is installed, if not then install package
##uncomment if required
# for (package in packages) {
#   if (!require(package, character.only = TRUE)) {
#     install.packages(package)
#   }
# }

#load libraries
lapply(packages, library, character.only = TRUE)

################################################################################

# #install remotes
# #uncomment if required
# install.packages("remotes")
# remotes::install_github(sprintf("rstudio/%s",c("reticulate", "tensorflow", "keras")))


# Load the reticulate package
library(reticulate)

# Activate the conda environment in R-studio
#use_condaenv('rminiconda') #rename to your environment

# Load the tensorflow library
library(tensorflow)

# Install tensorflow in the rminiconda environment
#install_tensorflow(envname = 'rminiconda') #rename to your environment

# Check if tensorflow is active
#tf$constant("Hello Tensorflow")

#load library keras
library(keras)

```

```{r dataprocess, eval=T, include=F}

#load in data
sona = readRDS("data/preprocessed_sona.rds")

#ensure data is in tibble format
sona <- as_tibble(sona)

#check head
head(sona)

#check tail
tail(sona)

#check data types
str(sona)

#remove date which occurs before a speech
sona$speech = sona$speech %>% str_replace("^\\d{1,2}\\s[A-Za-z]{3,9}\\s\\d{4}", "")

#remove dates from speeches which start with "Thursday ..."
sona$speech = sona$speech %>% str_replace("^(Thursday,\\s10\\sFebruary\\s2022\\s)", "")

#remove trailing whitespaces before and after a speech
sona$speech = sona$speech %>% str_trim(side="both")

#check column names
colnames(sona)

#rename columns
colnames(sona) = c("filename", "speech", "year", "president_label", "date")

#rearrange columns
sona = sona %>% select(filename, president_label, year, date, speech)

#check for class imbalance
sona %>% group_by(president_label) %>% count() #classes are imbalanced

#remove outliers: Motlanthe and deKlerk - only 1 record
sona = sona %>% filter(!(president_label %in% c("Motlanthe", "deKlerk")))


```

# Abstract

Text mining refers to the process of transforming unstructured text data into structured clusters of information. This project explores a specific aspect of text mining known as authorship attribution, which involves analysing various linguistic and stylistic features of text to predict its author, or in this case the "speaker". This project assesses transcription data containing the speeches delivered by South African presidents during the State of the Nation Address (SONA) from 1994 to 2023. The primary goal was to develop a classification model that can take a sentence from a SONA speech and correctly predict the president who said it. Various models were evaluated in this project, such as Classification Trees, and Random Forests, in addition to XGBoost  - , Naïve Bayesian- and feed forward Neural network- models. However, I find that .... outperformed all other models with a test ....F1-score of...

# Introduction

Text mining is a branch of artificial intelligence (AI) that aims to transform unstructured text data into structured formats (Ibm.com, 2023)^1^. Text mining employs a variety of statistical and machine learning methods, including Classification Trees, Random Forests, Naïve Bayesian models, and numerous other deep learning algorithms. These methods are used to uncover textual patterns, trends, and hidden relationships within unstructured data.

While traditional text mining relied solely on machine learning algorithms, modern text mining also employs sophisticated methods of Natural Language Processing (NLP) such as parsing and part-of-speech tagging (Greenbook.org, 2017)^2^. This advancement in text mining is largely attributed to the exponential growth in data, with approximately 80% of global data residing in unstructured formats. This vast amount of data has necessitated the use of text mining, making it a significant task in the field of data analytics.

The application of text mining is particularly useful in large organizations where decision-making is central and time is limiting (Ibm.com, 2023)^1^. For example, banks employ text mining in risk management to scrutinize changes in the sentiment of financial reports.

The many applications of text mining across various domains have led to the development of several models, including supervised, and unsupervised methods (Dogra et al., 2022)^3^. However, determining the most appropriate and effective model for a specific text mining task remains a complex and nuanced challenge.

The aim of this project is to identify the most effective classification model for authorship attribution, which involves determining the author of a given document (Mohamed Amine Boukhaled and Jean-Gabriel Ganascia, 2017)^4^. This project specifically focuses on analysing transcription text data from speeches delivered by South African presidents during the State of the Nation Address (SONA) from 1994 to 2023 (www.gov.za, 2023)^5^. SONA serves as an annual opening to South African Parliament, where the President reports on the socio-economic state of the nation to a joint sitting of Parliament. The main objective is to train a model that can take a sentence from a SONA speech and correctly predict which president said it.

# Literature Review

Several studies have machine learning methods for author prediction. These techniques include Support Vector Machines (SVMs), Decision Trees, Random Forests (RF), and Neural Networks (NNs), to name a few. The choice of model often depends on the nature of the data and the specific requirements of the task.

Feature selection plays a crucial role in author prediction, where features are broadly categorized into lexical features (e.g., word usage) and syntactic features (e.g., part-of-speech tags). Some studies have also explored semantic features (e.g., topics and sentiments).

An article by Shukri, (2021)^6^ suggests a method for author prediction, by training models on Arabic opinion articles. The study collected 8109 articles from 428 authors for the period 2016 to 2021. Their NN model achieved the highest accuracy of 81.1%.

Another article by Bauersfeld et al., (2023)^7^ proposed a transformer-based, neural-network architecture that uses text content and author names in the bibliography to determine the author of an anonymous manuscript. The authors used all research papers publicly available on arXiv and achieved a 73% accuracy rate.

A similar paper by Khalid, (2021)^8^ performed author prediction on 210 000 anonymous, news headlines from HuffPost (2012-2022). The study used Bag of Words (BoW) and Latent Semantic Analysis (LSA) features as input to train classification algorithms such as LR and RF. The study found that the LR model trained on all features outperformed all other models with an accuracy of 94.9%.

These manuscripts demonstrate the potential of text mining in various applications. These papers also highlight the variety of author-prediction techniques available, such as NN-, LR-, and RF- and SVM- models. However, we also note the challenges in applying these techniques, such as the need for large datasets, as well as the ability of these models to discriminate between content-related features and author-specific features.

# Data

## Data Source

The data set contains the speeches delivered by South African presidents during the State of the Nation Address (SONA) from 1994 to 2023. The data is publicly available on the South African government website (www.gov.za, 2023)^5^.

## Data Description

Seven speeches are available for former president Mandela (1994-1999). Ten speeches are available for former president Mbeki (2000-2008). Ten speeches are also available for former president Zuma (2009-2017). President Ramaphosa has a total of seven speeches to date (2018-2023). Two outliers exist, namely one speech for former president deKlerk (1994) and former president Motlanthe (2009).

These records cumulatively formed the "sona" data set with 36 records and 5 variables, namely: *filename, speech, year, president* and *date of speech delivered*.

## Data Pre-processing

All data was read into R version 4.3.1 , using R-Studio version 2023.9.1.494. The year of each speech was extracted from the first four characters in the "filename" column. The president names were extracted from the "filename" column using regular expressions, where alphabetical text ending in a ".txt" extension was matched as the presidents' name. Subsequently, all other unnecessary text such as "http"-, fullstop-, ampersand-, greater-than-, and less-than characters were removed, in addition to trailing white spaces and new-line characters. Dates were then re-formatted into a *dd-mm-yyyy* format. Finally, the pre-processed data was saved as an RDS object for downstream analysis.

# Methods

## Data processing

The pre-processed data was read into R and converted to tibble format. The data was then assessed by looking at the head and tail of the tibble, in addition to looking at the data types of each column. Dates which appeared before a president's speech were removed using regular expressions. Trailing white spaces before and after a president's speech were also removed. I also noticed that former president- Motlanthe and deKlerk only had one record in comparison to the other presidents with more than one record, and so these observations were removed from the data set.

As a result, the processed data had 34 rows and 5 columns.

## Tokenization

The processed sona data set was then tokenized into sentences using unnest_tokens(), since the goal is to make predictions on sentence inputs. All text was then converted to lowercase to remove word redundancy, where each word is treated as its own unique feature, irrespective of letter case. I also removed all punctuation so that root-words are treated alike*.* I then included a sentence id column to track sentence membership.

The sentence tokens were then tokenized into word and bigram tokens respectively, where all stop words were then removed. I also ensured that "blank" tokens were removed. For the tokenization by bigram implementation, bigrams were split into individual words, where each word was assessed for stop words. If a stop word was detected, the entire bigram was removed, while the remaining bigrams were unified.

## Exploratory Data Analysis

I looked at the 20 most frequently used- words and bigrams: *(1)* for all presidents, and *(2)* for each president. I also looked at the average number of sentences per speech for a particular president. I then looked at the average number of words per sentence for a particular president.

## Bag-of-Words Model 

The machine learning models to be discussed implement the bag-of-words model (BoW). The BoW model computes the frequency of occurrence of an unordered collection of words and uses these frequencies as features to train the classifier.

A word bag was generated by taking the word tokens and grouping the unique sentence ID-president-word combinations and computing each grouping's frequency, after which the top 200 words for each grouping was selected to create the final word bag. The choice of the top 200 most frequently occurring words was chosen due to its superior model performance relative to the top 100 and 500 words. The word bag consisted of 363 rows (words) and 3 columns (sentence ID, president, word).

The BoW was constructed by identifying all the words in a speech that overlap with the word bag. The frequency of a word within a sentence was then calculated. Finally, the BoW table was reformatted to a wide format (tidy format), where the column names are words (features), the rows are identified by the sentence ID and president name (observations) and the cell values are the frequency of a word within a sentence. All words not found in a sentence obtained a value of 0.

## Term Frequency-Inverse Document Frequency Model

Term Frequency -- Inverse Document Frequency (TF-IDF) refers to the metric that describes how important a word is in a document relative to other documents in the corpus. The TF-IDF is calculated by multiplying the Term Frequency (TF) by the Inverse Document Frequency (IDF), where TF is frequency of occurrence of a word in a document divided by the total number of words in the document, whereas IDF is the proportion of documents in the corpus that contain the word.

The bind_tf_idf() function was used to calculate the TF-IDF for each word in each sentence. It is important to note the "document" here refers to the sentence ID. The BoW table previously discussed was then manipulated to include the TF-IDF instead of the frequency of the word.

## Class imbalance and up-sampling

I checked for class imbalance by comparing the frequency of records in each of the target variable classes, namely: Mandela, Mbeki, Zuma and Ramaphosa. I found that the classes were imbalanced, where Mbeki had the largest proportion of sentences (92), followed by Ramaphosa (51), Zuma (41) and Mandela (21). As a result, I used upSample() in the Caret package to oversample the minority classes so that the number of observations in each class match that of the majority class.

## Split Balanced Data into Training, Validation and Test sets

I partitioned the data into 70% training and 30% test sets using createDataPartition(), which performs stratified sampling. The target variable, *president* was then converted to factors, where classes: *Mandela, Mbeki, Ramaphosa* and *Zuma* were categorized as levels 1 to 4, respectively. For the validation set, 5-fold cross-validation was applied during training, which is discussed below.

## Models

Each model (*a-e* below) was implemented for both the BoW and TF-IDF methods discussed above. For each model, a grid search was performed to find the optimal hyperparameters, generating sub-models as a result. For each of these sub-models, 5-fold cross-validation was performed on the training set. The best sub-model was determined by the model performance on both the training and validation sets. The hyperparameters of the best sub-model was then used to rebuild the model on the full training set, after which predictions were made on the test set. The final models were compared based on their test set performance. **Figure 1** below shows the general workflow.

#### a. Classification Tree

Classification trees are decision trees that recursively partition the input space and assign a class label to each partitioned region based on the majority class of the training samples in that region. A grid search was performed using the rpart() function with  the following parameters:

-   The complexity parameter, a stopping criterion where tree splitting terminates once the reduction in relative error is less than a specified *cp-threshold.*

**cp = {0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889, 1.000}**

-   The minimum number of observations at any terminal node.

    **minbucket = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}**

-   The minimum number of observations that must exist in a node for a split to be attempted.

    **minsplit= {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}**

#### b. Random Forest

```{mermaid}
%%| echo: false 
%%| fig-cap: "__Figure 1:__ Flow-chart showing the work-flow of model construction, hyperparameter tuning and model selection and testing after splitting the data into 70% training and 30% test sets."
flowchart TB
  A>1. Select Features] --> B[a. Bag-of-Words]
  A --> D>2. Select Training Model]
  A --> C[b. Term Frequency-Inverse Document Frequency]
  D --> E[a. Classification Tree]
  D --> F[b. Random Forest]
  D --> J>3. Grid Search using 5-fold cross-validation]
  D --> G[c. Extreme Gradient Boosting]
  D --> H[d. Naïve Bayes]
  D --> I[e. Feed Forward Neural Network]
  J --> K>4. Choose best sub-model based on train and validation performance]
  K --> L>5. Rebuild model using the optimal hyperparameters on the full training set]
  L --> M>6. Predictions on test set]
  
```

::: callout-note
In **Figure 1** above, the ribbons are the steps in the process and the rectangles are the choices made at that step.
:::

## Performance Metrics

For all models, performance on the training(folds-1), validation(fold), full training and test sets were determined by the metrics discussed below. For 5-fold cross-validation, the best sub-model was determined which had the best validation and training macro-average F1-score. The overall best model was also chosen based on the model with the best test set macro-average F1-score. Other metrics were also considered (see below).

When training a model with 5-fold cross-validation, each metric was determined at each fold for both the training and validation sets. At each fold, the metric is computed for every class, and the macro-average metric is obtained by averaging these class-specific metrics (**Equation 1**). After training is complete, the final metric is computed as the average of the macro-average metrics over all folds (**Equation 2**).

$$
\text{MacroAverageMetric}_{\text{fold}} = \frac{1}{n} \sum_{i=1}^{n} \text{metric}_{i,\text{fold}}
$$

*...equation 1*, where metric~i, fold~ is the metric for class i at a specific fold, and n is the number of classes.

$$
\text{FinalMetric} = \frac{1}{k} \sum_{j=1}^{k} \text{MacroAverageMetric}_{j}
$$

*...equation 2*, where MacroAverageMetric~j~ is the macro-average metric at fold j, and k is the total number of folds.

#### a. Accuracy

This is the proportion of correct classifications among the total number of classifications. The formula is shown in **Equation 3** below:

$$
\text{Accuracy} = \frac{(\text{TP} + \text{TN})}{(\text{TP} + \text{TN} + \text{FP} + \text{FN})}
$$

*...equation 3,* where TP, TN, FP and FN are the number of true positives, true negatives, false positives, and false negatives, respectively found in the confusion matrix.

#### b. Recall

Also known as the sensitivity or true positive rate, this is the proportion of actual positives that are correctly classified.

$$
\text{Recall} = \frac{\text{TP}}{(\text{TP} + \text{FN})}
$$*...equation 4*

#### c. Precision

Also known as the positive predictive value, this is the proportion of positive predictions that are actually correctly classified.

$$
\text{Precision} = \frac{\text{TP}}{(\text{TP} + \text{FP})}
$$*...equation 5*

#### d. F1-score

This is a measure that combines precision and recall using the harmonic mean, by obtaining a balance of precision and recall.

$$
F1 = 2 \frac{(\text{Precision} \times \text{Recall})}{(\text{Precision} + \text{Recall})}
$$*...equation 6*

# Results & Discussion

## Exploratory Data Analysis

#### a. Top 20 most frequent words and bigrams

**Figure 2** below shows the top 20 most frequently used words and bigrams among all presidents. The bigrams provide more context relative to the unigrams, and we see that most president's agenda is about economic growth with bigrams such as "economic growth", "job creation" and "economic empowerment". **Figure 3** below shows the top 20 most frequently used words for each president and **Figure 4** below shows the top 20 most frequently used bigrams for each president. In these figures we see that the agenda is more specific to that year or that president, where Mandela has bigrams such as "people-centered society" alluding to the end of Apartheid, Mbeki focuses on social justice with bigrams such as "social security" and "social partners", Zuma focuses on the "world cup" which alludes to the 2010 soccer world cup hosted in South Africa and Ramaphosa's focus is on crime prevention with bigrams such as "gender-based violence" and "law enforcement".

```{r tokenization, eda1, eval=T, fig.show=T, fig.cap= "Figure 2: Bar plots showing the 20 most frequently used (a) words and (b) bigrams used by all presidents"}

###############################Tokenize by sentence#############################

sona_tokenized_by_sentence = unnest_tokens(sona, sentence, speech,
  token = 'sentences',
  to_lower = T) %>% #convert text to lowercase
  select(sentence, everything())

#strip all punctuation from each sentence
sona_tokenized_by_sentence = sona_tokenized_by_sentence %>%
  mutate(sentence= str_replace_all(sentence, "[[:punct:]]", ""))

#add sentence id column
sona_tokenized_by_sentence = sona_tokenized_by_sentence %>%
  mutate(sentence_id = 1:nrow(sona_tokenized_by_sentence))

###############################Tokenize by word#################################

sona_tokenized_by_word = unnest_tokens(sona_tokenized_by_sentence, word, sentence,
  token = 'words',
  to_lower = T)%>%  #ensure text is in lowercase
  select(word, everything())  %>%
  #remove stop words and empty tokens
  filter(!word %in% stop_words$word, str_detect(word, '[A-Za-z]'))

#Bar plot of top 20 words for all president_labels for all years
plot_twenty_words_all_presidents = sona_tokenized_by_word%>%
  count(word, sort = TRUE) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(word, n), n, fill=word))+
  geom_col() + coord_flip() + xlab('')+
  guides(fill= "none") + labs(x = "word", y = "Frequency")+
  ggtitle("A. Frequent words")

###############################Tokenize by bigram###############################

sona_tokenized_by_bigram = unnest_tokens(sona_tokenized_by_sentence, bigram, sentence,
  token = 'ngrams', n=2,
  to_lower = T) %>% #ensure text is in lowercase
  select(bigram, everything())

#separate the bigrams into words
bigrams_separated <- sona_tokenized_by_bigram %>%
  separate(bigram, c('word1', 'word2'), sep = ' ')

#remove stop words
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word)

#join up the bigrams again
sona_tokenized_by_bigram <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = ' ')

#Bar plot of top 20 most frequently used bigrams for all president_labels over all speeches and years
plot_twenty_bigrams_all_presidents = sona_tokenized_by_bigram%>%
  count(bigram, sort = TRUE) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(bigram, n), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('')+
  guides(fill= "none")+labs(x = "bigram", y = "Frequency")+
  ggtitle("B. Frequent bigrams")

#save image
grid.arrange(plot_twenty_words_all_presidents,
             plot_twenty_bigrams_all_presidents, ncol= 2)

```

<br>

<br>

```{r eda1, eval=T, fig.show=T, fig.cap="Figure 3: Bar plots showing the 20 most frequently used words by each president. "}

#Mandela most commonly used words
plot_twenty_words_mandela = sona_tokenized_by_word %>%
  select(president_label, word) %>%
  filter(president_label=="Mandela")%>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(word, n, fill=word), n,
  fill=word)) + geom_col() + coord_flip() + xlab('') +
  ggtitle("A. Mandela")+
  guides(fill="none") + labs(x = "word", y = "Frequency")


#Mbeki most commonly used words
plot_twenty_words_mbeki = sona_tokenized_by_word %>%
  select(president_label, word) %>%
  filter(president_label=="Mbeki")%>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(word, n), n,
  fill=word)) + geom_col() + coord_flip() + xlab('') +
  ggtitle("B. Mbeki")+
  guides(fill="none") + labs(x = "word", y = "Frequency")

#Zuma most commonly used words
plot_twenty_words_zuma = sona_tokenized_by_word %>%
  select(president_label, word) %>%
  filter(president_label=="Zuma")%>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20)%>%
  ggplot(aes(reorder(word, n), n,
  fill=word)) + geom_col() + coord_flip() + xlab('') +
  ggtitle("C. Zuma")+
  guides(fill="none") + labs(x = "word", y = "Frequency")

#Ramaphosa most commonly used words
plot_twenty_words_ramaphosa = sona_tokenized_by_word %>%
  select(president_label, word) %>%
  filter(president_label=="Ramaphosa")%>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(word, n), n,
  fill=word)) + geom_col() + coord_flip() + xlab('') +
  ggtitle("D. Ramaphosa")+
  guides(fill="none") + labs(x = "word", y = "Frequency")


#save image
grid.arrange(plot_twenty_words_mandela,
             plot_twenty_words_mbeki,
             plot_twenty_words_zuma,
             plot_twenty_words_ramaphosa,
             ncol= 2, nrow=2)

```

<br>

<br>

```{r eda2, eval=T, fig.show=T, fig.cap="Figure 4: Bar plots showing the 20 most frequently used bigrams by each president. "}

#Mandela most commonly used bigrams
plot_twenty_bigrams_mandela = sona_tokenized_by_bigram %>%
  select(president_label, bigram) %>%
  filter(president_label=="Mandela")%>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(bigram, n,
  fill=bigram), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('') +
  ggtitle("A. Mandela")+
  guides(fill="none") + labs(x = "bigram", y = "Frequency")


#Mbeki most commonly used bigrams
plot_twenty_bigrams_mbeki = sona_tokenized_by_bigram %>%
  select(president_label, bigram) %>%
  filter(president_label=="Mbeki")%>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(bigram, n), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('') +
  ggtitle("B. Mbeki")+
  guides(fill="none") + labs(x = "bigram", y = "Frequency")


#Zuma most commonly used bigrams
plot_twenty_bigrams_zuma = sona_tokenized_by_bigram %>%
  select(president_label, bigram) %>%
  filter(president_label=="Zuma")%>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20)%>%
  ggplot(aes(reorder(bigram, n), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('') +
  ggtitle("C. Zuma")+
  guides(fill="none") + labs(x = "bigram", y = "Frequency")


#Ramaphosa most commonly used bigrams
plot_twenty_bigrams_ramaphosa = sona_tokenized_by_bigram %>%
  select(president_label, bigram) %>%
  filter(president_label=="Ramaphosa")%>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(bigram, n), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('') +
  ggtitle("D. Ramaphosa")+
  guides(fill="none") + labs(x = "bigram", y = "Frequency")


#save image
grid.arrange(plot_twenty_bigrams_mandela,
             plot_twenty_bigrams_mbeki,
             plot_twenty_bigrams_zuma,
             plot_twenty_bigrams_ramaphosa,
             ncol= 2, nrow=2)

```

<br>

#### b. Average speech and sentence length

**Figure 5** below shows the average speech length per president **(a)** and the average sentence length per president **(b)**. In **Figure 5 (a)** we see that Ramaphosa has a relatively long average speech length, in other words the average number of sentences within a speech is quite long for Ramaphosa (327), followed by Zuma (266), Mbeki (242) and Mandela (238). In **Figure 5 (b)** we see that Mbeki (30) has the largest average sentence length (average number of words per sentence), followed by Mandela (25) , Ramaphosa (22) and Zuma (19).

```{r eda3, eval=T, include=F}

#Calculate the cumulative frequency of sentences for each president_label
total_sentences <- sona_tokenized_by_sentence %>%
  group_by(president_label) %>%
  summarise(total = n())


#Calculate number of speeches for each president_label
total_files <- sona_tokenized_by_sentence %>%
  select(president_label, filename) %>%
  group_by(filename)%>%
  distinct() %>% #find unique speeches per president_label
  group_by(president_label) %>%
  summarise(num_files = n())


#Average number of sentences per speech for each president_label
mean_num_sentences = total_sentences %>%
  left_join(total_files, by = "president_label") %>%
  mutate(mean_sentences = total / num_files)

#plot average number of sentences per speech for each president_label
plot_average_num_sentences_per_president = ggplot(mean_num_sentences, 
                                                  aes(x = president_label,
y = mean_sentences, fill = president_label)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(x = "president_label", y = "Average number of sentences per speech",
  title = "A. Average speech length")+
  guides(fill="none")+
   coord_flip()+
  labs(x = "president")

```

<br>

<br>

```{r eda4, eval=T, fig.show=T, fig.cap="Figure 5: Bar plots showing (a) the average number of sentences per speech and (b) the average number of words per sentence for each president."}

#plot average number of words per sentence for each president_label
plot_average_num_words_per_president = sona_tokenized_by_sentence %>% group_by(filename, president_label)%>%
  reframe(sentence_length=str_count(sentence, '\\w+'))%>% #count words of length 1 or greater
  group_by(president_label)%>% #per president_label
  reframe(Average = as.integer(mean(sentence_length)))%>% #mean number of words per sentence
  ggplot(aes(x = president_label, y = Average, fill = president_label)) +
  geom_col() +
  theme_minimal() +
  labs(x = "president_label",
  y = "Average number of words per sentence",
  title = "B. Average sentence length") +
  guides(fill="none")+
   coord_flip()+
  labs(x = "president")

#save image
grid.arrange(plot_average_num_sentences_per_president,
             plot_average_num_words_per_president, ncol = 2)

```

```{r bow, eval=T, include=F}
#prepare dataframe for all BEST model results
all_results = data.frame(model_ID = character(), parameters= character(),
           test_accuracy = double(),
           test_f1 = double(),
           test_precision = double(), 
           test_recall = double())

#create word bag
set.seed(123)
word_bag <- sona_tokenized_by_word %>%
  group_by(sentence_id, president_label, word) %>%
  count() %>% #frequency of word 
  ungroup() %>%
  top_n(200, wt = n) %>% #select top 200 most frequent words
  select(-n) #remove frequency of words column

#dimensions of word bag
nrow(word_bag)
ncol(word_bag)

#find the most frequently used words by each president
sona_tdf = sona_tokenized_by_word %>% inner_join(word_bag) %>%
  group_by(sentence_id,president_label, word) %>%
  count() %>% #frequency of most frequently used words 
  group_by(president_label) %>%
  mutate(total = sum(n)) %>% #get per president_label frequency
  ungroup()


#create bag of words table
bag_of_words = sona_tdf %>%
  select(sentence_id, president_label, word, n) %>%
  group_by(sentence_id, president_label)%>%
  pivot_wider(names_from = word, values_from = n, values_fill = 0)

```

<br>

<br>

```{r bowSplits, eval=T, include=F}

#check for class imbalance
table(bag_of_words$president_label)

#up-sample
set.seed(123)
bag_of_words = upSample(x = bag_of_words[, colnames(bag_of_words) != "president_label"],  # all predictor variables
                      y = as.factor(bag_of_words$president_label),  # target variable
                      yname = "president_label")

#check for class balance after up-sampling
table(bag_of_words$president_label)

#split into 70% training and 30% test sets
set.seed(123)
training_ids <- createDataPartition(bag_of_words$president_label, p = .7, 
                                    list = FALSE, times = 1)

#70% training data
df_train <- bag_of_words[training_ids,]

#30% test data
df_test <- bag_of_words[-training_ids,]

#check train and test dimensions
dim(df_train)[1] + dim(df_test)[1] == dim(bag_of_words)[1] #rows
dim(df_train)[2] == dim(df_test)[2] & dim(df_test)[2] == dim(bag_of_words)[2] #columns

#check for class imbalance
table(df_train$president_label) #balanced
table(df_test$president_label) #balanced


#check data types
str(df_train)
str(df_test)

#convert categorical dependent variable to factor
df_train$president_label = as.factor(df_train$president_label) #train
df_test$president_label = as.factor(df_test$president_label) #test

#exclude filename for training models
df_train = subset(df_train, select=-sentence_id)
df_test = subset(df_test, select=-sentence_id)

```

## Bag-of-Words Models

#### a. Classification Trees for Bag-of-Word Models

In **Table 1** below, we see the results of the best performing classification trees for the BoW model, arranged in descending order of training and validation F1-score. We see that model \# has the best cross-validation F1 score of and training-fold F1 score of. Other metrics.. other models.

We see two interesting clusters in the tree diagram in **Figure i** of **Addendum A**, where Ramaphosa (2018-present) is more likley to talk about health in accordance with the Covid-19 pandemic. Mandela (1994-1999) is more likely to talk about peace and freedom, which is in accordance with the end of the Apartheid era.

```{r bowClassTree, eval=T, include=F}

# Define the parameters for grid search
cp_grid <- seq(0.001, 1, length=10)
minbucket_grid <- seq(1, 10, 1)
minsplit_grid <- seq(1, 10, 1)

#empty data frame to store results for each combination of parameters above
results <<- data.frame(cp = double(), minbucket = integer(), minsplit = integer(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

#for each combination of parameters...
for (cp_val in cp_grid) {
  for (minbuck in minbucket_grid) {
    for (minspl in minsplit_grid) {

      ###########################################

      #empty vectors for results at each fold...
      train_accuracy_values <- numeric(num_folds)
      valid_accuracy_values <- numeric(num_folds)

      train_recall_values <- numeric(num_folds)
      valid_recall_values <- numeric(num_folds)

      train_ppv_values <- numeric(num_folds)
      valid_ppv_values <- numeric(num_folds)

      train_f1_values <- numeric(num_folds)
      valid_f1_values <- numeric(num_folds)


      ###########################################
      #for each fold...
      for (fold in 1:num_folds) {

        #split data into k-fold train and validation sets
        train_indices <- unlist(folds[-fold]) #all folds except 1 fold
        valid_indices <- unlist(folds[fold]) #only 1 fold
        
        df_train_fold <- df_train[train_indices, ]
        df_valid_fold <- df_train[valid_indices, ]

      ###########################################
        
        #fit the model on the training-fold data
        #with parameters at the current iteration
        set.seed(123)
        tree_fit <- rpart(president_label ~ ., df_train_fold,
                          control = rpart.control(minsplit = minspl,
                                                  minbucket = minbuck,
                                                  cp = cp_val),
                          method="class") #classification task

      ###########################################

        # Predict on train data
        set.seed(123)
        fittedtrain <- unname(predict(tree_fit, type = 'class'))

        # Train confusion matrix
        train_conf_mat <- confusionMatrix(data=fittedtrain, #predicted
                                          reference=as.factor(df_train_fold$president_label), #true
                                          mode = "everything") #all metrics

        #ensure Nan and NA values are 0 --> Nans appear when division by 0
        train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
        train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0

        # Compute the average train accuracy
        train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)

        # Compute the average train recall
        train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)


        # Compute the average train positive predictive value or "precision"
        train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute the average train F1-score
        train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)

        ###########################################

        # Predict on validation set
        set.seed(123)
        fittedvalid <- unname(predict(tree_fit, df_valid_fold, type = 'class'))

        # validation set confusion matrix
        valid_conf_mat <- confusionMatrix(data=fittedvalid, #predicted
                                          reference=as.factor(df_valid_fold$president_label), #true
                                          mode = "everything") #all metrics

        #ensure Nan and NA values are 0
        valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
        valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0

        # Compute the average validation accuracy
        valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)

        # Compute the average validation recall
        valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)

        # Compute the average validation positive predictive value or "precision"
        valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute the average validation F1-score
        valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)

      }
      ###########################################

      # Calculate the mean results across all folds
      
      ##Accuracy
      mean_train_accuracy <- mean(train_accuracy_values) #train
      mean_valid_accuracy <- mean(valid_accuracy_values) #validation
      
      ##recall
      mean_train_recall <- mean(train_recall_values) #train
      mean_valid_recall <- mean(valid_recall_values) #validation
      

      ##Positive predictive value
      mean_train_ppv <- mean(train_ppv_values) #train
      mean_valid_ppv <- mean(valid_ppv_values) #validation
      
      ##F1-score
      mean_train_f1 <- mean(train_f1_values) #train
      mean_valid_f1 <- mean(valid_f1_values) #validation
      

      ###########################################

      # Store the results
      results <- rbind(results, data.frame(cp = cp_val,
                                           minbucket = minbuck,
                                           minsplit = minspl,
                                           train_accuracy = mean_train_accuracy,
                                           valid_accuracy = mean_valid_accuracy,
                                           train_recall = mean_train_recall,
                                           valid_recall = mean_valid_recall,
                                           train_ppv = mean_train_ppv, valid_ppv = mean_valid_ppv,
                                           train_f1 = mean_train_f1, valid_f1 = mean_valid_f1
                                           ))
    }
  }
}

################################################################################

#rename models
rownames(results) = paste0("BoW tree model ", 1:nrow(results))


#check for optimal model with best train and validation f1-scores
best_results = results %>% filter(train_f1 > 0.5 & valid_f1> 0.5) %>% select(cp, #best = 0.001
                                                              minbucket, #best = 1
                                                              minsplit, #best = 1
                                                        train_f1, valid_f1, train_accuracy, valid_accuracy,
                                                        train_recall, valid_recall, train_ppv, valid_ppv)%>% 
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))


################################################################################

# Fit the best tree model to the original training data
set.seed(123)
bow_tree_fit <- rpart(president_label ~ . , df_train,
                             control = rpart.control(minsplit =1,
                                                     minbucket =1 ,
                                                     cp =0.001),
                                                     method="class")

#plot tree
# options(repr.plot.width = 12, repr.plot.height = 10) # set plot size in the notebook
tree_plot = prp(bow_tree_fit, cex=0.4, type=0,col="darkgreen",
    extra=1, #display number of observations for each terminal node
    roundint=F, #don't round to integers in output
    digits=1) #display 5 decimal places in output

##############################################################################

#predictions on test data
set.seed(123)
fittedtest <- predict(tree_fit, df_test, type = 'class')

#test set confusion matrix
test_conf_mat <- confusionMatrix(data=fittedtest, #predicted
                                 reference=as.factor(df_test$president_label), #true
                                 mode = "everything") #all metrics

#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
all_results[1, "test_accuracy"] <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the test recall
all_results[1, "test_recall"] <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the test positive predictive value or "precision"
all_results[1, "test_ppv"] <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
all_results[1, "test_f1"] <- round(mean(test_conf_mat$byClass[,'F1']), 3)

#model ID of best sub model
all_results[1, "model_ID"] = rownames(best_results)[1]

#parameters of best sub model
#all_results[1, "parameters"] = rownames(best_results)[1]

best_results

```

#### d. Random Forests for Bag-of-Word Models

```{r bowRF, eval=T, include=F}

#empty data frame to store results for each combination of parameters above
results <<- data.frame(ntree = integer(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

#grid search
ntrees_grid = c(100, 500, 1000)

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

###########################################

#gridsearch
for (ntree in ntrees_grid) {
  
  #empty vectors for results at each fold...
  train_accuracy_values <- numeric(num_folds)
  valid_accuracy_values <- numeric(num_folds)
  
  train_recall_values <- numeric(num_folds)
  valid_recall_values <- numeric(num_folds)
  
  train_ppv_values <- numeric(num_folds)
  valid_ppv_values <- numeric(num_folds)
  
  train_f1_values <- numeric(num_folds)
  valid_f1_values <- numeric(num_folds)


 ###########################################

  #for each fold...
  for (fold in 1:num_folds) {
  
    #split data into k-fold train and validation sets
    train_indices <- unlist(folds[-fold]) #all folds except 1 fold
    valid_indices <- unlist(folds[fold]) #only 1 fold
    
    df_train_fold <- df_train[train_indices, ]
    df_valid_fold <- df_train[valid_indices, ]
  
    ###########################################
    
    #fit the model at current fold
    set.seed(123)
    rf_fit <- randomForest(as.factor(president_label) ~ ., data=df_train_fold,
                       ntree = 500,  #no improvement at ntree=200 or ntree= 500
                        importance = TRUE,
                       na.action=na.exclude,
                        do.trace = 25)
  
   ###########################################
  
    # Predict on train data
    set.seed(123)
    fittedtrain <- unname(predict(rf_fit, type = 'class'))
  
    # Train confusion matrix
    train_conf_mat <- confusionMatrix(data=fittedtrain, #predicted
                                      reference=as.factor(df_train_fold$president_label), #true
                                      mode = "everything") #all metrics
  
    #ensure Nan and NA values are 0
    train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
    train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0
  
    # Compute the average train accuracy
    train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)
  
    # Compute the average train recall
    train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)
  
    # Compute the average train positive predictive value or "precision"
    train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)
  
    # Compute the average train F1-score
    train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)
  
    ###########################################
  
    # Predict on validation data
    set.seed(123)
    fittedvalid <- unname(predict(rf_fit, df_valid_fold, type = 'class'))
  
    # validation set confusion matrix
    valid_conf_mat <- confusionMatrix(data=fittedvalid, #fitted
                                      reference=as.factor(df_valid_fold$president_label), #true
                                      mode = "everything") #all metrics
  
    #ensure Nan and NA values are 0
    valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
    valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0
  
    # Compute the average validation accuracy
    valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)
  
    # Compute the average validation  recall
    valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)
  
    # Compute the average validation positive predictive value or "precision"
    valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)
  
    # Compute average validation F1-score
    valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)
  
  }
###########################################
  
  # Calculate the mean results across all folds
  
  ##average accuracy
  mean_train_accuracy <- mean(train_accuracy_values) #train
  mean_valid_accuracy <- mean(valid_accuracy_values) #validation
  
  ##average recall
  mean_train_recall <- mean(train_recall_values) #train
  mean_valid_recall <- mean(valid_recall_values) #validation
  
  ##average PPV
  mean_train_ppv <- mean(train_ppv_values) #train
  mean_valid_ppv <- mean(valid_ppv_values) #validation
  
  ##average F1-score
  mean_train_f1 <- mean(train_f1_values) #train
  mean_valid_f1 <- mean(valid_f1_values) #validation

  
  ###########################################
  
  # Store the results
  results <- rbind(results, data.frame(ntree = ntree, 
                                       train_accuracy = mean_train_accuracy,
                                       valid_accuracy = mean_valid_accuracy,
                                       
                                       train_recall = mean_train_recall,
                                       valid_recall = mean_valid_recall,
                                       
                                       train_ppv = mean_train_ppv, 
                                       valid_ppv = mean_valid_ppv,
                                       
                                       train_f1 = mean_train_f1, 
                                       valid_f1 = mean_valid_f1))
}

################################################################################

#rename models
rownames(results) = paste0("bow_RF_model_", 1:nrow(results))

#check for optimal model with best train and validation f1-scores
results %>% filter(train_f1 > 0.5 & valid_f1> 0.5) %>% select(ntree,
                                                        train_f1, valid_f1)%>%
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))


################################################################################

#re-fit the best model on entire train set
set.seed(123)
rf_fit <- randomForest(as.factor(president_label) ~ ., data=df_train,
                   ntree = 500, #keep it 500 to be consistent with BoW model
                   importance = TRUE,
                   na.action = na.exclude,
                    do.trace = 25)

###############################################################################

#prediction on test data
set.seed(123)
fittedtest <- predict(rf_fit, df_test, type = 'class')

#test confusion matrix
test_conf_mat <- confusionMatrix(data=fittedtest, #predicted data
                                 reference=as.factor(df_test$president_label), #reference data
                                 mode = "everything") #all metrics

#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
all_results[2, "test_accuracy"] <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the test recall
all_results[2, "test_recall"] <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the test positive predictive value or "precision"
# Compute the test positive predictive value or "precision"
all_results[2, "test_ppv"] <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
all_results[2, "test_f1"] <- round(mean(test_conf_mat$byClass[,'F1']), 3)

#model ID of best sub model
#all_results[2, "model_ID"] = rownames(best_results)[1]

#parameters of best sub model
#all_results[1, "parameters"] = rownames(best_results)[1]

best_results

```

# Conclusion

In EDA (**Figure 5** above), we see differences in the average number of words used in a sentence for each president,this might be useful to include as a feature when training the models.

# Addendum

```{r addendum1, eval=T, fig.show=T, fig.cap= "Figure i: Tree diagram showing results of the best bag-of-words classification sub-model, built on the entire training set after the grid search was performed using 5-fold cross validation."}

#tree plot of BOW tree model
prp(bow_tree_fit, cex=0.4, type=0,col="darkgreen",
    extra=1, #display number of observations for each terminal node
    roundint=F, #don't round to integers in output
    digits=1)

```

# References

1.     Ibm.com. (2023). *What is Text Mining? \| IBM*. \[online\] Available at: https://www.ibm.com/. \[Accessed 14 Oct. 2023\].

2.     Greenbook.org. (2017). *Text Analytics: A Primer*. \[online\] Available at: https://www.greenbook.org/insights/market-research-leaders/text-analytics-a-primer \[Accessed 14 Oct. 2023\].

3.     Dogra, V., Verma, S., Kavita Kavita, Chatterjee, P., Shafi, J., Choi, J. and Muhammad Fazal Ijaz (2022). A Complete Process of Text Classification System Using State-of-the-Art NLP Models. Computational Intelligence and Neuroscience, \[online\] 2022, pp.1--26. doi:https://doi.org/10.1155/2022/1883698.

4.     Mohamed Amine Boukhaled and Jean-Gabriel Ganascia (2017). Stylistic Features Based on Sequential Rule Mining for Authorship Attribution. \[online\] doi:https://doi.org/10.1016/b978-1-78548-253-3.50008-1.

5.     www.gov.za. (2023). *State of the Nation Address \| South African Government*. \[online\] Available at: https://www.gov.za/state-nation-address \[Accessed 14 Oct. 2023\].

6.     Shukri, N. (2021). Author Prediction in Text Mining of the Opinion Articles in Arabic Newspapers. \[online\] 16(2), pp.1-05. doi:https://doi.org/10.9790/2834-1602020105.

7.     Bauersfeld, L., Romero, A., Manasi Muglikar and Davide Scaramuzza (2023). Cracking double-blind review: Authorship attribution with deep learning. PLOS ONE, \[online\] 18(6), pp.e0287611--e0287611. doi: https://doi.org/10.1371/journal.pone.0287611.

8.     Khalid, N. (2021). AUTHOR IDENTIFICATION BASED ON NLP. *European Journal of Computer Science and Information Technology*, \[online\] 9(1), pp.1--26. Available at: https://www.eajournals.org/wp-content/uploads/Author-Identification-Based-on-NLP.pdf.
