"0","#laplace values"
"0","laplace_grid = seq(0, 1, 0.1)"
"0",""
"0","#number of folds"
"0","num_folds <- 5"
"0",""
"0","#create folds"
"0","set.seed(123)"
"0","folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)"
"0",""
"0","#empty data frame to store results for each combination of parameters above"
"0","results <<- data.frame(laplace = double(),"
"0","                      train_accuracy = double(), valid_accuracy = double(),"
"0","                      train_recall = double(), valid_recall = double(),"
"0","                      train_ppv = double(), valid_ppv = double(),"
"0","                      train_f1 = double(), valid_f1 = double())"
"0",""
"0",""
"0","###########################################"
"0","for (lap in laplace_grid) {"
"0","  #empty vectors for results at each fold..."
"0","  train_accuracy_values <- numeric(num_folds)"
"0","  valid_accuracy_values <- numeric(num_folds)"
"0",""
"0","  train_recall_values <- numeric(num_folds)"
"0","  valid_recall_values <- numeric(num_folds)"
"0",""
"0","  train_ppv_values <- numeric(num_folds)"
"0","  valid_ppv_values <- numeric(num_folds)"
"0",""
"0","  train_f1_values <- numeric(num_folds)"
"0","  valid_f1_values <- numeric(num_folds)"
"0",""
"0",""
"0","  ###########################################"
"0",""
"0","  #for each fold..."
"0","  for (fold in 1:num_folds) {"
"0",""
"0","    #split data into k-fold train and validation sets"
"0","    train_indices <- unlist(folds[-fold]) #all folds except 1 fold"
"0","    valid_indices <- unlist(folds[fold]) #only 1 fold"
"0","    df_train_fold <- df_train[train_indices, ]"
"0","    df_valid_fold <- df_train[valid_indices, ]"
"0",""
"0","    ###########################################"
"0","    #fit the model with parameters at the current iteration"
"0","    set.seed(123)"
"0","    nb_fit <- naiveBayes(president_label ~ ., data=df_train_fold,"
"0","                       laplace=lap)"
"0",""
"0","    ###########################################"
"0",""
"0","    # Predict on train data"
"0","    set.seed(123)"
"0","    fittedtrain <- unname(predict(nb_fit, df_train_fold, type = 'class'))"
"0",""
"0","    # Train confusion matrix"
"0","    train_conf_mat <- confusionMatrix(data=fittedtrain, reference=df_train_fold$president_label, mode = ""everything"")"
"0",""
"0","    #ensure Nan and NA values are 0"
"0","    train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0"
"0","    train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0"
"0",""
"0","    # Compute the train accuracy"
"0","    train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)"
"0",""
"0","    # Compute the average train recall"
"0","    train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)"
"0",""
"0",""
"0","    # Compute the average train positive predictive value or ""precision"""
"0","    train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)"
"0",""
"0","    # Compute the train F1-score"
"0","    train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)"
"0",""
"0",""
"0","    ###########################################"
"0",""
"0","    # Predict on valid data"
"0","    set.seed(123)"
"0","    fittedvalid <- unname(predict(nb_fit, df_valid_fold, type = 'class'))"
"0",""
"0","    # valid confusion matrix"
"0","    valid_conf_mat <- confusionMatrix(data=fittedvalid, reference=df_valid_fold$president_label, mode = ""everything"")"
"0",""
"0","    #ensure Nan and NA values are 0"
"0","    valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0"
"0","    valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0"
"0",""
"0","    # Compute the valid accuracy"
"0","    valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)"
"0",""
"0","    # Compute the average valid recall"
"0","    valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)"
"0",""
"0","    # Compute the average valid positive predictive value or ""precision"""
"0","    valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)"
"0",""
"0","    # Compute the valid F1-score"
"0","    valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)"
"0",""
"0","    }"
"0","    ###########################################"
"0",""
"0","    # Calculate the mean results across all folds"
"0","    mean_train_accuracy <- mean(train_accuracy_values)"
"0","    mean_valid_accuracy <- mean(valid_accuracy_values)"
"0",""
"0","    mean_train_recall <- mean(train_recall_values)"
"0","    mean_valid_recall <- mean(valid_recall_values)"
"0",""
"0",""
"0","    mean_train_ppv <- mean(train_ppv_values)"
"0","    mean_valid_ppv <- mean(valid_ppv_values)"
"0",""
"0","    mean_train_f1 <- mean(train_f1_values)"
"0","    mean_valid_f1 <- mean(valid_f1_values)"
"0",""
"0",""
"0","    ###########################################"
"0",""
"0","    # Store the results"
"0","    results <- rbind(results, data.frame(laplace=lap,"
"0","                                     train_accuracy = mean_train_accuracy, valid_accuracy = mean_valid_accuracy,"
"0","                                     train_recall = mean_train_recall, valid_recall = mean_valid_recall,"
"0","                                     "
"0","                                     train_ppv = mean_train_ppv, valid_ppv = mean_valid_ppv,"
"0","                                     train_f1 = mean_train_f1, valid_f1 = mean_valid_f1))"
"0",""
"0","}"
"0",""
"0","############################################################################"
"0",""
"0","#rename models"
"0","rownames(results) = paste0(""BoW NB model "", 1:nrow(results))"
"0",""
"0","#check for optimal model with best train and validation f1-scores"
"0","best_results = results"
"0",""
"0","################################################################################"
"0",""
"0","#re-fit the model on entire train set"
"0","set.seed(123)"
"0","nb_fit <- naiveBayes(president_label ~ ., data=df_train_fold, laplace=0)"
"0",""
"0","###############################################################################"
"0",""
"0","#test data"
"0","set.seed(123)"
"0","fittedtest <- predict(nb_fit, df_test, type = 'class')"
"0",""
"0","#test confusion matrix"
"0","test_conf_mat <- confusionMatrix(data=fittedtest, reference=df_test$president_label, mode = ""everything"")"
"0",""
"0","#ensure Nan and NA values are 0"
"0","test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0"
"0","test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0"
"0",""
"0","# Compute the test accuracy"
"0","all_results[4, ""test_accuracy""] <- round(test_conf_mat$overall['Accuracy'], 3)"
"0",""
"0","# Compute the test recall"
"0","all_results[4, ""test_recall""] <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)"
"0",""
"0","# Compute the test positive predictive value or ""precision"""
"0","# Compute the test positive predictive value or ""precision"""
"0","all_results[4, ""test_ppv""] <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)"
"0",""
"0","# Compute the test F1-score"
"0","all_results[4, ""test_f1""] <- round(mean(test_conf_mat$byClass[,'F1']), 3)"
"0",""
"0","#model ID of best sub model"
"0","#all_results[4, ""model_ID""] = rownames(best_results)[1]"
"0",""
"0","#parameters of best sub model"
"0","#all_results[4, ""parameters""] = rownames(best_results)[1]"
"0",""
