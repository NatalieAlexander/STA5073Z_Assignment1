---
title: "STA5073Z: Assignment 1"
author: "Natalie Bianca Alexander"
date: "2023-10-11"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warnings=F, message=F)

```

# A. Data processing, Tokenization and Analysis

# 1. Libraries

```{r, echo = FALSE, results="hide", include=FALSE}

#list of libraries to load
packages <- c("stringr", "tidytext", "tidyr", "tidyverse", "dplyr", "ggplot2",
              "readr", "lubridate", "caret", "wordcloud", "rpart", "tree",
              "rpart.plot", "e1071", "randomForest",
              "gbm", "MLmetrics", "xgboost", "gridExtra")

##check if package is installed, if not then install package
##uncomment if required
# for (package in packages) {
#   if (!require(package, character.only = TRUE)) {
#     install.packages(package)
#   }
# }

#load libraries
lapply(packages, library, character.only = TRUE)

################################################################################

# #install remotes
# #uncomment if required
# install.packages("remotes")
# remotes::install_github(sprintf("rstudio/%s",c("reticulate", "tensorflow", "keras")))


# Load the reticulate package
library(reticulate)

# Activate the conda environment in R-studio
#use_condaenv('rminiconda') #rename to your environment

# Load the tensorflow library
library(tensorflow)

# Install tensorflow in the rminiconda environment
#install_tensorflow(envname = 'rminiconda') #rename to your environment

# Check if tensorflow is active
tf$constant("Hello Tensorflow")

#load library keras
library(keras)

```

# 2. Data processing

```{r, results="hide"}

#load in data
sona = readRDS("data/preprocessed_sona.rds")

#ensure data is in tibble format
sona <- as_tibble(sona)

#check head
head(sona)

#check tail
tail(sona)

#check data types
str(sona)

#remove date which occurs before a speech
sona$speech = sona$speech %>% str_replace("^\\d{1,2}\\s[A-Za-z]{3,9}\\s\\d{4}", "")

#remove dates from speeches which start with "Thursday ..."
sona$speech = sona$speech %>% str_replace("^(Thursday,\\s10\\sFebruary\\s2022\\s)", "")

#remove trailing whitespaces before and after a speech
sona$speech = sona$speech %>% str_trim(side="both")

#check column names
colnames(sona)

#rename columns
colnames(sona) = c("filename", "speech", "year", "president_label", "date")

#rearrange columns
sona = sona %>% select(filename, president_label, year, date, speech)

#check for class imbalance
sona %>% group_by(president_label) %>% count() #classes are imbalanced

#remove outliers: Motlanthe and deKlerk - only 1 record
sona = sona %>% filter(!(president_label %in% c("Motlanthe", "deKlerk")))


```

\newpage

# 3. Tokenization

```{r, eval=T, include=F}

###############################Tokenize by sentence#############################

sona_tokenized_by_sentence = unnest_tokens(sona, sentence, speech,
  token = 'sentences',
  to_lower = T) %>% #convert text to lowercase
  select(sentence, everything())

#strip all punctuation from each sentence
sona_tokenized_by_sentence = sona_tokenized_by_sentence %>%
  mutate(sentence= str_replace_all(sentence, "[[:punct:]]", ""))

#add sentence id column
sona_tokenized_by_sentence = sona_tokenized_by_sentence %>%
  mutate(sentence_id = 1:nrow(sona_tokenized_by_sentence))

###############################Tokenize by word#################################

sona_tokenized_by_word = unnest_tokens(sona_tokenized_by_sentence, word, sentence,
  token = 'words',
  to_lower = T)%>%  #ensure text is in lowercase
  select(word, everything())  %>%
  #remove stop words and empty tokens
  filter(!word %in% stop_words$word, str_detect(word, '[A-Za-z]'))

#Bar plot of top 20 words for all president_labels for all years
plot_twenty_words_all_presidents = sona_tokenized_by_word%>%
  count(word, sort = TRUE) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(word, n), n, fill=word))+
  geom_col() + coord_flip() + xlab('')+
  guides(fill= "none") + labs(x = "word", y = "Frequency")+
  ggtitle("A. Twenty most frequently used words by all presidents")

###############################Tokenize by bigram###############################

sona_tokenized_by_bigram = unnest_tokens(sona_tokenized_by_sentence, bigram, sentence,
  token = 'ngrams', n=2,
  to_lower = T) %>% #ensure text is in lowercase
  select(bigram, everything())

#separate the bigrams into words
bigrams_separated <- sona_tokenized_by_bigram %>%
  separate(bigram, c('word1', 'word2'), sep = ' ')

#remove stop words
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word)

#join up the bigrams again
sona_tokenized_by_bigram <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = ' ')

#Bar plot of top 20 most frequently used bigrams for all president_labels over all speeches and years
plot_twenty_bigrams_all_presidents = sona_tokenized_by_bigram%>%
  count(bigram, sort = TRUE) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(bigram, n), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('')+
  guides(fill= "none")+labs(x = "bigram", y = "Frequency")+
  ggtitle("B. Twenty most frequently used bigrams by all presidents")

#save image
grid.arrange(plot_twenty_words_all_presidents, 
             plot_twenty_bigrams_all_presidents, ncol= 2)

```

# 4. Pre-exploratory Data Analysis

## 4.1 Twenty most frequently used words for each president

```{r, eval=T, include=F}

#Mandela most commonly used words
plot_twenty_words_mandela = sona_tokenized_by_word %>%
  select(president_label, word) %>%
  filter(president_label=="Mandela")%>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(word, n, fill=word), n,
  fill=word)) + geom_col() + coord_flip() + xlab('') +
  ggtitle("A. Twenty most frequently used words by Mandela")+
  guides(fill="none") + labs(x = "word", y = "Frequency")


#Mbeki most commonly used words
plot_twenty_words_mbeki = sona_tokenized_by_word %>%
  select(president_label, word) %>%
  filter(president_label=="Mbeki")%>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(word, n), n,
  fill=word)) + geom_col() + coord_flip() + xlab('') +
  ggtitle("B. Twenty most frequently used words by Mbeki")+
  guides(fill="none") + labs(x = "word", y = "Frequency")

#Zuma most commonly used words
plot_twenty_words_zuma = sona_tokenized_by_word %>%
  select(president_label, word) %>%
  filter(president_label=="Zuma")%>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20)%>%
  ggplot(aes(reorder(word, n), n,
  fill=word)) + geom_col() + coord_flip() + xlab('') +
  ggtitle("C. Twenty most frequently used words by Zuma")+
  guides(fill="none") + labs(x = "word", y = "Frequency")

#Ramaphosa most commonly used words
plot_twenty_words_ramaphosa = sona_tokenized_by_word %>%
  select(president_label, word) %>%
  filter(president_label=="Ramaphosa")%>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(word, n), n,
  fill=word)) + geom_col() + coord_flip() + xlab('') +
  ggtitle("D. Twenty most frequently used words by Ramaphosa")+
  guides(fill="none") + labs(x = "word", y = "Frequency")


#save image
grid.arrange(plot_twenty_words_mandela,
             plot_twenty_words_mbeki,
             plot_twenty_words_zuma,
             plot_twenty_words_ramaphosa,
             ncol= 2, nrow=2)

```

## 4.2. Twenty most frequently used bigrams for each president

```{r, eval=T, include=F}

#Mandela most commonly used bigrams
plot_twenty_bigrams_mandela = sona_tokenized_by_bigram %>%
  select(president_label, bigram) %>%
  filter(president_label=="Mandela")%>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(bigram, n,
  fill=bigram), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('') +
  ggtitle("A. Twenty most frequently used bigrams by Mandela")+
  guides(fill="none") + labs(x = "bigram", y = "Frequency")


#Mbeki most commonly used bigrams
plot_twenty_bigrams_mbeki = sona_tokenized_by_bigram %>%
  select(president_label, bigram) %>%
  filter(president_label=="Mbeki")%>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(bigram, n), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('') +
  ggtitle("B. Twenty most frequently used bigrams by Mbeki")+
  guides(fill="none") + labs(x = "bigram", y = "Frequency")


#Zuma most commonly used bigrams
plot_twenty_bigrams_zuma = sona_tokenized_by_bigram %>%
  select(president_label, bigram) %>%
  filter(president_label=="Zuma")%>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20)%>%
  ggplot(aes(reorder(bigram, n), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('') +
  ggtitle("C. Twenty most frequently used bigrams by Zuma")+
  guides(fill="none") + labs(x = "bigram", y = "Frequency")


#Ramaphosa most commonly used bigrams
plot_twenty_bigrams_ramaphosa = sona_tokenized_by_bigram %>%
  select(president_label, bigram) %>%
  filter(president_label=="Ramaphosa")%>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ggplot(aes(reorder(bigram, n), n, fill=bigram)) +
  geom_col() + coord_flip() + xlab('') +
  ggtitle("D. Twenty most frequently used bigrams by Ramaphosa")+
  guides(fill="none") + labs(x = "bigram", y = "Frequency")


#save image
grid.arrange(plot_twenty_bigrams_mandela,
             plot_twenty_bigrams_mbeki,
             plot_twenty_bigrams_zuma,
             plot_twenty_bigrams_ramaphosa,
             ncol= 2, nrow=2)

```

\newpage

## 4.3 Average speech length per president

### i.e., average number of sentences per speech for each president

```{r, eval=T, include=F}

#Calculate the cumulative frequency of sentences for each president_label
total_sentences <- sona_tokenized_by_sentence %>%
  group_by(president_label) %>%
  summarise(total = n())


#Calculate number of speeches for each president_label
total_files <- sona_tokenized_by_sentence %>%
  select(president_label, filename) %>%
  group_by(filename)%>%
  distinct() %>% #find unique speeches per president_label
  group_by(president_label) %>%
  summarise(num_files = n())


#Average number of sentences per speech for each president_label
mean_num_sentences = total_sentences %>%
  left_join(total_files, by = "president_label") %>%
  mutate(mean_sentences = total / num_files)

#plot average number of sentences per speech for each president_label
plot_average_num_sentences_per_president = ggplot(mean_num_sentences, 
                                                  aes(x = president_label,
y = mean_sentences, fill = president_label)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(x = "president_label", y = "Average number of sentences per speech",
  title = "A. Average speech length per president")+
  guides(fill="none")+
   coord_flip()+
  labs(x = "president")

```

\newpage

## 4.4 Average sentence length per president_label

### i.e., Average number of words per sentence for each president_label

```{r, eval=T, include=F}

#plot average number of words per sentence for each president_label
plot_average_num_words_per_president = sona_tokenized_by_sentence %>% group_by(filename, president_label)%>%
  reframe(sentence_length=str_count(sentence, '\\w+'))%>% #count words of length 1 or greater
  group_by(president_label)%>% #per president_label
  reframe(Average = as.integer(mean(sentence_length)))%>% #mean number of words per sentence
  ggplot(aes(x = president_label, y = Average, fill = president_label)) +
  geom_col() +
  theme_minimal() +
  labs(x = "president_label",
  y = "Average number of words per sentence",
  title = "B. Average sentence length per president") +
  guides(fill="none")+
   coord_flip()+
  labs(x = "president")

#save image
grid.arrange(plot_average_num_sentences_per_president,
             plot_average_num_words_per_president, ncol = 2)

```

\newpage


# B. *Bag of Words (BoW) Models*

```{r, eval=T, include=F}
#prepare dataframe for all BEST model results
all_results = data.frame(model_ID = character(), parameters= character(),
           train_accuracy = double(), valid_accuracy = double(), test_accuracy = double(), 
           train_f1 = double(), valid_f1 = double(), test_f1 =  double(), 
           train_precision = double(), valid_precision = double(), test_precision  = double(), 
           train_recall = double(), valid_recall = double(), test_recall = double())

#create word bag
set.seed(123)
word_bag <- sona_tokenized_by_word %>%
  group_by(sentence_id, president_label, word) %>%
  count() %>% #frequency of word 
  ungroup() %>%
  top_n(200, wt = n) %>% #select top 200 most frequent words
  select(-n) #remove frequency of words column

#dimensions of word bag
nrow(word_bag)
ncol(word_bag)

#find the most frequently used words by each president
sona_tdf = sona_tokenized_by_word %>% inner_join(word_bag) %>%
  group_by(sentence_id,president_label, word) %>%
  count() %>% #frequency of most frequently used words 
  group_by(president_label) %>%
  mutate(total = sum(n)) %>% #get per president_label frequency
  ungroup()


#create bag of words table
bag_of_words = sona_tdf %>%
  select(sentence_id, president_label, word, n) %>%
  group_by(sentence_id, president_label)%>%
  pivot_wider(names_from = word, values_from = n, values_fill = 0)

```

\newpage

## 1. Splitting data into train and test sets

```{r, eval=T, include=F}

#check for class imbalance
table(bag_of_words$president_label)

#up-sample
set.seed(123)
bag_of_words = upSample(x = bag_of_words[, colnames(bag_of_words) != "president_label"],  # all predictor variables
                      y = as.factor(bag_of_words$president_label),  # target variable
                      yname = "president_label")

#check for class balance after up-sampling
table(bag_of_words$president_label)

#split into 70% training and 30% test sets
set.seed(123)
training_ids <- createDataPartition(bag_of_words$president_label, p = .7, 
                                    list = FALSE, times = 1)

#70% training data
df_train <- bag_of_words[training_ids,]

#30% test data
df_test <- bag_of_words[-training_ids,]

#check train and test dimensions
dim(df_train)[1] + dim(df_test)[1] == dim(bag_of_words)[1] #rows
dim(df_train)[2] == dim(df_test)[2] & dim(df_test)[2] == dim(bag_of_words)[2] #columns

#check for class imbalance
table(df_train$president_label) #balanced
table(df_test$president_label) #balanced


#check data types
str(df_train)
str(df_test)

#convert categorical dependent variable to factor
df_train$president_label = as.factor(df_train$president_label) #train
df_test$president_label = as.factor(df_test$president_label) #test

#exclude filename for training models
df_train = subset(df_train, select=-sentence_id)
df_test = subset(df_test, select=-sentence_id)

```

\newpage

## 2 "Bag-of-words" models (BoW)

### 2.1 Classification tree model using BoW

```{r, eval=T, include=F}

# Define the parameters for grid search
cp_grid <- seq(0.001, 1, length=10)
minbucket_grid <- seq(1, 10, 1)
minsplit_grid <- seq(1, 10, 1)

#empty data frame to store results for each combination of parameters above
results <<- data.frame(cp = double(), minbucket = integer(), minsplit = integer(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

#for each combination of parameters...
for (cp_val in cp_grid) {
  for (minbuck in minbucket_grid) {
    for (minspl in minsplit_grid) {

      ###########################################

      #empty vectors for results at each fold...
      train_accuracy_values <- numeric(num_folds)
      valid_accuracy_values <- numeric(num_folds)

      train_recall_values <- numeric(num_folds)
      valid_recall_values <- numeric(num_folds)

      train_ppv_values <- numeric(num_folds)
      valid_ppv_values <- numeric(num_folds)

      train_f1_values <- numeric(num_folds)
      valid_f1_values <- numeric(num_folds)


      ###########################################
      #for each fold...
      for (fold in 1:num_folds) {

        #split data into k-fold train and validation sets
        train_indices <- unlist(folds[-fold]) #all folds except 1 fold
        valid_indices <- unlist(folds[fold]) #only 1 fold
        
        df_train_fold <- df_train[train_indices, ]
        df_valid_fold <- df_train[valid_indices, ]

      ###########################################
        
        #fit the model on the training-fold data
        #with parameters at the current iteration
        set.seed(123)
        tree_fit <- rpart(president_label ~ ., df_train_fold,
                          control = rpart.control(minsplit = minspl,
                                                  minbucket = minbuck,
                                                  cp = cp_val),
                          method="class") #classification task

      ###########################################

        # Predict on train data
        set.seed(123)
        fittedtrain <- unname(predict(tree_fit, type = 'class'))

        # Train confusion matrix
        train_conf_mat <- confusionMatrix(data=fittedtrain, #predicted
                                          reference=as.factor(df_train_fold$president_label), #true
                                          mode = "everything") #all metrics

        #ensure Nan and NA values are 0 --> Nans appear when division by 0
        train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
        train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0

        # Compute the average train accuracy
        train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)

        # Compute the average train recall
        train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)


        # Compute the average train positive predictive value or "precision"
        train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute the average train F1-score
        train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)

        ###########################################

        # Predict on validation set
        set.seed(123)
        fittedvalid <- unname(predict(tree_fit, df_valid_fold, type = 'class'))

        # validation set confusion matrix
        valid_conf_mat <- confusionMatrix(data=fittedvalid, #predicted
                                          reference=as.factor(df_valid_fold$president_label), #true
                                          mode = "everything") #all metrics

        #ensure Nan and NA values are 0
        valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
        valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0

        # Compute the average validation accuracy
        valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)

        # Compute the average validation recall
        valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)

        # Compute the average validation positive predictive value or "precision"
        valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute the average validation F1-score
        valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)

      }
      ###########################################

      # Calculate the mean results across all folds
      
      ##Accuracy
      mean_train_accuracy <- mean(train_accuracy_values) #train
      mean_valid_accuracy <- mean(valid_accuracy_values) #validation
      
      ##recall
      mean_train_recall <- mean(train_recall_values) #train
      mean_valid_recall <- mean(valid_recall_values) #validation
      

      ##Positive predictive value
      mean_train_ppv <- mean(train_ppv_values) #train
      mean_valid_ppv <- mean(valid_ppv_values) #validation
      
      ##F1-score
      mean_train_f1 <- mean(train_f1_values) #train
      mean_valid_f1 <- mean(valid_f1_values) #validation
      

      ###########################################

      # Store the results
      results <- rbind(results, data.frame(cp = cp_val,
                                           minbucket = minbuck,
                                           minsplit = minspl,
                                           train_accuracy = mean_train_accuracy,
                                           valid_accuracy = mean_valid_accuracy,
                                           train_recall = mean_train_recall,
                                           valid_recall = mean_valid_recall,
                                           train_ppv = mean_train_ppv, valid_ppv = mean_valid_ppv,
                                           train_f1 = mean_train_f1, valid_f1 = mean_valid_f1
                                           ))
    }
  }
}

################################################################################

#rename models
rownames(results) = paste0("BoW tree model ", 1:nrow(results))


#check for optimal model with best train and validation f1-scores
best_results = results %>% filter(train_f1 > 0.5 & valid_f1> 0.5) %>% select(cp, #best = 0.001
                                                              minbucket, #best = 1
                                                              minsplit, #best = 1
                                                        train_f1, valid_f1, train_accuracy, valid_accuracy,
                                                        train_recall, valid_recall, train_ppv, valid_ppv)%>% 
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))


#save as table.png
png("images/tbl_bow_Class_Tree.png")
grid.table(best_results)
dev.off()

################################################################################

# Fit the best tree model to the original training data
set.seed(123)
tree_fit <- rpart(president_label ~ . , df_train,
                             control = rpart.control(minsplit =1,
                                                     minbucket =1 ,
                                                     cp =0.001),
                                                     method="class")

#plot tree
# options(repr.plot.width = 12, repr.plot.height = 10) # set plot size in the notebook
prp(tree_fit, cex=0.4, type=0,col="darkgreen",
    extra=1, #display number of observations for each terminal node
    roundint=F, #don't round to integers in output
    digits=1) #display 5 decimal places in output

###############################################################################

#predictions on test data
set.seed(123)
fittedtest <- predict(tree_fit, df_test, type = 'class')

#test set confusion matrix
test_conf_mat <- confusionMatrix(data=fittedtest, #predicted
                                 reference=as.factor(df_test$president_label), #true
                                 mode = "everything") #all metrics

#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
test_accuracy_values <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the test recall
test_recall_values <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the test positive predictive value or "precision"
test_ppv_values <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
test_f1_values <- round(mean(test_conf_mat$byClass[,'F1']), 3)


```

### 2.2 Random forest model using BoW

```{r, eval=T, include=F}

#empty data frame to store results for each combination of parameters above
results <<- data.frame(ntree = integer(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

#grid search
ntrees_grid = c(100, 500, 1000)

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

###########################################

#gridsearch
for (ntree in ntrees_grid) {
  
  #empty vectors for results at each fold...
  train_accuracy_values <- numeric(num_folds)
  valid_accuracy_values <- numeric(num_folds)
  
  train_recall_values <- numeric(num_folds)
  valid_recall_values <- numeric(num_folds)
  
  train_ppv_values <- numeric(num_folds)
  valid_ppv_values <- numeric(num_folds)
  
  train_f1_values <- numeric(num_folds)
  valid_f1_values <- numeric(num_folds)


 ###########################################

  #for each fold...
  for (fold in 1:num_folds) {
  
    #split data into k-fold train and validation sets
    train_indices <- unlist(folds[-fold]) #all folds except 1 fold
    valid_indices <- unlist(folds[fold]) #only 1 fold
    
    df_train_fold <- df_train[train_indices, ]
    df_valid_fold <- df_train[valid_indices, ]
  
    ###########################################
    
    #fit the model at current fold
    set.seed(123)
    rf_fit <- randomForest(as.factor(president_label) ~ ., data=df_train_fold,
                       ntree = 500,  #no improvement at ntree=200 or ntree= 500
                        importance = TRUE,
                       na.action=na.exclude,
                        do.trace = 25)
  
   ###########################################
  
    # Predict on train data
    set.seed(123)
    fittedtrain <- unname(predict(rf_fit, type = 'class'))
  
    # Train confusion matrix
    train_conf_mat <- confusionMatrix(data=fittedtrain, #predicted
                                      reference=as.factor(df_train_fold$president_label), #true
                                      mode = "everything") #all metrics
  
    #ensure Nan and NA values are 0
    train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
    train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0
  
    # Compute the average train accuracy
    train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)
  
    # Compute the average train recall
    train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)
  
    # Compute the average train positive predictive value or "precision"
    train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)
  
    # Compute the average train F1-score
    train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)
  
    ###########################################
  
    # Predict on validation data
    set.seed(123)
    fittedvalid <- unname(predict(rf_fit, df_valid_fold, type = 'class'))
  
    # validation set confusion matrix
    valid_conf_mat <- confusionMatrix(data=fittedvalid, #fitted
                                      reference=as.factor(df_valid_fold$president_label), #true
                                      mode = "everything") #all metrics
  
    #ensure Nan and NA values are 0
    valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
    valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0
  
    # Compute the average validation accuracy
    valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)
  
    # Compute the average validation  recall
    valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)
  
    # Compute the average validation positive predictive value or "precision"
    valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)
  
    # Compute average validation F1-score
    valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)
  
  }
###########################################
  
  # Calculate the mean results across all folds
  
  ##average accuracy
  mean_train_accuracy <- mean(train_accuracy_values) #train
  mean_valid_accuracy <- mean(valid_accuracy_values) #validation
  
  ##average recall
  mean_train_recall <- mean(train_recall_values) #train
  mean_valid_recall <- mean(valid_recall_values) #validation
  
  ##average PPV
  mean_train_ppv <- mean(train_ppv_values) #train
  mean_valid_ppv <- mean(valid_ppv_values) #validation
  
  ##average F1-score
  mean_train_f1 <- mean(train_f1_values) #train
  mean_valid_f1 <- mean(valid_f1_values) #validation

  
  ###########################################
  
  # Store the results
  results <- rbind(results, data.frame(ntree = ntree, 
                                       train_accuracy = mean_train_accuracy,
                                       valid_accuracy = mean_valid_accuracy,
                                       
                                       train_recall = mean_train_recall,
                                       valid_recall = mean_valid_recall,
                                       
                                       train_ppv = mean_train_ppv, 
                                       valid_ppv = mean_valid_ppv,
                                       
                                       train_f1 = mean_train_f1, 
                                       valid_f1 = mean_valid_f1))
}

################################################################################

#rename models
rownames(results) = paste0("bow_RF_model_", 1:nrow(results))

#check for optimal model with best train and validation f1-scores
results %>% filter(train_f1 > 0.5 & valid_f1> 0.5) %>% select(ntree,
                                                        train_f1, valid_f1)%>%
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))


################################################################################

#re-fit the best model on entire train set
set.seed(123)
rf_fit <- randomForest(as.factor(president_label) ~ ., data=df_train,
                   ntree = 500, #keep it 500 to be consistent with BoW model
                   importance = TRUE,
                   na.action = na.exclude,
                    do.trace = 25)

###############################################################################

#prediction on test data
set.seed(123)
fittedtest <- predict(rf_fit, df_test, type = 'class')

#test confusion matrix
test_conf_mat <- confusionMatrix(data=fittedtest, #predicted data
                                 reference=as.factor(df_test$president_label), #reference data
                                 mode = "everything") #all metrics

#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
test_accuracy_values <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the test recall
test_recall_values <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the test positive predictive value or "precision"
test_ppv_values <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
test_f1_values <- round(mean(test_conf_mat$byClass[,'F1']), 3)

```

### 2.3 Extreme Gradient Boosting using BoW

```{r, eval=T, include=F}

#Grid search
max_depth_grid =c(1, 2, 3, 4, 5, 6) #up to default value = 6
eta_grid = c(0.2, 0.4, 0.6, 1) #ranges from 0 to 1
gamma_grid = c(0.5, 1)


#empty data frame to store results for each combination of parameters above
results <<- data.frame(max_depth = integer(),
                       eta = double(),
                       gamma = double(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

###########################################

#for each parameter combination...
for (md in max_depth_grid) {
  for (e in eta_grid) {
    for (g in gamma_grid) {

      #empty vectors for results at each fold...
      train_accuracy_values <- numeric(num_folds)
      valid_accuracy_values <- numeric(num_folds)

      train_recall_values <- numeric(num_folds)
      valid_recall_values <- numeric(num_folds)

      train_ppv_values <- numeric(num_folds)
      valid_ppv_values <- numeric(num_folds)

      train_f1_values <- numeric(num_folds)
      valid_f1_values <- numeric(num_folds)


################################################

      #for each fold...
      for (fold in 1:num_folds) {


        #split data into k-fold train and validation sets
        train_indices <- unlist(folds[-fold]) #all folds except 1 fold
        valid_indices <- unlist(folds[fold]) #only 1 fold
        
        df_train_fold <- df_train[train_indices, ]
        df_valid_fold <- df_train[valid_indices, ]

        #get train label data
        president_labels = df_train_fold$president_label
        train_fold_label = as.integer(df_train_fold$president_label)-1 #xgboost: numeric data & classes start at 0
        df_train_fold$president_label = NULL #remove label data

        #get validation label data
        president_labels = df_valid_fold$president_label
        valid_fold_label = as.integer(df_valid_fold$president_label)-1 #xgboost: numeric data & classes start at 0
        df_valid_fold$president_label = NULL #remove label data

        #convert dataframes to appropriate inputs for xgboost
        ##validation
        df_valid_fold_xgb = xgb.DMatrix(as.matrix(sapply(df_valid_fold,
                                                         as.numeric)),
                                        label = valid_fold_label,
                                        nthread = 2)
        ##train
        df_train_fold_xgb = xgb.DMatrix(as.matrix(sapply(df_train_fold,
                                                         as.numeric)),
                                        label = train_fold_label,
                                        nthread = 2)



        ###########################################
        
        #fit the model with parameters at the current iteration
        set.seed(123)
        xgb_fit <- xgboost(data=df_train_fold_xgb,
                           label=train_fold_label,
                           max_depth = md, eta = e, gamma= g, lambda = 1,
                           nthread = 2, nrounds = 2,
                           params=list(objective = "multi:softmax",
                                       num_class=4))

       ###########################################

        # Predict on train data
        set.seed(123)
        fittedtrain <- predict(xgb_fit, df_train_fold_xgb, type = 'class')

        #convert numeric fitted values in train data to factors
        fittedtrain = levels(as.factor(df_train$president_label))[fittedtrain+1] #offset by +1 because xgboost starts class at 0

        #convert numeric fitted values in reference data to factors
        ##use df_train factors which is gauranteed to include all classes
        ref = levels(as.factor(df_train$president_label))[train_fold_label+1] #offset by +1 because xgboost starts class at 0

        # train confusion matrix
        train_conf_mat <- confusionMatrix(data=factor(fittedtrain,
                                                         levels=c("Mandela",
                                                                  "Mbeki",
                                                                  "Ramaphosa",
                                                                  "Zuma")),
                                          reference=factor(ref,
                                                         levels=c("Mandela",
                                                                  "Mbeki",
                                                                  "Ramaphosa",
                                                                  "Zuma")),
                                          mode = "everything")

        #ensure Nan and NA values are 0
        train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
        train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0

        # Compute the average train accuracy
        train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)

        # Compute the average train recall
        train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)

        # Compute the average train positive predictive value or "precision"
        train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute the average train F1-score
        train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)


        ###########################################

        # Predict on validation data
        set.seed(123)
        fittedvalid <- predict(xgb_fit, df_valid_fold_xgb, type = 'class')

        #convert numeric fitted values in valid data to factors
        ##use df_train factors which is gauranteed to include all classes
        fittedvalid = levels(as.factor(df_train$president_label))[fittedvalid+1] #offset by +1 because xgboost starts class at 0

        #convert numeric fitted values in reference data to factors
        ##use df_train factors which is gauranteed to include all classes
        ref = levels(as.factor(df_train$president_label))[valid_fold_label+1] #offset by 1 because xgboost starts class at 0

        # validation confusion matrix
        valid_conf_mat <- confusionMatrix(data=factor(fittedvalid,
                                                         levels=c("Mandela",
                                                                  "Mbeki",
                                                                  "Ramaphosa",
                                                                  "Zuma")),
                                          reference=factor(ref,
                                                         levels=c("Mandela",
                                                                  "Mbeki",
                                                                  "Ramaphosa",
                                                                  "Zuma")),
                                          mode = "everything")

        #ensure Nan and NA values are 0
        valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
        valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0

        # Compute the average validation accuracy
        valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)

        # Compute the average validation recall
        valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)

        # Compute the average validation positive predictive value or "precision"
        valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute average validation F1-score
        valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)

      }
      ###########################################

      # Calculate the mean results across all folds
      
      ##accuracy
      mean_train_accuracy <- mean(train_accuracy_values) #train
      mean_valid_accuracy <- mean(valid_accuracy_values) #validation
      
      ##recall
      mean_train_recall <- mean(train_recall_values) #train
      mean_valid_recall <- mean(valid_recall_values) #validation

      ##PPV
      mean_train_ppv <- mean(train_ppv_values) #train
      mean_valid_ppv <- mean(valid_ppv_values) #validation

      ##F1-Score
      mean_train_f1 <- mean(train_f1_values) #train
      mean_valid_f1 <- mean(valid_f1_values) #validation
      

      ###########################################

      # Store the results
      results <- rbind(results, data.frame(max_depth = md,
                       eta = e,
                       gamma = g,
                       train_accuracy = mean_train_accuracy,
                       valid_accuracy = mean_valid_accuracy,
                       train_recall = mean_train_recall, valid_recall = mean_valid_recall,
                       train_ppv = mean_train_ppv, valid_ppv = mean_valid_ppv,
                       train_f1 = mean_train_f1, valid_f1 = mean_valid_f1))
    }
  }
}


################################################################################

#rename models
rownames(results) = paste0("bow_xgb_model_", 1:nrow(results))

#check for optimal model with best train and validation f1-scores
best_results = results %>% filter(train_f1 > 0.4 & valid_f1> 0.4) %>% select(max_depth, eta, gamma, train_f1, valid_f1, 
                                                                             train_accuracy, valid_accuracy,
                                                                             train_recall, valid_recall,
                                                                             train_ppv, valid_ppv)%>%
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))

################################################################################
#re-fit the model on entire train set

#get train label data
president_labels = df_train$president_label
train_label = as.integer(df_train$president_label)-1 #convert to numeric and offset by -1 because xgboost starts classes at 0


#convert dataframes to appropriate inputs for xgboost
df_train_xgb = xgb.DMatrix(as.matrix(sapply(df_train[, -c(150)], #remove label column
                                               as.numeric)),
                              label = train_label,
                              nthread = 2)

#fit model to full train data set
set.seed(123)
xgb_fit <- xgboost(data=df_train_xgb,
                 label=train_label,
                 max_depth = 6, eta = 1, gamma= 0.5, lambda = 1,
                 nthread = 2, nrounds = 2,
                 params=list(objective = "multi:softmax",
                             num_class=4))




# ###############################################################################
# ###############################################################################
#predictions on test set

##get test label data
president_labels = df_test$president_label
test_label = as.integer(df_test$president_label)-1 #convert to numeric and offset by -1 because xgboost starts classes at 0

#true test labels
test_labels = df_test$president_label
test_label = as.integer(as.factor(df_test$president_label))-1

##convert dataframes to appropriate inputs for xgboost
df_test_xgb = xgb.DMatrix(as.matrix(sapply(df_test[, -c(150)], #remove label column
                                               as.numeric)),
                              label = test_label,
                              nthread = 2)

#make predictions on test data
set.seed(123)
fittedtest <- predict(xgb_fit, df_test_xgb, type = 'class')


#convert numeric fitted values in test data to factors
fittedtest = levels(as.factor(df_train$president_label))[fittedtest+1] #offset by 

#convert numeric reference values in reference data to factors
ref = levels(as.factor(df_train$president_label))[test_label+1] #offset by 1 

# validation confusion matrix
test_conf_mat <- confusionMatrix(data=factor(fittedtest,
                                                 levels=c("Mandela",
                                                          "Mbeki",
                                                          "Ramaphosa",
                                                          "Zuma")),
                                  reference=factor(ref,
                                                 levels=c("Mandela",
                                                          "Mbeki",
                                                          "Ramaphosa",
                                                          "Zuma")),
                                  mode = "everything")


#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
all_results[3, "test_accuracy"] <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the test recall
all_results[3, "test_recall"] <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the test positive predictive value or "precision"
# Compute the test positive predictive value or "precision"
all_results[3, "test_ppv"] <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
all_results[3, "test_f1"] <- round(mean(test_conf_mat$byClass[,'F1']), 3)

#model ID of best sub model
#all_results[2, "model_ID"] = rownames(best_results)[1]

#parameters of best sub model
#all_results[1, "parameters"] = rownames(best_results)[1]


```

### 2.4 Naive Bayes model using BoW

```{r, eval=T, include=F}

#laplace values
laplace_grid = seq(0, 1, 0.1)

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

#empty data frame to store results for each combination of parameters above
results <<- data.frame(laplace = double(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())


###########################################
for (lap in laplace_grid) {
  #empty vectors for results at each fold...
  train_accuracy_values <- numeric(num_folds)
  valid_accuracy_values <- numeric(num_folds)

  train_recall_values <- numeric(num_folds)
  valid_recall_values <- numeric(num_folds)

  train_specificity_values <- numeric(num_folds)
  valid_specificity_values <- numeric(num_folds)

  train_ppv_values <- numeric(num_folds)
  valid_ppv_values <- numeric(num_folds)

  train_f1_values <- numeric(num_folds)
  valid_f1_values <- numeric(num_folds)


  ###########################################

  #for each fold...
  for (fold in 1:num_folds) {

    #split data into k-fold train and validation sets
    train_indices <- unlist(folds[-fold]) #all folds except 1 fold
    valid_indices <- unlist(folds[fold]) #only 1 fold
    df_train_fold <- df_train[train_indices, ]
    df_valid_fold <- df_train[valid_indices, ]

    ###########################################
    #fit the model with parameters at the current iteration
    set.seed(123)
    nb_fit <- naiveBayes(president_label ~ ., data=df_train_fold,
                       laplace=lap)

    ###########################################

    # Predict on train data
    set.seed(123)
    fittedtrain <- unname(predict(nb_fit, df_train_fold, type = 'class'))

    # Train confusion matrix
    train_conf_mat <- confusionMatrix(data=fittedtrain, reference=df_train_fold$president_label, mode = "everything")

    #ensure Nan and NA values are 0
    train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
    train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0

    # Compute the train accuracy
    train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)

    # Compute the average train recall
    train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)

    # Compute the train specificity
    train_specificity_values[fold] <- round(mean(train_conf_mat$byClass[,'Specificity']), 3)

    # Compute the average train positive predictive value or "precision"
    train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)

    # Compute the train F1-score
    train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)


    ###########################################

    # Predict on valid data
    set.seed(123)
    fittedvalid <- unname(predict(nb_fit, df_valid_fold, type = 'class'))

    # valid confusion matrix
    valid_conf_mat <- confusionMatrix(data=fittedvalid, reference=df_valid_fold$president_label, mode = "everything")

    #ensure Nan and NA values are 0
    valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
    valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0

    # Compute the valid accuracy
    valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)

    # Compute the average valid recall
    valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)

    # Compute the valid specificity
    valid_specificity_values[fold] <- round(mean(valid_conf_mat$byClass[,'Specificity']), 3)

    # Compute the average valid positive predictive value or "precision"
    valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)

    # Compute the valid F1-score
    valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)

    }
    ###########################################

    # Calculate the mean results across all folds
    mean_train_accuracy <- mean(train_accuracy_values)
    mean_valid_accuracy <- mean(valid_accuracy_values)

    mean_train_recall <- mean(train_recall_values)
    mean_valid_recall <- mean(valid_recall_values)

    mean_train_specificity <- mean(train_specificity_values)
    mean_valid_specificity <- mean(valid_specificity_values)


    mean_train_ppv <- mean(train_ppv_values)
    mean_valid_ppv <- mean(valid_ppv_values)

    mean_train_f1 <- mean(train_f1_values)
    mean_valid_f1 <- mean(valid_f1_values)


    ###########################################

    # Store the results
    results <- rbind(results, data.frame(laplace=lap,
                                     train_accuracy = mean_train_accuracy, valid_accuracy = mean_valid_accuracy,
                                     train_recall = mean_train_recall, valid_recall = mean_valid_recall,
                                     train_specificity = mean_train_specificity, valid_specificity = mean_valid_specificity,
                                     train_ppv = mean_train_ppv, valid_ppv = mean_valid_ppv,
                                     train_f1 = mean_train_f1, valid_f1 = mean_valid_f1))

}


################################################################################

#re-fit the model on entire train set
set.seed(123)
nb_fit <- naiveBayes(president_label ~ ., data=df_train_fold, laplace=0)

###############################################################################

#test data
set.seed(123)
fittedtest <- predict(nb_fit, df_test, type = 'class')

#test confusion matrix
test_conf_mat <- confusionMatrix(data=fittedtest, reference=df_test$president_label, mode = "everything")

#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
test_accuracy_values <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the average test recall
test_recall_values <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the average test positive predictive value or "precision"
test_ppv_values <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
test_f1_values <- round(mean(test_conf_mat$byClass[,'F1']), 3)


```

\newpage

### 2.5 Feed forward neural network using TF-IDF

```{r, eval=T, include=F}

#read in saved results
results = readRDS(file.choose())

# PREPARING THE DATA

# Set random seed
set.seed(104)
tensorflow::set_random_seed(104)


#grid search
units_grid = c(50, 100, 200)
learning_rate_grid = c(0.001, 0.01, 0.1)
dropout_grid = c(0.01, 0.1)

#empty data frame to store results for each combination of parameters above
results <<- data.frame(num_nodes = integer(), 
                      drop_out = double(), 
                      activation_function = character(),
                      learning_rate = double(),
                      L1_regularization = double(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

# Make the target variable numeric classes, starting at 0
bag_of_words_train_target <- as.integer(factor(df_train$president_label)) - 1 #train
bag_of_words_test_target <- as.integer(factor(df_test$president_label)) - 1 #test

#save a copy of original y-data before hot-coding
bag_of_words_train_target_original <- bag_of_words_train_target #train
bag_of_words_test_target_original <- bag_of_words_test_target #test

#hot-coding
bag_of_words_train_target <- to_categorical(bag_of_words_train_target ) #train
bag_of_words_test_target <- to_categorical(bag_of_words_test_target ) #test

#prepare features
bag_of_words_train_features <- df_train %>% subset(select = -c(president_label)) %>% as.matrix()
bag_of_words_test_features <- df_test %>% subset(select = -c(president_label)) %>% as.matrix()

################################################################################

#DEFINE NUMBER OF FOLDS


#Number of folds
num_folds <- 5 #5-fold cross-validation

#Split the data into k-folds
set.seed(104)
folds <- createFolds(df_train$president_label, k = num_folds)

################################################################################

#CONSTRUCTING MODELS USING 5-FOLD CROSS-VALIDATION AND GRID-SEARCH
                      
#for each combination of parameters...
for (num_nodes in units_grid) {
  for (learning_rate in learning_rate_grid) {
    for (dropout in dropout_grid) {

      ###########################################

      #empty vectors for results at each fold...
      
      ##accuracy
      train_accuracy_values <- numeric(num_folds) #train
      valid_accuracy_values <- numeric(num_folds) #validation
      
      ##recall
      train_recall_values <- numeric(num_folds) #train
      valid_recall_values <- numeric(num_folds) #validation
      
      
      ##positive predictive value
      train_ppv_values <- numeric(num_folds) #train
      valid_ppv_values <- numeric(num_folds) #validation
      
      ##F1-score
      train_f1_values <- numeric(num_folds) #train
      valid_f1_values <- numeric(num_folds) #validation


      ###########################################
      
      #for each fold...
      for (fold in 1:num_folds) {
        
        #extract train and validation folds
        train_indices <- unname(unlist(folds[-fold])) #train = all folds except 1 fold
        valid_indices <- unname(unlist(folds[fold])) #validation = 1 fold
        
        #prepare features at each fold 
        x_train <- bag_of_words_train_features[train_indices, ] #train
        x_valid <- bag_of_words_train_features[valid_indices, ] #validation
        
        #prepare target data at each fold
        y_train <- bag_of_words_train_target[train_indices,] #train
        y_valid <- bag_of_words_train_target[valid_indices,] #validation

      ###########################################
        
        #fit the model on the training-fold data
        #with parameters at the current iteration
        
        
        ##initialize model
        set.seed(104)
        nn_model <- keras_model_sequential()
        
        ##define the model
        nn_model %>%
          layer_dense(units = num_nodes, activation = 'tanh', input_shape = c(149), #149 features
                      kernel_regularizer = regularizer_l1(0.01)) %>%
          layer_dropout(rate = dropout) %>%
          layer_dense(units = 4, activation = 'softmax')

        ##compile model
        nn_model %>% compile(
          loss = 'categorical_crossentropy',
          optimizer = optimizer_nadam(learning_rate = learning_rate),
          metrics = c('Recall', 'Precision', 'accuracy'))
      
        

        ##train the model on train-fold data
        set.seed(104)
        history <- nn_model %>% fit(
          x_train, y_train,
          epochs = 30,
          batch_size = 5,
          shuffle = TRUE
          )
        
        
      ###########################################

        ##Predictions on train data
        set.seed(104)
        train_metrics <- nn_model %>% evaluate(x_train, y_train)
        
        # Train confusion matrix
        #train_conf_mat <- table(bag_of_words_train_target_original, train_bag_of_words_fitted)
        
        
        # #ensure Nan and NA values are 0 --> Nans appear when division by 0
        # train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
        # train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0
        
        #train metrics
        ##Train recall
        train_recall <- unname(train_metrics[2])
        
        ##Train precision
        train_ppv <- unname(train_metrics[3])
        
         ##Train accuracy
        train_accuracy <- unname(train_metrics[4])   
        
        ##F1-score
        train_f1_score <- 2 * (train_ppv * train_recall) / (train_ppv + train_recall)


        #append metric to vector containing all folds' metrics
        ##train recall
        train_recall_values[fold] <-  train_recall 

        ##train precision
        train_ppv_values[fold] <-  train_ppv
        
        ##train accuracy
        train_accuracy_values[fold] <-  train_accuracy
        
        ##train f1
        train_f1_values[fold] <-  train_f1_score


        ###########################################

        ##Predictions on valid data
        set.seed(104)
        valid_metrics <- nn_model %>% evaluate(x_valid, y_valid)
        
        # valid confusion matrix
        #valid_conf_mat <- table(bag_of_words_valid_target_original, valid_bag_of_words_fitted)
        
        
        # #ensure Nan and NA values are 0 --> Nans appear when division by 0
        # valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
        # valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0
        
        #valid metrics
        ##valid recall
        valid_recall <- unname(valid_metrics[2])
        
        ##valid precision
        valid_ppv <- unname(valid_metrics[3])
        
         ##valid accuracy
        valid_accuracy <- unname(valid_metrics[4])   
        
        
        ##F1-score
        valid_f1_score <- 2 * (valid_ppv * valid_recall) / (valid_ppv + valid_recall)


        #append metric to vector containing all folds' metrics
        ##valid recall
        valid_recall_values[fold] <-  valid_recall 

        ##valid precision
        valid_ppv_values[fold] <-  valid_ppv
        
        ##valid accuracy
        valid_accuracy_values[fold] <-  valid_accuracy
        
        
        ##valid f1
        valid_f1_values[fold] <-  valid_f1_score

      } #end of current fold
      
      ###########################################

      # Calculate the mean results across all folds
      
      ##Accuracy
      mean_train_accuracy <- mean(train_accuracy_values) #train
      mean_valid_accuracy <- mean(valid_accuracy_values) #validation

      ##recall
      mean_train_recall <- mean(train_recall_values) #train
      mean_valid_recall <- mean(valid_recall_values) #validation


      ##Positive predictive value
      mean_train_ppv <- round(mean(train_ppv_values), 3) #train
      mean_valid_ppv <- round(mean(valid_ppv_values), 3) #validation
      
      ##F1-score
      mean_train_f1 <- round(mean(train_f1_values), 3) #train
      mean_valid_f1 <- round(mean(valid_f1_values), 3) #validation
      

      ###########################################

      # Store the results
      results <- rbind(results, data.frame(num_nodes = c(num_nodes), 
                                           drop_out = c(dropout), 
                                           activation_function = activ,
                                           learning_rate = learning_rate,
                                           L1_regularization = 0.01,
                                           
                                           train_accuracy = mean_train_accuracy,
                                           valid_accuracy = mean_valid_accuracy,
                                           
                                           train_ppv = mean_train_ppv,
                                           valid_ppv = mean_valid_ppv,
                                           
                                           train_recall = mean_train_recall,
                                           valid_recall = mean_valid_recall,
                                           
                                           train_f1 = mean_train_f1, 
                                           valid_f1 = mean_valid_f1))
      }
    }
  }

      

#save results
#saveRDS(results, "data/nn_model_bow_results.rds")

################################################################################

#rename models
rownames(results) = paste0("BoW NN model ", 1:nrow(results))

#check for optimal model with best train and validation f1-scores
best_results_bow_nn = results %>% filter(train_f1 > 0.8 & valid_f1> 0.6) %>% select(num_nodes,
                                           drop_out, 
                                           activation_function,
                                           learning_rate,
                                           L1_regularization,
                                                              drop_out,
                                                              activation_function,
                                                              learning_rate,
                                                              L1_regularization,
                                                        train_f1, valid_f1)%>%
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))  


################################################################################
#rebuild model on entire training set

##Set random seed
set.seed(104)
tensorflow::set_random_seed(104)


##initialize model
set.seed(104)
nn_model <- keras_model_sequential()


##define the model
nn_model %>%
  layer_dense(units = 100, activation = 'relu', input_shape = c(149), #149 features
              kernel_regularizer = regularizer_l1(0.01)) %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 4, activation = 'softmax')

##compile model
nn_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_nadam(learning_rate = 0.01),
  metrics = c('Recall', 'Precision', 'accuracy'))



##train the model on train-fold data
set.seed(104)
history <- nn_model %>% fit(
 bag_of_words_train_features, bag_of_words_train_target,
  epochs = 30,
  batch_size = 5,
  shuffle = TRUE
  )


################################################################################

##Predictions on test data
set.seed(104)
nn_bow_test_metrics <- nn_model %>% evaluate(bag_of_words_test_features, bag_of_words_test_target) 
nn_bow_test_metrics  

#Confusion matrix
set.seed(104)
nn_bow_test_fitted <- nn_model %>% predict(bag_of_words_test_features) %>% k_argmax() %>% as.numeric()
nn_bow_neural_network_conf_mat = table(bag_of_words_test_target_original, nn_bow_test_fitted)


nn_bow_test_ppv = unname(test_metrics[3])
nn_bow_test_recall = unname(test_metrics[2])

##F1-score
nn_bow_test_f1_score <- 2 * (test_ppv * test_recall) / (test_ppv + test_recall)
nn_bow_test_f1_score 

```

\newpage

# C. Term Frequency-Inverse-Document-Frequency (TF-IDF)

```{r, eval=T, include=F}

#caclulate tf-idf
sona_tdf = sona_tdf %>%
  group_by(sentence_id, word)%>% #bind_tf_idf takes in input where rows are unique combinations of file id and word
  bind_tf_idf(word,sentence_id, n) #get tf-idf

################################################################################

#wide format table
tfidf = sona_tdf %>%
  select(sentence_id, president_label, word, tf_idf) %>%
  group_by(sentence_id, president_label)%>%
  pivot_wider(names_from = word,  #columns are words
  values_from = tf_idf, values_fill = 0) 

################################################################################

```

## 1. Splitting data into training and test sets

```{r, eval=T, include=F}


#check for class imbalance
table(tfidf$president_label)

# #number of samples in min class
# min_class_size <- min(table(tfidf$president_label))

#up-sample
set.seed(123)
tfidf = upSample(x = tfidf[, colnames(tfidf) != "president_label"],  # all predictor variables
                      y = as.factor(tfidf$president_label),  # target variable
                      yname = "president_label")

#check for class balance after up-sampling
table(tfidf$president_label)

################################################################################

#70% training data
df_train <- tfidf[training_ids,]

#30% test data
df_test <- tfidf[-training_ids,]

#convert categorical dependent variable to factor
df_train$president_label = as.factor(df_train$president_label) #train
df_test$president_label = as.factor(df_test$president_label) #test

#exclude filename for training models
df_train = subset(df_train, select=-sentence_id)
df_test = subset(df_test, select=-sentence_id)

#check train and test dims
dim(df_train)[1] + dim(df_test)[1] == dim(bag_of_words)[1] #rows
dim(df_train)[2] == dim(df_test)[2] & dim(df_test)[2] == dim(tfidf)[2]-1 #columns, excluding filename column

#check for class balance
table(df_train$president_label)
table(df_test$president_label)

```

## 2. TF-IDF models

### 2.1 Classification tree model using TF-IDF

```{r, eval=T, include=F}

# Define the parameters for grid search
cp_grid <- seq(0.001, 1, length=10)
minbucket_grid <- seq(1, 10, 1)
minsplit_grid <- seq(1, 10, 1)

#empty data frame to store results for each combination of parameters above
results <<- data.frame(cp = double(), minbucket = integer(), minsplit = integer(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

#for each combination of parameters...
for (cp_val in cp_grid) {
  for (minbuck in minbucket_grid) {
    for (minspl in minsplit_grid) {

      ###########################################

      #empty vectors for results at each fold...
      train_accuracy_values <- numeric(num_folds)
      valid_accuracy_values <- numeric(num_folds)

      train_recall_values <- numeric(num_folds)
      valid_recall_values <- numeric(num_folds)

      train_ppv_values <- numeric(num_folds)
      valid_ppv_values <- numeric(num_folds)

      train_f1_values <- numeric(num_folds)
      valid_f1_values <- numeric(num_folds)


      ###########################################
      #for each fold...
      for (fold in 1:num_folds) {

        #split data into k-fold train and validation sets
        train_indices <- unlist(folds[-fold]) #all folds except 1 fold
        valid_indices <- unlist(folds[fold]) #only 1 fold
        
        df_train_fold <- df_train[train_indices, ]
        df_valid_fold <- df_train[valid_indices, ]

      ###########################################
        
        #fit the model on the training-fold data
        #with parameters at the current iteration
        set.seed(123)
        tree_fit <- rpart(president_label ~ ., df_train_fold,
                          control = rpart.control(minsplit = minspl,
                                                  minbucket = minbuck,
                                                  cp = cp_val),
                          method="class") #classification task

      ###########################################

        # Predict on train data
        set.seed(123)
        fittedtrain <- unname(predict(tree_fit, type = 'class'))

        # Train confusion matrix
        train_conf_mat <- confusionMatrix(data=fittedtrain, #predicted
                                          reference=as.factor(df_train_fold$president_label), #true
                                          mode = "everything") #all metrics

        #ensure Nan and NA values are 0 --> Nans appear when division by 0
        train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
        train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0

        # Compute the average train accuracy
        train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)

        # Compute the average train recall
        train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)

        # Compute the average train positive predictive value or "precision"
        train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute the average train F1-score
        train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)


        ###########################################

        # Predict on validation set
        set.seed(123)
        fittedvalid <- unname(predict(tree_fit, df_valid_fold, type = 'class'))

        # validation set confusion matrix
        valid_conf_mat <- confusionMatrix(data=fittedvalid, #predicted
                                          reference=as.factor(df_valid_fold$president_label), #true
                                          mode = "everything") #all metrics

        #ensure Nan and NA values are 0
        valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
        valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0

        # Compute the average validation accuracy
        valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)

        # Compute the average validation recall
        valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)

        # Compute the average validation positive predictive value or "precision"
        valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute the average validation F1-score
        valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)

      }
      ###########################################

      # Calculate the mean results across all folds
      
      ##Accuracy
      mean_train_accuracy <- mean(train_accuracy_values) #train
      mean_valid_accuracy <- mean(valid_accuracy_values) #validation
      
      ##recall
      mean_train_recall <- mean(train_recall_values) #train
      mean_valid_recall <- mean(valid_recall_values) #validation

      ##Positive predictive value
      mean_train_ppv <- mean(train_ppv_values) #train
      mean_valid_ppv <- mean(valid_ppv_values) #validation
      
      ##F1-score
      mean_train_f1 <- mean(train_f1_values) #train
      mean_valid_f1 <- mean(valid_f1_values) #validation
      

      ###########################################

      # Store the results
      results <- rbind(results, data.frame(cp = cp_val,
                                           minbucket = minbuck,
                                           minsplit = minspl,
                                           
                                           train_accuracy = mean_train_accuracy,
                                           valid_accuracy = mean_valid_accuracy,
                                           
                                           train_recall = mean_train_recall,
                                           valid_recall = mean_valid_recall,
                                           
                                           train_ppv = mean_train_ppv, 
                                           valid_ppv = mean_valid_ppv,
                                           
                                           train_f1 = mean_train_f1, 
                                           valid_f1 = mean_valid_f1))
    }
  }
}

################################################################################

#rename models
rownames(results) = paste0("tfidf_tree_model_", 1:nrow(results))

#check for optimal model with best train and validation f1-scores
results %>% filter(train_f1 > 0.5 & valid_f1> 0.5) %>% select(cp,
                                                              minbucket, minsplit,
                                                        train_f1, valid_f1)%>%
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))

################################################################################

# Fit the best tree model to the original training data
set.seed(123)
tree_fit <- rpart(president_label ~ . , df_train,
                             control = rpart.control(minsplit =1,
                                                     minbucket =1 ,
                                                     cp =0.001),
                                                     method="class")

#plot tree
# options(repr.plot.width = 12, repr.plot.height = 10) # set plot size in the notebook
#plot tree
# options(repr.plot.width = 12, repr.plot.height = 10) # set plot size in the notebook
prp(tree_fit, cex=0.4, type=0,col="darkgreen",
    extra=1, #display number of observations for each terminal node
    roundint=F, #don't round to integers in output
    digits=1) #display 5 decimal places in output

###############################################################################

#predictions on test data
set.seed(123)
fittedtest <- predict(tree_fit, df_test[, -c(150)], #remove president_label
                      type = 'class')

#test set confusion matrix
test_conf_mat <- confusionMatrix(data=fittedtest, #predicted
                                 reference=as.factor(df_test$president_label), #true
                                 mode = "everything") #all metrics

#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
test_accuracy_values <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the test recall
test_recall_values <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the test positive predictive value or "precision"
test_ppv_values <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
test_f1_values <- round(mean(test_conf_mat$byClass[,'F1']), 3)


```

### 2.2 Random forest model using TF-IDF

```{r, eval=T, include=F}

#empty data frame to store results for each combination of parameters above
results <<- data.frame(ntree = integer(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

#grid search
ntrees_grid = c(100, 500, 1000)

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

###########################################

#gridsearch
for (ntree in ntrees_grid) {
  
  #empty vectors for results at each fold...
  train_accuracy_values <- numeric(num_folds)
  valid_accuracy_values <- numeric(num_folds)
  
  train_recall_values <- numeric(num_folds)
  valid_recall_values <- numeric(num_folds)
  
  train_ppv_values <- numeric(num_folds)
  valid_ppv_values <- numeric(num_folds)
  
  train_f1_values <- numeric(num_folds)
  valid_f1_values <- numeric(num_folds)


 ###########################################

  #for each fold...
  for (fold in 1:num_folds) {
  
    #split data into k-fold train and validation sets
    train_indices <- unlist(folds[-fold]) #all folds except 1 fold
    valid_indices <- unlist(folds[fold]) #only 1 fold
    
    df_train_fold <- df_train[train_indices, ]
    df_valid_fold <- df_train[valid_indices, ]
  
    ###########################################
    
    #fit the model at current fold
    set.seed(123)
    rf_fit <- randomForest(as.factor(president_label) ~ ., data=df_train_fold,
                       ntree = 500,  #no improvement at ntree=200 or ntree= 500
                        importance = TRUE,
                       na.action=na.exclude,
                        do.trace = 25)
  
   ###########################################
  
    # Predict on train data
    set.seed(123)
    fittedtrain <- unname(predict(rf_fit, type = 'class'))
  
    # Train confusion matrix
    train_conf_mat <- confusionMatrix(data=fittedtrain, #predicted
                                      reference=as.factor(df_train_fold$president_label), #true
                                      mode = "everything") #all metrics
  
    #ensure Nan and NA values are 0
    train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
    train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0
  
    # Compute the average train accuracy
    train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)
  
    # Compute the average train recall
    train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)
  
    # Compute the average train positive predictive value or "precision"
    train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)
  
    # Compute the average train F1-score
    train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)
  
    ###########################################
  
    # Predict on validation data
    set.seed(123)
    fittedvalid <- unname(predict(rf_fit, df_valid_fold, type = 'class'))
  
    # validation set confusion matrix
    valid_conf_mat <- confusionMatrix(data=fittedvalid, #fitted
                                      reference=as.factor(df_valid_fold$president_label), #true
                                      mode = "everything") #all metrics
  
    #ensure Nan and NA values are 0
    valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
    valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0
  
    # Compute the average validation accuracy
    valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)
  
    # Compute the average validation  recall
    valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)
  
    # Compute the average validation positive predictive value or "precision"
    valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)
  
    # Compute average validation F1-score
    valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)
  
  }
###########################################
  
  # Calculate the mean results across all folds
  
  ##average accuracy
  mean_train_accuracy <- mean(train_accuracy_values) #train
  mean_valid_accuracy <- mean(valid_accuracy_values) #validation
  
  ##average recall
  mean_train_recall <- mean(train_recall_values) #train
  mean_valid_recall <- mean(valid_recall_values) #validation
  
  ##average PPV
  mean_train_ppv <- mean(train_ppv_values) #train
  mean_valid_ppv <- mean(valid_ppv_values) #validation
  
  ##average F1-score
  mean_train_f1 <- mean(train_f1_values) #train
  mean_valid_f1 <- mean(valid_f1_values) #validation

  
  ###########################################
  
  # Store the results
  results <- rbind(results, data.frame(ntree = ntree, 
                                       train_accuracy = mean_train_accuracy,
                                       valid_accuracy = mean_valid_accuracy,
                                       
                                       train_recall = mean_train_recall,
                                       valid_recall = mean_valid_recall,
                                       
                                       train_ppv = mean_train_ppv, 
                                       valid_ppv = mean_valid_ppv,
                                       
                                       train_f1 = mean_train_f1, 
                                       valid_f1 = mean_valid_f1))
}

################################################################################

#rename models
rownames(results) = paste0("TFIDF_RF_model_", 1:nrow(results))

#check for optimal model with best train and validation f1-scores
results %>% filter(train_f1 > 0.5 & valid_f1> 0.5) %>% select(ntree,
                                                        train_f1, valid_f1)%>%
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))


################################################################################

#re-fit the best model on entire train set
set.seed(123)
rf_fit <- randomForest(as.factor(president_label) ~ ., data=df_train,
                   ntree = 500,
                   importance = TRUE,
                   na.action = na.exclude,
                    do.trace = 25)

###############################################################################

#prediction on test data
set.seed(123)
fittedtest <- predict(rf_fit, df_test, type = 'class')

#test confusion matrix
test_conf_mat <- confusionMatrix(data=fittedtest, #predicted data
                                 reference=as.factor(df_test$president_label), #reference data
                                 mode = "everything") #all metrics

#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
test_accuracy_values <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the test recall
test_recall_values <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the test positive predictive value or "precision"
test_ppv_values <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
test_f1_values <- round(mean(test_conf_mat$byClass[,'F1']), 3)

```

### 2.3 Extreme Gradient Boosting using TF-IDF

```{r, eval=T, include=F}

#Grid search
max_depth_grid =c(1, 2, 3, 4, 5, 6) #up to default value = 6
eta_grid = c(0.2, 0.4, 0.6, 1) #ranges from 0 to 1
gamma_grid = c(0.5, 1)


#empty data frame to store results for each combination of parameters above
results <<- data.frame(max_depth = integer(),
                       eta = double(),
                       gamma = double(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

###########################################

#for each parameter combination...
for (md in max_depth_grid) {
  for (e in eta_grid) {
    for (g in gamma_grid) {

      #empty vectors for results at each fold...
      train_accuracy_values <- numeric(num_folds)
      valid_accuracy_values <- numeric(num_folds)

      train_recall_values <- numeric(num_folds)
      valid_recall_values <- numeric(num_folds)

      train_ppv_values <- numeric(num_folds)
      valid_ppv_values <- numeric(num_folds)

      train_f1_values <- numeric(num_folds)
      valid_f1_values <- numeric(num_folds)


################################################

      #for each fold...
      for (fold in 1:num_folds) {


        #split data into k-fold train and validation sets
        train_indices <- unlist(folds[-fold]) #all folds except 1 fold
        valid_indices <- unlist(folds[fold]) #only 1 fold
        
        df_train_fold <- df_train[train_indices, ]
        df_valid_fold <- df_train[valid_indices, ]

        #get train label data
        president_labels = df_train_fold$president_label
        train_fold_label = as.integer(df_train_fold$president_label)-1 #xgboost: numeric data & classes start at 0
        df_train_fold$president_label = NULL #remove label data

        #get validation label data
        president_labels = df_valid_fold$president_label
        valid_fold_label = as.integer(df_valid_fold$president_label)-1 #xgboost: numeric data & classes start at 0
        df_valid_fold$president_label = NULL #remove label data

        #convert dataframes to appropriate inputs for xgboost
        ##validation
        df_valid_fold_xgb = xgb.DMatrix(as.matrix(sapply(df_valid_fold,
                                                         as.numeric)),
                                        label = valid_fold_label,
                                        nthread = 2)
        ##train
        df_train_fold_xgb = xgb.DMatrix(as.matrix(sapply(df_train_fold,
                                                         as.numeric)),
                                        label = train_fold_label,
                                        nthread = 2)



        ###########################################
        
        #fit the model with parameters at the current iteration
        set.seed(123)
        xgb_fit <- xgboost(data=df_train_fold_xgb,
                           label=train_fold_label,
                           max_depth = md, eta = e, gamma= g, lambda = 1,
                           nthread = 2, nrounds = 2,
                           params=list(objective = "multi:softmax",
                                       num_class=4))

       ###########################################

        # Predict on train data
        set.seed(123)
        fittedtrain <- predict(xgb_fit, df_train_fold_xgb, type = 'class')

        #convert numeric fitted values in train data to factors
        fittedtrain = levels(as.factor(df_train$president_label))[fittedtrain+1] #offset by +1 because xgboost starts class at 0

        #convert numeric fitted values in reference data to factors
        ##use df_train factors which is gauranteed to include all classes
        ref = levels(as.factor(df_train$president_label))[train_fold_label+1] #offset by +1 because xgboost starts class at 0

        # train confusion matrix
        train_conf_mat <- confusionMatrix(data=factor(fittedtrain,
                                                         levels=c("Mandela",
                                                                  "Mbeki",
                                                                  "Ramaphosa",
                                                                  "Zuma")),
                                          reference=factor(ref,
                                                         levels=c("Mandela",
                                                                  "Mbeki",
                                                                  "Ramaphosa",
                                                                  "Zuma")),
                                          mode = "everything")

        #ensure Nan and NA values are 0
        train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
        train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0

        # Compute the average train accuracy
        train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)

        # Compute the average train recall
        train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)

        # Compute the average train positive predictive value or "precision"
        train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute the average train F1-score
        train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)


        ###########################################

        # Predict on validation data
        set.seed(123)
        fittedvalid <- predict(xgb_fit, df_valid_fold_xgb, type = 'class')

        #convert numeric fitted values in valid data to factors
        ##use df_train factors which is gauranteed to include all classes
        fittedvalid = levels(as.factor(df_train$president_label))[fittedvalid+1] #offset by +1 because xgboost starts class at 0

        #convert numeric fitted values in reference data to factors
        ##use df_train factors which is gauranteed to include all classes
        ref = levels(as.factor(df_train$president_label))[valid_fold_label+1] #offset by 1 because xgboost starts class at 0

        # validation confusion matrix
        valid_conf_mat <- confusionMatrix(data=factor(fittedvalid,
                                                         levels=c("Mandela",
                                                                  "Mbeki",
                                                                  "Ramaphosa",
                                                                  "Zuma")),
                                          reference=factor(ref,
                                                         levels=c("Mandela",
                                                                  "Mbeki",
                                                                  "Ramaphosa",
                                                                  "Zuma")),
                                          mode = "everything")

        #ensure Nan and NA values are 0
        valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
        valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0

        # Compute the average validation accuracy
        valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)

        # Compute the average validation recall
        valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)

        # Compute the average validation positive predictive value or "precision"
        valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)

        # Compute average validation F1-score
        valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)

      }
      ###########################################

      # Calculate the mean results across all folds
      
      ##accuracy
      mean_train_accuracy <- mean(train_accuracy_values) #train
      mean_valid_accuracy <- mean(valid_accuracy_values) #validation
      
      ##recall
      mean_train_recall <- mean(train_recall_values) #train
      mean_valid_recall <- mean(valid_recall_values) #validation

      ##PPV
      mean_train_ppv <- mean(train_ppv_values) #train
      mean_valid_ppv <- mean(valid_ppv_values) #validation

      ##F1-Score
      mean_train_f1 <- mean(train_f1_values) #train
      mean_valid_f1 <- mean(valid_f1_values) #validation
      

      ###########################################

      # Store the results
      results <- rbind(results, data.frame(max_depth = md,
                       eta = e,
                       gamma = g,
                       train_accuracy = mean_train_accuracy,
                       valid_accuracy = mean_valid_accuracy,
                       train_recall = mean_train_recall, valid_recall = mean_valid_recall,
                       train_ppv = mean_train_ppv, valid_ppv = mean_valid_ppv,
                       train_f1 = mean_train_f1, valid_f1 = mean_valid_f1))
    }
  }
}


################################################################################

#rename models
rownames(results) = paste0("tfidf_xgb_model_", 1:nrow(results))

#check for optimal model with best train and validation f1-scores
results %>% filter(train_f1 > 0.4 & valid_f1> 0.4) %>% select(max_depth, eta, gamma,
                                                        train_f1, valid_f1)%>%
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))

################################################################################
#re-fit the model on entire train set

#get train label data
president_labels = df_train$president_label
train_label = as.integer(df_train$president_label)-1 #convert to numeric and offset by -1 because xgboost starts classes at 0


#convert dataframes to appropriate inputs for xgboost
df_train_xgb = xgb.DMatrix(as.matrix(sapply(df_train[, -c(150)], #remove label column
                                               as.numeric)),
                              label = train_label,
                              nthread = 2)

#fit model to full train data set
set.seed(123)
xgb_fit <- xgboost(data=df_train_xgb,
                 label=train_label,
                 max_depth = 5, eta = 1, gamma= 0.5, lambda = 1,
                 nthread = 2, nrounds = 2,
                 params=list(objective = "multi:softmax",
                             num_class=4))




# ###############################################################################
#predictions on test set

##get test label data
president_labels = df_test$president_label
test_label = as.integer(df_test$president_label)-1 #convert to numeric and offset by -1 because xgboost starts classes at 0

president_labels_tfidf = tfidf$president_label
tfidf_label = as.integer(as.factor(tfidf$president_label))-1

##convert dataframes to appropriate inputs for xgboost
df_test_xgb = xgb.DMatrix(as.matrix(sapply(df_test[, -c(150)], #remove label column
                                               as.numeric)),
                              label = test_label,
                              nthread = 2)

# df_tfidf_xgb = xgb.DMatrix(as.matrix(sapply(tfidf[, -c(150, 2)], #remove filename and label column
#                                                as.numeric)),
#                               label = tfidf_label,
#                               nthread = 2)

#make predictions on test data
set.seed(123)
fittedtest <- predict(xgb_fit, df_test_xgb, type = 'class')
# tfidftest = predict(xgb_fit, df_tfidf_xgb, type = 'class')

#convert numeric fitted values in test data to factors
fittedtest = levels(as.factor(df_train$president_label))[fittedtest+1] #offset by +1 because xgboost starts class at 0
# tfidftest = levels(as.factor(tfidf$president_label))[tfidftest+1] #offset by 1 because xgboost starts class at 0

#convert numeric reference values in reference data to factors
ref = levels(as.factor(df_train$president_label))[test_label+1] #offset by 1 because xgboost starts class at 0
# ref = levels(as.factor(tfidf$president_label))[tfidf_label+1] #offset by 1 because xgboost starts class at 0


# validation confusion matrix
test_conf_mat <- confusionMatrix(data=factor(fittedtest,
                                                 levels=c("Mandela",
                                                          "Mbeki",
                                                          "Ramaphosa",
                                                          "Zuma")),
                                  reference=factor(ref,
                                                 levels=c("Mandela",
                                                          "Mbeki",
                                                          "Ramaphosa",
                                                          "Zuma")),
                                  mode = "everything")

# tfidf_conf_mat <- confusionMatrix(data=factor(tfidftest,
#                                                  levels=c("Mandela",
#                                                           "Mbeki",
#                                                           "Ramaphosa",
#                                                           "Zuma")),
#                                   reference=factor(ref,
#                                                  levels=c("Mandela",
#                                                           "Mbeki",
#                                                           "Ramaphosa",
#                                                           "Zuma")),
#                                   mode = "everything")


#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
test_accuracy_values <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the  test recall
test_recall_values <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the test positive predictive value or "precision"
test_ppv_values <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
test_f1_values <- round(mean(test_conf_mat$byClass[,'F1']), 3)


```

### 2.4 Naive Bayes model

```{r, eval=T, include=F}

#laplace values
laplace_grid = seq(0, 1, 0.1)

#number of folds
num_folds <- 5

#create folds
set.seed(123)
folds <- createFolds(df_train$president_label, k = num_folds, list = TRUE)

#empty data frame to store results for each combination of parameters above
results <<- data.frame(laplace = double(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())


###########################################
for (lap in laplace_grid) {
  #empty vectors for results at each fold...
  train_accuracy_values <- numeric(num_folds)
  valid_accuracy_values <- numeric(num_folds)

  train_recall_values <- numeric(num_folds)
  valid_recall_values <- numeric(num_folds)

  train_ppv_values <- numeric(num_folds)
  valid_ppv_values <- numeric(num_folds)

  train_f1_values <- numeric(num_folds)
  valid_f1_values <- numeric(num_folds)


  ###########################################

  #for each fold...
  for (fold in 1:num_folds) {

    #split data into k-fold train and validation sets
    train_indices <- unlist(folds[-fold]) #all folds except 1 fold
    valid_indices <- unlist(folds[fold]) #only 1 fold
    df_train_fold <- df_train[train_indices, ]
    df_valid_fold <- df_train[valid_indices, ]

    ###########################################
    #fit the model with parameters at the current iteration
    set.seed(123)
    nb_fit <- naiveBayes(president_label ~ ., data=df_train_fold,
                       laplace=lap)

    ###########################################

    # Predict on train data
    set.seed(123)
    fittedtrain <- unname(predict(nb_fit, df_train_fold, type = 'class'))

    # Train confusion matrix
    train_conf_mat <- confusionMatrix(data=fittedtrain, reference=df_train_fold$president_label, mode = "everything")

    #ensure Nan and NA values are 0
    train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
    train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0

    # Compute the train accuracy
    train_accuracy_values[fold] <- round(train_conf_mat$overall['Accuracy'], 3)

    # Compute the average train recall
    train_recall_values[fold] <- round(mean(train_conf_mat$byClass[,'Sensitivity']), 3)

    # Compute the average train positive predictive value or "precision"
    train_ppv_values[fold] <- round(mean(train_conf_mat$byClass[,'Pos Pred Value']), 3)

    # Compute the train F1-score
    train_f1_values[fold] <- round(mean(train_conf_mat$byClass[,'F1']), 3)


    ###########################################

    # Predict on valid data
    set.seed(123)
    fittedvalid <- unname(predict(nb_fit, df_valid_fold, type = 'class'))

    # valid confusion matrix
    valid_conf_mat <- confusionMatrix(data=fittedvalid, reference=df_valid_fold$president_label, mode = "everything")

    #ensure Nan and NA values are 0
    valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
    valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0

    # Compute the valid accuracy
    valid_accuracy_values[fold] <- round(valid_conf_mat$overall['Accuracy'], 3)

    # Compute the average valid recall
    valid_recall_values[fold] <- round(mean(valid_conf_mat$byClass[,'Sensitivity']), 3)


    # Compute the average valid positive predictive value or "precision"
    valid_ppv_values[fold] <- round(mean(valid_conf_mat$byClass[,'Pos Pred Value']), 3)

    # Compute the valid F1-score
    valid_f1_values[fold] <- round(mean(valid_conf_mat$byClass[,'F1']), 3)


    }
    ###########################################

    # Calculate the mean results across all folds
    mean_train_accuracy <- mean(train_accuracy_values)
    mean_valid_accuracy <- mean(valid_accuracy_values)

    mean_train_recall <- mean(train_recall_values)
    mean_valid_recall <- mean(valid_recall_values)


    mean_train_ppv <- mean(train_ppv_values)
    mean_valid_ppv <- mean(valid_ppv_values)

    mean_train_f1 <- mean(train_f1_values)
    mean_valid_f1 <- mean(valid_f1_values)


    ###########################################

    # Store the results
    results <- rbind(results, data.frame(laplace=lap,
                                     train_accuracy = mean_train_accuracy, valid_accuracy = mean_valid_accuracy,
                                     train_recall = mean_train_recall, valid_recall = mean_valid_recall,
                                     train_ppv = mean_train_ppv, valid_ppv = mean_valid_ppv,
                                     train_f1 = mean_train_f1, valid_f1 = mean_valid_f1))

}


################################################################################

#re-fit the model on entire train set
set.seed(123)
nb_fit <- naiveBayes(president_label ~ ., data=df_train_fold, laplace=0)

###############################################################################

#test data
set.seed(123)
fittedtest <- predict(nb_fit, df_test[,-150], type = 'class')

#test confusion matrix
test_conf_mat <- confusionMatrix(data=fittedtest, reference=df_test$president_label, mode = "everything")

#ensure Nan and NA values are 0
test_conf_mat$byClass[is.na(test_conf_mat$byClass)] <- 0
test_conf_mat$byClass[is.nan(test_conf_mat$byClass)] <- 0

# Compute the test accuracy
test_accuracy_values <- round(test_conf_mat$overall['Accuracy'], 3)

# Compute the average test recall
test_recall_values <- round(mean(test_conf_mat$byClass[,'Sensitivity']), 3)

# Compute the average test positive predictive value or "precision"
test_ppv_values <- round(mean(test_conf_mat$byClass[,'Pos Pred Value']), 3)

# Compute the test F1-score
test_f1_values <- round(mean(test_conf_mat$byClass[,'F1']), 3)


```

### 2.5 Feed forward neural network using TF-IDF

```{r, eval=T, include=F}
#read in saved results
results = readRDS(file.choose())

# PREPARING THE DATA

# Set random seed
set.seed(104)
tensorflow::set_random_seed(104)


#grid search
units_grid = c(50, 100, 200)
learning_rate_grid = c(0.001, 0.01, 0.1)
dropout_grid = c(0.01, 0.1)


#empty data frame to store results for each combination of parameters above
results <<- data.frame(num_nodes = integer(), 
                      drop_out = double(), 
                      activation_function = character(),
                      learning_rate = double(),
                      L1_regularization = double(),
                      train_accuracy = double(), valid_accuracy = double(),
                      train_recall = double(), valid_recall = double(),
                      train_ppv = double(), valid_ppv = double(),
                      train_f1 = double(), valid_f1 = double())

# Make the target variable numeric classes, starting at 0
tfidf_train_target <- as.integer(factor(df_train$president_label)) - 1 #train
tfidf_test_target <- as.integer(factor(df_test$president_label)) - 1 #test

#save a copy of original y-data before hot-coding
tfidf_train_target_original <- tfidf_train_target #train
tfidf_test_target_original <- tfidf_test_target #test

#hot-coding
tfidf_train_target <- to_categorical(tfidf_train_target ) #train
tfidf_test_target <- to_categorical(tfidf_test_target ) #test

#prepare features
tfidf_train_features <- df_train %>% subset(select = -c(president_label)) %>% as.matrix()
tfidf_test_features <- df_test %>% subset(select = -c(president_label)) %>% as.matrix()

################################################################################

#DEFINE NUMBER OF FOLDS


#Number of folds
num_folds <- 5 #5-fold cross-validation

#Split the data into k-folds
set.seed(104)
folds <- createFolds(df_train$president_label, k = num_folds)

################################################################################

#CONSTRUCTING MODELS USING 5-FOLD CROSS-VALIDATION AND GRID-SEARCH
                      
#for each combination of parameters...
for (num_nodes in units_grid) {
  for (learning_rate in learning_rate_grid) {
    for (dropout in dropout_grid) {
      

      ###########################################

      #empty vectors for results at each fold...
      
      ##accuracy
      train_accuracy_values <- numeric(num_folds) #train
      valid_accuracy_values <- numeric(num_folds) #validation
      
      ##recall
      train_recall_values <- numeric(num_folds) #train
      valid_recall_values <- numeric(num_folds) #validation
      
      
      ##positive predictive value
      train_ppv_values <- numeric(num_folds) #train
      valid_ppv_values <- numeric(num_folds) #validation
      
      ##F1-score
      train_f1_values <- numeric(num_folds) #train
      valid_f1_values <- numeric(num_folds) #validation


      ###########################################
      
      #for each fold...
      for (fold in 1:num_folds) {
        
        #extract train and validation folds
        train_indices <- unname(unlist(folds[-fold])) #train = all folds except 1 fold
        valid_indices <- unname(unlist(folds[fold])) #validation = 1 fold
        
        #prepare features at each fold 
        x_train <- tfidf_train_features[train_indices, ] #train
        x_valid <- tfidf_train_features[valid_indices, ] #validation
        
        #prepare target data at each fold
        y_train <- tfidf_train_target[train_indices,] #train
        y_valid <- tfidf_train_target[valid_indices,] #validation

      ###########################################
        
        #fit the model on the training-fold data
        #with parameters at the current iteration
        
        
        ##initialize model
        set.seed(104)
        nn_model <- keras_model_sequential()
        
        ##define the model
        nn_model %>%
          layer_dense(units = num_nodes, activation = 'tanh', input_shape = c(149), #149 features
                      kernel_regularizer = regularizer_l1(0.01)) %>%
          layer_dropout(rate = dropout) %>%
          layer_dense(units = 4, activation = 'softmax')

        ##compile model
        nn_model %>% compile(
          loss = 'categorical_crossentropy',
          optimizer = optimizer_nadam(learning_rate = learning_rate),
          metrics = c('Recall', 'Precision', 'accuracy'))
      
        

        ##train the model on train-fold data
        set.seed(104)
        history <- nn_model %>% fit(
          x_train, y_train,
          epochs = 30,
          batch_size = 5,
          shuffle = TRUE
          )
        
        
      ###########################################

        ##Predictions on train data
        set.seed(104)
        train_metrics <- nn_model %>% evaluate(x_train, y_train)
        
        # Train confusion matrix
        #train_conf_mat <- table(tfidf_train_target_original, train_tfidf_fitted)
        
        
        # #ensure Nan and NA values are 0 --> Nans appear when division by 0
        # train_conf_mat$byClass[is.na(train_conf_mat$byClass)] <- 0
        # train_conf_mat$byClass[is.nan(train_conf_mat$byClass)] <- 0
        
        #train metrics
        ##Train recall
        train_recall <- unname(train_metrics[2])
        
        ##Train precision
        train_ppv <- unname(train_metrics[3])
        
         ##Train accuracy
        train_accuracy <- unname(train_metrics[4])   
        
        ##F1-score
        train_f1_score <- 2 * (train_ppv * train_recall) / (train_ppv + train_recall)


        #append metric to vector containing all folds' metrics
        ##train recall
        train_recall_values[fold] <-  train_recall 

        ##train precision
        train_ppv_values[fold] <-  train_ppv
        
        ##train accuracy
        train_accuracy_values[fold] <-  train_accuracy
        
        ##train f1
        train_f1_values[fold] <-  train_f1_score


        ###########################################

        ##Predictions on valid data
        set.seed(104)
        valid_metrics <- nn_model %>% evaluate(x_valid, y_valid)
        
        # valid confusion matrix
        #valid_conf_mat <- table(tfidf_valid_target_original, valid_tfidf_fitted)
        
        
        # #ensure Nan and NA values are 0 --> Nans appear when division by 0
        # valid_conf_mat$byClass[is.na(valid_conf_mat$byClass)] <- 0
        # valid_conf_mat$byClass[is.nan(valid_conf_mat$byClass)] <- 0
        
        #valid metrics
        ##valid recall
        valid_recall <- unname(valid_metrics[2])
        
        ##valid precision
        valid_ppv <- unname(valid_metrics[3])
        
         ##valid accuracy
        valid_accuracy <- unname(valid_metrics[4])   
        
        
        ##F1-score
        valid_f1_score <- 2 * (valid_ppv * valid_recall) / (valid_ppv + valid_recall)


        #append metric to vector containing all folds' metrics
        ##valid recall
        valid_recall_values[fold] <-  valid_recall 

        ##valid precision
        valid_ppv_values[fold] <-  valid_ppv
        
        ##valid accuracy
        valid_accuracy_values[fold] <-  valid_accuracy
        
        
        ##valid f1
        valid_f1_values[fold] <-  valid_f1_score

      } #end of current fold
      
      ###########################################

      # Calculate the mean results across all folds
      
      ##Accuracy
      mean_train_accuracy <- mean(train_accuracy_values) #train
      mean_valid_accuracy <- mean(valid_accuracy_values) #validation

      ##recall
      mean_train_recall <- mean(train_recall_values) #train
      mean_valid_recall <- mean(valid_recall_values) #validation


      ##Positive predictive value
      mean_train_ppv <- round(mean(train_ppv_values), 3) #train
      mean_valid_ppv <- round(mean(valid_ppv_values), 3) #validation
      
      ##F1-score
      mean_train_f1 <- round(mean(train_f1_values), 3) #train
      mean_valid_f1 <- round(mean(valid_f1_values), 3) #validation
      

      ###########################################

      # Store the results
      results <- rbind(results, data.frame(num_nodes = c(num_nodes), 
                                           drop_out = c(dropout), 
                                           activation_function = "TanH",
                                           learning_rate = learning_rate,
                                           L1_regularization = 0.01,
                                           
                                           train_accuracy = mean_train_accuracy,
                                           valid_accuracy = mean_valid_accuracy,
                                           
                                           train_ppv = mean_train_ppv,
                                           valid_ppv = mean_valid_ppv,
                                           
                                           train_recall = mean_train_recall,
                                           valid_recall = mean_valid_recall,
                                           
                                           train_f1 = mean_train_f1, 
                                           valid_f1 = mean_valid_f1))
      }
    }
  }

      
saveRDS(results, "data/nn_model_tfidf_resultsss.rds")
#save results
#saveRDS(results, "data/nn_model_tfidf_results.rds")

################################################################################

#rename models
rownames(results) = paste0("tfidf_tree_model_", 1:nrow(results))

#check for optimal model with best train and validation f1-scores
results %>% filter(train_f1 > 0.8 & valid_f1> 0.6) %>% select(num_nodes,
                                           drop_out, 
                                           activation_function,
                                           learning_rate,
                                           L1_regularization,
                                                              drop_out,
                                                              activation_function,
                                                              learning_rate,
                                                              L1_regularization,
                                                        train_f1, valid_f1)%>%
                                                        arrange(desc(train_f1),
                                                                desc(valid_f1))  


################################################################################
#rebuild model on entire training set

##Set random seed
set.seed(104)
tensorflow::set_random_seed(104)


##initialize model
set.seed(104)
nn_model <- keras_model_sequential()

##define the model
nn_model %>%
  layer_dense(units = 50, activation = 'tanh', input_shape = c(149), #149 features
              kernel_regularizer = regularizer_l1(0.01)) %>%
  layer_dropout(rate = 0.01) %>%
  layer_dense(units = 4, activation = 'softmax')

##compile model
nn_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_nadam(learning_rate = 0.01),
  metrics = c('Recall', 'Precision', 'accuracy'))



##train the model on train-fold data
set.seed(104)
history <- nn_model %>% fit(
 tfidf_train_features, tfidf_train_target,
  epochs = 30,
  batch_size = 5,
  shuffle = TRUE
  )


################################################################################

##Predictions on test data
set.seed(104)
test_metrics <- nn_model %>% evaluate(tfidf_test_features, tfidf_test_target) 
test_metrics    

#Confusion matrix
set.seed(104)
y_test_hat <- nn_model %>% predict(tfidf_test_features) %>% k_argmax() %>% as.numeric()
table(tfidf_test_target_original, y_test_hat)


test_ppv = unname(test_metrics[3])
test_recall = unname(test_metrics[2])

##F1-score
test_f1_score <- 2 * (test_ppv * test_recall) / (test_ppv + test_recall)
test_f1_score 

```

