{
  "hash": "c415fc15d9696d3b05c7eba27b09cbda",
  "result": {
    "markdown": "---\ntitle: \"Data Science for Industry Report\" \nsubtitle: \"Assignment 1\"\nauthor: \"Natalie Bianca Alexander\"\nformat: \n  html:\n    page-layout: article\n    grid:\n      margin-width:\t5px\n      gutter-width: 5em\n    embed-resources: true\n    toc: true\n    toc-location: left\n    number-sections: true\n    number-depth: 4\n    theme: journal\nexecute:\n  echo: false\n  cache: true\nbibliography: references.bib\ncsl: data/ieee-with-url.csl\n---\n\n\n\n\n\n\n\n\n# Abstract\n\nText mining refers to the process of transforming unstructured text data into structured clusters of information. This project explores a specific aspect of text mining known as authorship attribution, which involves analyzing linguistic and stylistic features of text to predict its author. This project uses transcription data from South African State of the Nation Address (SONA) speeches, delivered by presidents between 1994 to 2023. The primary goal was to develop a classification model that takes a sentence from a SONA speech as input and correctly predicts the president who said it. Various models were trained, such as Classification Trees, and Random Forests, in addition to XGBoost- , Naïve Bayesian- and Feed Forward Neural Network models. I find that the Neural Net model, with TF-IDF features outperformed all other models with a test F1-score of 0.632 and a test accuracy of 0.639.\n\n# Introduction\n\nText mining is a branch of Artificial Intelligence (AI) that aims to transform unstructured text data into structured formats [@ibm2023]. Text mining employs a variety of statistical and machine learning methods, including deep learning algorithms. These methods are used to uncover textual patterns, trends, and hidden relationships within unstructured data.\n\nModern text mining employs Natural Language Processing (NLP) e.g., part-of-speech tagging [@greenbook2017]. These advancements in text mining are largely attributed to the exponential growth in data, with approximately 80% of global data residing in unstructured formats. This vast amount of data necessitated the use of text mining in fields such as data analytics.\n\nText mining is particularly useful in large organizations where decision-making is central and time is limiting [@ibm2023]. The many applications of text mining in numerous fields have led to the development of several models, including supervised, and unsupervised methods [@dogra2022]. However, the choice of the most effective model remains a complex task.\n\nThe aim of this project is to identify the most effective classification model to predict which president said a specific sentence. The models are trained on transcription data that includes the State of the Nation Address (SONA) speeches delivered by presidents between 1994 to 2023 [@govza2023]. SONA serves as an annual opening to South African Parliament, where the President reports on the socio-economic state of the nation to a joint sitting of Parliament. The main objective is to train models that can take in a SONA sentence and accurately predict the president who said it, with a relatively good test- F1-score and accuracy.\n\n# Literature Review\n\nSeveral studies have used machine learning methods for author classification [@boukhaled2017]. These techniques include Logistic Regression (LR), Random Forests (RF), and Neural Networks (NN). The choice of model depends on the nature of the data and the specific task requirements.\n\nFeature selection plays an important role in author classification, where features are broadly categorized into lexical features (e.g., word usage) and syntactic features (e.g., part-of-speech tags). Some studies have also used semantic features (e.g., topics and sentiments).\n\nAn article by Shukri, (2021) [@shukri2021] trained models on 8109 Arabic opinion articles from 428 authors for the period 2016 to 2021. Their NN model achieved the highest accuracy of 81.1%.\n\nBauersfeld et al., (2023) [@bauersfeld2023] proposed a transformer-based, neural-network architecture that uses text content and author names in the bibliography to determine the author of an anonymous manuscript. The authors used all research papers publicly available on arXiv and achieved a 73% accuracy rate.\n\nA similar paper by Khalid (2021) [@khalid2021] performed author prediction on 210 000 anonymous, news headlines. The study used Bag-of-Words (BoW) and Latent Semantic Analysis (LSA) features as input to train an LR and RF. The study found that the LR model outperformed all other models with an accuracy of 94.9%.\n\nThese papers highlight the variety of author-classification techniques available. However, we also note the challenges in applying these techniques, such as the need for large datasets, as well as the ability of these models to discriminate between content-related features and author-specific features.\n\n# Data\n\n## Data Source and Description\n\nThe SONA dataset is publicly available on the South African government website [@govza2023]. The data contains the speeches delivered by South African presidents at the annual State of the Nation Address (SONA) from 1994 to 2023.\n\nThe following data is available:\n\n-   Mandela (1994-1999) - 7 speeches\n\n-   Mbeki (2000-2008) - 10 speeches\n\n-   Zuma (2009-2017) - 10 speeches\n\n-   Ramaphosa (2018-2023) - 7 speeches\n\n-   Two outliers exist:\n\n    -   deKlerk (1994) - 1 speech\n\n    -   Motlanthe (2009) - 1 speech\n\nThese records cumulatively formed the SONA dataset with 36 records and 5 variables, namely: *filename, speech, year, president* and *date of speech delivered*.\n\n## Data Pre-processing\n\nAll data was read into R *version 4.3.1* , using R-Studio *version 2023.9.1.494*. The year of each speech was extracted from the first four characters in the *filename* column. The president names were extracted from the *filename* column using regular expressions, where alphabetical text ending in a \".txt\" extension was matched as the presidents' name. Subsequently, all other unnecessary text such as \"http\"-, fullstop-, ampersand-, greater-than-, and less-than characters were removed, in addition to trailing white spaces and new-line characters. Dates were then re-formatted into a *dd-mm-yyyy* format. Finally, the pre-processed data was saved as an RDS object for downstream analysis.\n\n# Methods\n\n## Data processing\n\nThe pre-processed data was read into R and converted to a tibble. The data was then assessed by looking at the head and tail of the tibble, in addition to looking at the data types of each column. Dates which appeared before a president's speech were removed using regular expressions. Trailing white spaces before and after a speech were also removed. I also noticed that former president- Motlanthe and deKlerk only had one record and so these observations were removed from the dataset.\n\nAs a result, the processed data had 34 rows and 5 columns.\n\n## Tokenization\n\nThe processed SONA dataset was then tokenized into sentences using unnest_tokens(), since the goal is to make predictions on sentence inputs. All text was then converted to lowercase to remove word redundancy. I also removed all punctuation so that root-words are treated alike*.* I then included a sentence ID column to track sentence membership.\n\nThe sentence tokens were then tokenized into word and bigram tokens respectively, where all stop words were removed. I also ensured that \"blank\" tokens were removed. For the tokenization by bigram implementation, bigrams were first split into individual words, where each word was assessed for stop words. If a stop word was detected, the entire bigram was removed, while the remaining bigrams were unified.\n\n## Exploratory Data Analysis\n\nI looked at the 20 most frequently used- words and bigrams: *(1)* for all presidents, and *(2)* for each president. I also looked at the average number of sentences per speech and the average number of words per sentence for each president.\n\n## Features\n\n### Bag-of-Words Model\n\nThe Bag-of-Words (BoW) model computes the frequency of occurrence of an unordered collection of words within a document and uses these frequencies as features to train the classifier.\n\nA word bag was generated by taking the word tokens and grouping the unique combinations of \"sentence ID, president and word\" and computing each grouping's frequency, after which the top 200 words for each grouping was selected to create the final word bag. The choice of the top 200 words was chosen due to its superior model performance relative to the top 100- and 500 word models (tested informally). The word bag consisted of 363 rows (words) and 3 columns, namely, *sentence ID, president, and word.*\n\nThe BoW table was then constructed by identifying all the words in a sentence that overlap with the word bag. The frequency of each word within a sentence was then calculated. Finally, the BoW table was reformatted to a tidy format, where columns represent the features (149 words), the rows represent the observations (sentence ID) and the cell values are the frequency of a word within a sentence. All words not found in a sentence obtained a value of 0.\n\n### Term Frequency-Inverse Document Frequency Model\n\nTerm Frequency -- Inverse Document Frequency (TF-IDF) refers to the metric that describes how important a word is in a document relative to other documents in the corpus. TF-IDF is calculated by multiplying the Term Frequency (TF) by the Inverse Document Frequency (IDF), where TF is the frequency of a word within a document divided by the total number of words in that document, whereas IDF is the logarithmically scaled inverse fraction of the documents that contain the word. Bind_tf_idf() was used to calculate the TF-IDF for each word in each sentence. It is important to note that the \"document\" here refers to the sentence ID. The BoW table previously discussed was then manipulated to include TF-IDF values instead of word frequencies.\n\n## Class Imbalance and Up-Sampling\n\nI checked for class imbalance by comparing the frequency of records in each of the target variable classes, namely: *Mandela, Mbeki, Zuma* and *Ramaphosa*. I found that the classes were imbalanced, where Mbeki had the largest proportion of sentences (92), followed by Ramaphosa (51), Zuma (41) and Mandela (21). As a result, I used upSample() in the Caret package to oversample the minority classes so that the number of observations in each class matched the majority class.\n\n## Split Balanced Data into Training, Validation and Test sets\n\nI partitioned the data into 70% training and 30% test sets using createDataPartition(), which performs stratified sampling. The target variable, *president* was then converted to factors, where classes: *Mandela, Mbeki, Ramaphosa* and *Zuma* were categorized as levels 1 to 4, respectively. For the validation set, 5-fold cross-validation was applied during training to determine the optimal model parameters by means of a grid-search (discussed below).\n\n## Workflow\n\nEach classification model implemented both the BoW and TF-IDF features discussed above. For each model, a grid search was performed to find the optimal hyperparameters, generating sub-models as a result. For each sub-model, 5-fold cross-validation was performed on the training set. The best sub-model was determined by the model performance on both the training and validation sets. The hyperparameters of the best sub-model was then used to re-train the model on the full training set, after which predictions were made on the test set. The final models were compared based on their test set performance. @fig-1 below shows the general workflow.\n\n\n```{mermaid}\n%%| echo: false \n%%| fig-cap: \"Flow-chart showing the work-flow for model construction, hyperparameter tuning and model selection and testing after splitting the data into 70% training and 30% test sets.\"\n%%| label: fig-1\nflowchart TB\n  A>1. Select Features] --> B[a. Bag-of-Words]\n  A --> D>2. Select Training Model]\n  A --> C[b. Term Frequency-Inverse Document Frequency]\n  D --> E[a. Classification Tree]\n  D --> F[b. Random Forest]\n  D --> J>3. Grid Search using 5-fold cross-validation]\n  D --> G[c. Extreme Gradient Boosting]\n  D --> H[d. Naïve Bayes]\n  D --> I[e. Feed Forward Neural Network]\n  J --> K>4. Choose best sub-model based on train and validation performance]\n  K --> L>5. Rebuild model using the optimal hyperparameters on the full training set]\n  L --> M>6. Predictions on test set and compare test set model performance]\n  \n```\n\n\n::: callout-note\nIn **Figure 1** above, the ribbons are the steps in the process and the rectangles are the choices made at each step.\n:::\n\n## Model Construction\n\n### Classification Tree\n\nClassification trees recursively partition the input space and assigns a class label to each partitioned region based on the majority observation classes in that region. A grid search was performed using the rpart() function with the following parameters:\n\n-   The complexity parameter, a stopping criterion where tree splitting terminates once the reduction in relative error is less than a specified *cp-threshold :*\n\n    **cp** = {0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889, 1.000}\n\n-   The minimum number of observations at any terminal node:\n\n    **minbucket** = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\n-   The minimum number of observations that must exist in a node for a split to be attempted:\n\n    **minsplit** = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\n### Random Forest\n\nRandom Forests use bagging to train multiple decision trees in parallel, reaching a majority vote of class classification. This type of ensemble learning helps to overcome overfitting.\n\nThe randomForest() function was used, taking in the following parameters:\n\n-   The number of trees to grow:\n\n    **ntree** = {100, 500, 1000}\n\n-   Feature importance:\n\n    **Importance** = TRUE\n\n-   **na.action** = na.exclude\n\n### Extreme Gradient Boosting\n\nExtreme Gradient Boosting (XGBoost) models aggregate the output of a sequential ensemble of tree models, where each subsequent model improves on the previous model. The xgboost() function was used, with the following *important* parameters:\n\n-   The maximum depth of the tree, where increasing this value results in a more complex model:\n\n    **max depth** = {1, 2, 3, 4, 5, 6}\n\n-   The step size shrinkage, which is used to prevent overfitting. This value of *eta* shrinks the feature weights to make the boosting process more conservative:\n\n    **eta** = {0.2, 0.4, 0.6, 1}\n\n-   The minimum loss reduction, where the larger the value of *gamma* is, the more conservative the algorithm is:\n\n    **gamma** = {0.5, 1}\n\n-   The L2 regularization term on the weights:\n\n    **lambda** = 1\n\n-   Objective function for multi-class classification:\n\n    **objective** = \"multi:softmax\"\n\nIt is important to note, that the labels of the input data were converted to factors then to integers and finally offset by -1 because xgboost() takes in numeric data where classes of the target variable should be indexed from 0. The features need to be converted to a DMatrix object using the xgb.DMatrix() function. Consequently, I offset the predictions by +1 when computing the confusion matrix, since R indexes from 1. I then had to convert the numeric predictions back to factors, where:\n\n-   0+1: Mandela\n\n-   1+1: Mbeki\n\n-   2+1: Ramaphosa\n\n-   3+1: Zuma\n\n### Naïve Bayes\n\nNaïve Bayes is a generative model that seeks to model the distribution of inputs of a given class. The naiveBayes() function was used, while applying Laplace smoothing to handle zero probabilities in categorical data. Laplace values of **0, 0.1 and 1** were investigated.\n\n### Feed Forward Neural Network\n\nA feed-forward neural network is a type of artificial neural network where the connections between nodes do not form a cycle. Information in this network moves only in one direction, forward, from the input layer through the hidden layers, to the output layer.\n\nA feed-forward neural network was built using Keras and TensorFlow within a Conda environment managed by Anaconda, all executed in RStudio. The target variable (*president*) was extracted from the training data, and converted to factors, after which the factors were converted to discrete integers from 0 to 3, where:\n\n-   0: Mandela\n\n-   1: Mbeki\n\n-   2: Ramaphosa\n\n-   3: Zuma.\n\nSubsequently, the target variable *president* was one hot-coded to a binary representation. The features were then extracted and converted to a matrix format. The following hyperparameters were used:\n\n-     **Input neurons =** {50, 100, 200}\n\n-     **Learning rate =** {0.001, 0.01, 0.1}\n\n-     **Drop-out rate =** {0.01, 0.1}\n\nThe network was built as follows:\n\n-   **Layer 1:** a dense, fully connect layer with *x* input neurons specified above, and which uses either a Tanh or ReLU activation function. This layer expects 149 predictor variables (words).\n\n-   **Layer 2:** a dropout layer to prevent overfitting by randomly selecting a fraction (drop-out rate above) of the input units at each update and setting them to 0 during training time.\n\n-   **Layer 3:** a dense, fully connected layer with 4 output neurons (representing the 4 president classes). A softmax activation function was used to generate a vector of probabilities of class membership for each observation.\n\nThe categorical cross-entropy loss function was minimized which is invariant to shifting of the predicted probabilities. The n-Adam optimizer was used to update the model parameters (weights and biases). This choice of optimizer was based on its superior performance compared to other optimizers, such as SGD, RMSprop, and Adam (informally tested). Finally, the data was trained for 30 epochs, with a batch size of 5, while shuffling the training data at each epoch.\n\n## Model Performance\n\nFor all models, performance on the training, validation and test sets were determined by the metrics discussed below. For 5-fold cross-validation, the best sub-model was determined which had the best mean cross-validation (CV) and training macro-average F1-score. The final model was chosen based on the model with the best test set macro-average F1-score. Other metrics were also considered (see below).\n\nAt each fold, the metric is computed for every class, and the macro-average metric is obtained by averaging these class-specific metrics (**Equation 1**). After training is complete, the final metric is computed as the average of the macro-average metrics over all folds (**Equation 2**).\n\n$$\n\\text{MacroAverageMetric}_{\\text{j}} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{metric}_{i,\\text{j}}\n$$\n\n*...equation 1*, where metric~i, j~ is the metric for class i at a specific fold j , and n is the total number of classes. We see that j = \\[1,5\\], n = 4 and i \\[1,4\\].\n\n$$\n\\text{FinalMetric} = \\frac{1}{k} \\sum_{j=1}^{k} \\text{MacroAverageMetric}_{j}\n$$\n\n*...equation 2*, where MacroAverageMetric~j~ is the macro-average metric at fold j, and k is the total number of folds. We see that k = 5 and j = \\[1, 5\\].\n\n### Accuracy\n\nThe proportion of correct classifications among the total number of classifications.\n\n$$\n\\text{Accuracy} = \\frac{(\\text{TP} + \\text{TN})}{(\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN})}\n$$\n\n*...equation 3,* where TP, TN, FP and FN are the number of true positives, true negatives, false positives, and false negatives, respectively.\n\n### Recall\n\nAlso known as the sensitivity, this is the proportion of actual positives that are correctly classified.\n\n$$\n\\text{Recall} = \\frac{\\text{TP}}{(\\text{TP} + \\text{FN})}\n$$*...equation 4*\n\n### Precision\n\nAlso known as the positive predictive value, this is the proportion of positive predictions that are correctly classified.\n\n$$\n\\text{Precision} = \\frac{\\text{TP}}{(\\text{TP} + \\text{FP})}\n$$*...equation 5*\n\n### F1-score\n\nI chose the F1-score as my main criterion for model performance because the F1-score provides information on the model's ability to capture positive cases (recall) and be accurate with the cases it does capture (precision).\n\n$$\nF1 = 2 \\frac{(\\text{Precision} \\times \\text{Recall})}{(\\text{Precision} + \\text{Recall})}\n$$*...equation 6*\n\n# Results\n\n## Exploratory Data Analysis\n\n### Top 20 most frequently used words and bigrams\n\nIn @fig-2 below we see the top 20 most frequently used- **(a)** words and **(b)** bigrams, among all presidents. The bigrams provide more context relative to the unigrams, and we see that most president's agenda is about the economy with bigrams such as \"economic growth\" and \"job creation\". @fig-3 below shows the top 20 most frequently used words- and @fig-4 below shows the top 20 most frequently used bigrams- for each president. In these figures we see that the agenda is specific to a president or period of presidency, where Mandela has bigrams such as \"people-centered society\" alluding to the end of Apartheid, Mbeki focuses on social justice with bigrams such as \"social security\" and \"social partners\", Zuma mentions the \"world cup\" which alludes to the 2010 soccer world cup hosted in South Africa and Ramaphosa's focus is on crime prevention with bigrams such as \"gender-based violence\" and \"law enforcement\".\n\n\n::: {.cell hash='STA5073Z_Assignment1_cache/html/fig-2_dc588647611d4fa7ca53d08f58199407'}\n::: {.cell-output-display}\n![Bar plots showing the 20 most frequently used (a) words and (b) bigrams used by all presidents](STA5073Z_Assignment1_files/figure-html/fig-2-1.png){#fig-2 width=672}\n:::\n:::\n\n\n<br>\n\n<br>\n\n\n::: {.cell hash='STA5073Z_Assignment1_cache/html/fig-3_1b5c82450e6462837e972928f836e13c'}\n::: {.cell-output-display}\n![Bar plots showing the 20 most frequently used words by each president. ](STA5073Z_Assignment1_files/figure-html/fig-3-1.png){#fig-3 width=672}\n:::\n:::\n\n\n<br>\n\n<br>\n\n\n::: {.cell hash='STA5073Z_Assignment1_cache/html/fig-4_95de6ac1fed6b783a0264a28a2538e5f'}\n::: {.cell-output-display}\n![Bar plots showing the 20 most frequently used bigrams by each president. ](STA5073Z_Assignment1_files/figure-html/fig-4-1.png){#fig-4 width=672}\n:::\n:::\n\n\n<br>\n\n### Average speech and Sentence Length\n\n@fig-5 below shows the **(a)** average speech length per president and **(b)** the average sentence length per president. In @fig-5 **(a)** we see that Ramaphosa has a relatively long average speech length (327 sentences per speech), followed by Zuma (266), Mbeki (242) and Mandela (238). In @fig-5 **(b)** we see that Mbeki has the longest average sentence length (30 words per sentence), followed by Mandela (25) , Ramaphosa (22) and Zuma (19).\n\n\n\n\n::: {.cell hash='STA5073Z_Assignment1_cache/html/fig-5_57d4ee439fd794947fc3d8675dcbb9d1'}\n::: {.cell-output-display}\n![Bar plots showing (a) the average number of sentences per speech and (b) the average number of words per sentence for each president.](STA5073Z_Assignment1_files/figure-html/fig-5-1.png){#fig-5 width=672}\n:::\n:::\n\n\n\n\n<br>\n\n<br>\n\n\n\n\n\n## Bag-of-Word Models\n\n### Classification Trees\n\nIn @tbl-1 below, we see the results of the best performing classification trees for the BoW features.\n\nWe see that model 1 and 2 both have the best mean CV F1-score of 0.546 and a training F1-score of 0.749. These models also have the best mean CV accuracy of 0.565 and a training accuracy of 0.751. These models perform relatively well on the other metrics as well. Model 1 parameters were chosen to re-train the model on the full training set. The results of training can be seen in the BoW tree diagram in @fig-6 .\n\n\n\n\n::: {#tbl-1 .cell tbl-cap='Best Classification Trees, using bag-of-words features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-1_7d6eac6355093669665d0f8d56201ba7'}\n::: {.cell-output-display}\nTable: \n\n|                 |    cp| minbucket| minsplit| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:----------------|-----:|---------:|--------:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|BoW tree model 1 | 0.001|         1|        1|          0.749|               0.546|          0.751|               0.565|        0.751|             0.565|           0.832|                0.649|\n|BoW tree model 2 | 0.001|         1|        2|          0.749|               0.546|          0.751|               0.565|        0.751|             0.565|           0.832|                0.649|\n|BoW tree model 3 | 0.001|         1|        3|          0.743|               0.540|          0.746|               0.561|        0.746|             0.561|           0.827|                0.643|\n|BoW tree model 4 | 0.001|         1|        4|          0.741|               0.540|          0.744|               0.561|        0.744|             0.561|           0.824|                0.643|\n|BoW tree model 5 | 0.001|         1|        5|          0.739|               0.540|          0.742|               0.561|        0.742|             0.561|           0.820|                0.643|\n:::\n:::\n\n\n<br>\n\n### Random Forests\n\nIn @tbl-2 we see that all models performed equally well. All metrics have values above 0.6, suggesting good model fit. The mean CV-F1 score = 0.648 and the training F1-score = 0.641. The training accuracy = 0.639 and mean CV accuracy = 0.65. I chose RF sub-model 2 with *ntrees* = 500 to re-train the full training set, which strikes a balance between too many and too few trees.\n\n\n\n\n::: {#tbl-2 .cell tbl-cap='Best Random Forest models, using bag-of-words features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-2_7be3e644552baf7ebb03ff057a1a00da'}\n::: {.cell-output-display}\nTable: \n\n|               | ntree| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:--------------|-----:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|BoW RF model 1 |   100|          0.641|               0.648|          0.639|                0.65|        0.639|              0.65|           0.682|                0.692|\n|BoW RF model 2 |   500|          0.641|               0.648|          0.639|                0.65|        0.639|              0.65|           0.682|                0.692|\n|BoW RF model 3 |  1000|          0.641|               0.648|          0.639|                0.65|        0.639|              0.65|           0.682|                0.692|\n:::\n:::\n\n\n### Extreme Gradient Boosting\n\n@tbl-3 below suggests that sub-model 47 had the best model performance, and so the model was re-trained on the full training dataset using max depth = 6, eta = 1 and gamma = 0.5. We see that the training F1-score (0.621) and training accuracy (0.62) is relatively good, however the mean CV F1-score (0.426) and mean CV accuracy (0.438) is relatively low . We see a similar trend for all other metrics, where the training performance is good, but the validation performance is poor, suggesting that the model overfits\n\n\n\n\n::: {#tbl-3 .cell tbl-cap='Best XGBoost models, using bag-of-words features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-3_9a05a7fb88a0c9f6d42d42aba07bd736'}\n::: {.cell-output-display}\nTable: \n\n|                 | maxdepth| eta| gamma| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:----------------|--------:|---:|-----:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|BoW XGB model 47 |        6| 1.0|   0.5|          0.621|               0.426|          0.620|               0.438|        0.620|             0.438|           0.721|                0.559|\n|BoW XGB model 48 |        6| 1.0|   1.0|          0.615|               0.427|          0.613|               0.442|        0.613|             0.442|           0.724|                0.563|\n|BoW XGB model 39 |        5| 1.0|   0.5|          0.614|               0.435|          0.612|               0.450|        0.612|             0.450|           0.714|                0.594|\n|BoW XGB model 45 |        6| 0.6|   0.5|          0.609|               0.428|          0.606|               0.439|        0.606|             0.439|           0.727|                0.603|\n|BoW XGB model 40 |        5| 1.0|   1.0|          0.609|               0.435|          0.606|               0.450|        0.606|             0.450|           0.722|                0.597|\n:::\n:::\n\n\n### Naïve Bayes\n\nAs seen in @tbl-4 below, all \"top\" performing models displayed poor performance. These models all displayed the same metric values, with a training F1-score of 0.106 and a mean CV F1-score of 0.1. The training accuracy of 0.253 and mean CV accuracy of 0.25 also suggests poor model performance. I used the simplest model, model 1 with no Laplace smoothing to re-train the model on the full training dataset.\n\n\n\n\n::: {#tbl-4 .cell tbl-cap='Best Naïve Bayesian models, using bag-of-words features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-4_fa649f38c1728b5af21f43080c54aecc'}\n::: {.cell-output-display}\nTable: \n\n|               | Laplace| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:--------------|-------:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|BoW NB model 1 |     0.0|          0.106|                 0.1|          0.253|                0.25|        0.253|              0.25|           0.112|                0.062|\n|BoW NB model 2 |     0.1|          0.106|                 0.1|          0.253|                0.25|        0.253|              0.25|           0.112|                0.062|\n|BoW NB model 3 |     0.2|          0.106|                 0.1|          0.253|                0.25|        0.253|              0.25|           0.112|                0.062|\n|BoW NB model 4 |     0.3|          0.106|                 0.1|          0.253|                0.25|        0.253|              0.25|           0.112|                0.062|\n|BoW NB model 5 |     0.4|          0.106|                 0.1|          0.253|                0.25|        0.253|              0.25|           0.112|                0.062|\n:::\n:::\n\n\n### Feed Forward Neural Network\n\nIn @tbl-5 below, we see the results of the feed forward neural network, where model 55 had the best training F1-score of 0.857 (as well as model 17) and the best mean CV F1-score of 0.593. We also see that model 55 had one of the best training accuracies (0.852) and the best mean CV accuracy (0.612). All other metrics for model 55 are also among the top performing models.\n\n@fig-7 in **Addendum A** shows the change in the Log Loss, training- recall, precision and accuracy over all 30 epochs.\n\n\n\n\n\n\n::: {#tbl-5 .cell tbl-cap='Best Feed Forward Neural Network models, using bag-of-words features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-5_94b5f880c4049d8da801a8e7a5eaff33'}\n::: {.cell-output-display}\nTable: \n\n|                | Input nodes| Drop-out rate|Activation function | Learning rate| L1 regularization| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:---------------|-----------:|-------------:|:-------------------|-------------:|-----------------:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|BoW NN model 55 |         100|          0.10|relu                |          0.01|              0.01|          0.857|               0.593|          0.852|               0.612|        0.823|             0.531|           0.894|                0.677|\n|BoW NN model 17 |         100|          0.01|relu                |          0.01|              0.01|          0.857|               0.571|          0.856|               0.569|        0.819|             0.504|           0.901|                0.669|\n|BoW NN model 5  |          50|          0.01|relu                |          0.01|              0.01|          0.856|               0.597|          0.856|               0.623|        0.817|             0.523|           0.900|                0.702|\n|BoW NN model 65 |         200|          0.01|relu                |          0.01|              0.01|          0.856|               0.596|          0.853|               0.608|        0.813|             0.546|           0.904|                0.657|\n|BoW NN model 19 |         100|          0.10|relu                |          0.01|              0.01|          0.855|               0.586|          0.855|               0.619|        0.812|             0.512|           0.902|                0.693|\n:::\n:::\n\n\n\n\n<br>\n\n## Term Frequency-Inverse Document Frequency Models\n\n\n\n\n\n\n\n### Classification Trees\n\nIn @tbl-6 below, we see that sub-model 1 and 2 had the best mean CV F1-score of 0.583, training F1-score of 0.742, mean CV accuracy of 0.596, and training accuracy of 0.746. In parallel, we see that that all other metrics for sub-models 1 and 2 are among the best performing models. Model 1 parameters were chosen to re-train the model on the full training set. We see interesting clusters in the TF-IDF tree diagram in @fig-8 .\n\n\n\n\n::: {#tbl-6 .cell tbl-cap='Best Classification Trees, using TF-IDF features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-6_2bb0478f6767f340a44e6dff42c95c75'}\n::: {.cell-output-display}\nTable: \n\n|                   |    cp| minbucket| minsplit| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:------------------|-----:|---------:|--------:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|TFIDF tree model 1 | 0.001|         1|        1|          0.742|               0.583|          0.746|               0.596|        0.746|             0.596|           0.817|                0.677|\n|TFIDF tree model 2 | 0.001|         1|        2|          0.742|               0.583|          0.746|               0.596|        0.746|             0.596|           0.817|                0.677|\n|TFIDF tree model 3 | 0.001|         1|        3|          0.738|               0.573|          0.742|               0.589|        0.742|             0.589|           0.814|                0.662|\n|TFIDF tree model 4 | 0.001|         1|        4|          0.738|               0.573|          0.742|               0.589|        0.742|             0.589|           0.814|                0.662|\n|TFIDF tree model 5 | 0.001|         1|        5|          0.738|               0.573|          0.742|               0.589|        0.742|             0.589|           0.814|                0.662|\n:::\n:::\n\n\n<br>\n\n### Random Forests\n\nIn @tbl-7 below we see that the grid-search performed equally well for all *ntree* values. All metrics are relatively good, with values above 0.6, suggesting good model fit. We see that the mean CV-F1 score (0.653) is similar to the training F1-score (0.647). We also see that the mean CV accuracy (0.654) is similar to the training accuracy (0.647). I chose RF sub-model 2 with *ntrees* = 500 to re-train the full training set, which strikes a balance between too many and too few trees.\n\n\n\n\n::: {#tbl-7 .cell tbl-cap='Best Random Forest models, using TF-IDF features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-7_d08828edfcf1ef41d049df5469634119'}\n::: {.cell-output-display}\nTable: \n\n|                 | ntree| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:----------------|-----:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|TFIDF RF model 1 |   100|          0.647|               0.653|          0.647|               0.654|        0.647|             0.654|           0.698|                0.699|\n|TFIDF RF model 2 |   500|          0.647|               0.653|          0.647|               0.654|        0.647|             0.654|           0.698|                0.699|\n|TFIDF RF model 3 |  1000|          0.647|               0.653|          0.647|               0.654|        0.647|             0.654|           0.698|                0.699|\n:::\n:::\n\n\n<br>\n\n### Extreme Gradient Boosting\n\n@tbl-8 below suggests that sub-model 39 has the best model performance, and so this model was re-trained on the full training dataset. We see that the training F1-score (0.637) and training accuracy (0.637) is relatively good, however the mean CV F1-score (0.465) and mean CV accuracy (0.481) is relatively low . We see a similar trend for all other metrics, where the training performance is good, but the validation performance is poor, suggestive of model overfitting.\n\n\n\n\n::: {#tbl-8 .cell tbl-cap='Best XGBoost models, using TF-IDF features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-8_ec8f6fbea20b09b628ab9d0ad21e792b'}\n::: {.cell-output-display}\nTable: \n\n|                   | maxdepth| eta| gamma| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:------------------|--------:|---:|-----:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|TFIDF XGB model 39 |        5| 1.0|   0.5|          0.637|               0.465|          0.637|               0.481|        0.637|             0.481|           0.738|                0.593|\n|TFIDF XGB model 47 |        6| 1.0|   0.5|          0.636|               0.460|          0.637|               0.477|        0.637|             0.477|           0.737|                0.594|\n|TFIDF XGB model 48 |        6| 1.0|   1.0|          0.633|               0.462|          0.635|               0.477|        0.635|             0.477|           0.735|                0.592|\n|TFIDF XGB model 40 |        5| 1.0|   1.0|          0.630|               0.457|          0.630|               0.473|        0.630|             0.473|           0.731|                0.579|\n|TFIDF XGB model 45 |        6| 0.6|   0.5|          0.626|               0.460|          0.625|               0.473|        0.625|             0.473|           0.733|                0.587|\n:::\n:::\n\n\n<br>\n\n### Naïve Bayes\n\nAs seen in @tbl-9 below, all \"top\" performing models displayed poor performance. These models all displayed the same metric values, with a training F1-score of 0.1 equivalent to the mean CV F1-score of 0.1. The training accuracy and mean CV accuracy are both 0.25 suggesting poor model performance. I used the simplest model, model 1 with no Laplace smoothing to re-train the model on the full training dataset.\n\n\n\n\n::: {#tbl-9 .cell tbl-cap='Best Naïve Bayesian models, using TF-IDF features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-9_490087035f4469ec423dfc0bd4929b7a'}\n::: {.cell-output-display}\nTable: \n\n|                 | Laplace| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:----------------|-------:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|TFIDF NB model 1 |     0.0|            0.1|                 0.1|           0.25|                0.25|         0.25|              0.25|           0.063|                0.062|\n|TFIDF NB model 2 |     0.1|            0.1|                 0.1|           0.25|                0.25|         0.25|              0.25|           0.063|                0.062|\n|TFIDF NB model 3 |     0.2|            0.1|                 0.1|           0.25|                0.25|         0.25|              0.25|           0.063|                0.062|\n|TFIDF NB model 4 |     0.3|            0.1|                 0.1|           0.25|                0.25|         0.25|              0.25|           0.063|                0.062|\n|TFIDF NB model 5 |     0.4|            0.1|                 0.1|           0.25|                0.25|         0.25|              0.25|           0.063|                0.062|\n:::\n:::\n\n\n\n\n<br>\n\n### Feed Forward Neural Network\n\nIn @tbl-10 below, we see that model 42 had the best training F1-score of 0.849 and one of the best mean CV F1-scores of 0.637. We also see that model 42 had among the best training accuracies (0.851) and the best validation accuracy (0.650). All other metrics for model 42 are also among the top performing models.\n\n@fig-9 in **Addendum A** shows the change in the Log Loss, training- recall, precision and accuracy over all 30 epochs.\n\n\n\n\n::: {#tbl-10 .cell tbl-cap='Best Feed Forward Neural Network models, using TF-IDF features and sorted in descending order of training and validation F1-score' hash='STA5073Z_Assignment1_cache/html/tbl-10_66336e8f6512fec0275b4175412054d8'}\n::: {.cell-output-display}\nTable: \n\n|                  | Input nodes| Drop-out rate|Activation function | Learning rate| L1 regularization| Train F1-score| Validation F1-score| Train accuracy| Validation accuracy| Train recall| Validation recall| Train precision| Validation precision|\n|:-----------------|-----------:|-------------:|:-------------------|-------------:|-----------------:|--------------:|-------------------:|--------------:|-------------------:|------------:|-----------------:|---------------:|--------------------:|\n|TFIDF NN model 42 |          50|          0.01|tanh                |          0.01|              0.01|          0.849|               0.637|          0.851|               0.650|        0.806|             0.596|           0.897|                0.686|\n|TFIDF NN model 5  |          50|          0.01|relu                |          0.01|              0.01|          0.848|               0.637|          0.853|               0.646|        0.802|             0.600|           0.900|                0.680|\n|TFIDF NN model 41 |          50|          0.01|relu                |          0.01|              0.01|          0.848|               0.632|          0.849|               0.654|        0.805|             0.581|           0.898|                0.704|\n|TFIDF NN model 6  |          50|          0.01|tanh                |          0.01|              0.01|          0.846|               0.639|          0.850|               0.646|        0.804|             0.600|           0.894|                0.686|\n|TFIDF NN model 44 |          50|          0.10|tanh                |          0.01|              0.01|          0.846|               0.635|          0.855|               0.673|        0.803|             0.596|           0.896|                0.680|\n:::\n:::\n\n\n\n\n<br>\n\n## Test Set Model Performance\n\n\n\n\n\nIn @tbl-11 below we see the model performance on the test set. The neural net model 42, using the TF-IDF fetaures outperformed all other models. @fig-10 in **Addendum A** shows the confusion matrix of TF-IDF NN model 42.\n\n\n::: {#tbl-11 .cell tbl-cap='Model performance on the test set when training the best performing models on the full training set' hash='STA5073Z_Assignment1_cache/html/tbl-11_404d189ad76f64adbe9982a2df52fb78'}\n::: {.cell-output-display}\n|Model ID           |Parameters                                                                                    | Test Accuracy| Test-F1| Test Precision| Test Recall|\n|:------------------|:---------------------------------------------------------------------------------------------|-------------:|-------:|--------------:|-----------:|\n|BoW tree model 1   |cp = 0.001, minbucket = 1, minsplit = 1                                                       |         0.500|   0.495|          0.664|       0.500|\n|BoW RF model 2     |ntree=500                                                                                     |         0.602|   0.613|          0.686|       0.602|\n|BoW XGB model 47   |maxdepth = 6, eta = 1, gamma = 0.5                                                            |         0.370|   0.378|          0.596|       0.370|\n|BoW NB model 1     |Laplace0                                                                                      |         0.250|   0.100|          0.062|       0.250|\n|BoW NN model 55    |Input nodes = 50, activation function = 0.01, Learning rate = relu, L1 regularization = 0.001 |         0.593|   0.629|          0.709|       0.565|\n|TFIDF tree model 1 |cp = 0.001, minbucket = 1, minsplit = 1                                                       |         0.463|   0.460|          0.615|       0.463|\n|TFIDF RF model 2   |ntree=500                                                                                     |         0.620|   0.628|          0.696|       0.620|\n|TFIDF XGB model 39 |maxdepth = 5, eta = 1, gamma = 0.5                                                            |         0.417|   0.422|          0.625|       0.417|\n|TFIDF NB model 1   |Laplace=0                                                                                     |         0.250|   0.100|          0.062|       0.250|\n|TFIDF NN model 42  |Input nodes = 50, Learning rate = 0.01, activation function = tanh, L1 regularization = 0.01  |         0.639|   0.632|          0.653|       0.611|\n:::\n:::\n\n\n# Discussion\n\n## Training and Validation Model Performance\n\nFor the BoW @tbl-1 and TF-IDF @tbl-6 classification trees we see that the training performance is generally better than the validation performance, suggestive of model overfitting, where the model does not generalize well to the validation data. We see a similar trend for the XGBoost models ( BoW @tbl-3 and TF-IDF @tbl-8 ).\n\nWe see two interesting clusters in the tree diagrams in @fig-6 (BoW) and @fig-8 (TF-IDF) of **Addendum A**, which suggests that the unigram \"health\" is an important feature in the sentences of Ramaphosa (2018-present), which is in accordance with the Covid-19 pandemic. In addition, \"peace\" is an important feature in the sentences of Mandela (1994-1999), which is in accordance with the end of the Apartheid era (1994).\n\nFor the BoW (@tbl-2) and TF-IDF (@tbl-7) Random Forest models, we see a trend where the validation metrics are similar to the training metrics, where all metrics have a value above 0.6 suggesting good model fit. These models generalize well to the validation data.\n\nThe Naive Bayesian models (BoW, @tbl-4 and TF-IDF, @tbl-9 ) have poor model fit on the training data, and does not generalize well to the validation data.\n\nFor the BoW **(** @tbl-5 and @fig-7 in **Addendum A )** and TF-IDF (@tbl-10 and @fig-9 in **Addendum A)** Neural Network models, we see the log loss decreases to 0, as the number of epochs increase, whereas the training- recall, precision and accuracy values increase towards a maximum value. This suggests that the loss has been minimized and the performance, maximized.\n\n## Test Set Model Performance\n\nThe overall best model (Neural Net model 42, using the TF-IDF features) (@tbl-11), outperformed all other models with a test F1-score of 0.632. This model also had the best test set accuracy (0.639). However, the model's test precision score of 0.653 felt short of the neural net model 55, using BoW features (0.709). In addition, NN model 42 had a test recall of 0.611, which felt short of the random forest model 2 using TF-IDF features, with a recall of 0.620.\n\nThe neural net models and the random forest models for both BoW and TF-IDF generalized well to the unseen, test data, with performance metrics above 0.6 in most cases. This suggests that in most cases these models can correctly predict the president who said a sentence.\n\nThe classification trees and XGBoost models had sub-optimal performance, which suggests that these models do not generalize as well to the unseen, test data. In addition, the Naïve Bayesian models have very poor model fit and do not generalize well to the unseen, test data. These sub-optimal and poorly fit models may be over-fitting the training data. As a result, the model does not \"learn\" but memorizes the characteristics of the training data.\n\nIn general, we also see a trend where in most cases the TF-IDF models outperform the BoW models, which may indicate that word importance is a better feature choice than word frequency.\n\n# Conclusion\n\nIn this comparative analysis of different classifiers and features, the neural net model with TF-IDF features demonstrated the highest performance in terms of test F1-score and accuracy. In addition, the neural net and random forest models also showed good generalization to unseen data. Notably, models trained on TF-IDF features performed better than those using BoW features. In future, one should consider additional features such as the average number of words used in a sentence by each president, along with sentiment and topical features, to potentially improve the model's performance.\n\n# Addendum\n\n\n::: {.cell hash='STA5073Z_Assignment1_cache/html/fig-6_49af049064b3b9700102010f9fb27c38'}\n::: {.cell-output-display}\n![Tree diagram showing results of the best bag-of-words classification sub-model, built on the entire training set after the grid search was performed using 5-fold cross validation.](STA5073Z_Assignment1_files/figure-html/fig-6-1.png){#fig-6 width=672}\n:::\n:::\n\n\n<br>\n\n![BoW Neural Net model 55 trained using the full training data set](images/bowNN.png){#fig-7 fig-align=\"center\" width=\"100%\"}\n\n<br>\n\n\n::: {.cell hash='STA5073Z_Assignment1_cache/html/fig-8_d72715a6945bf4ed23cbc0ff31edbd32'}\n::: {.cell-output-display}\n![Tree diagram showing results of the best TF-IDF classification sub-model, built on the entire training set after the grid search was performed using 5-fold cross validation.](STA5073Z_Assignment1_files/figure-html/fig-8-1.png){#fig-8 width=672}\n:::\n:::\n\n\n<br>\n\n![TF-IDF Neural Net model 42 trained using the full training set](images/tfidfNN.png){#fig-9}\n\n<br>\n\n\n::: {.cell hash='STA5073Z_Assignment1_cache/html/fig-10_60bb4d4eb85547d8cdd15929ad63a1d5'}\n::: {#fig-10 .cell-output-display}\n|   |  0|  1|  2|  3|\n|:--|--:|--:|--:|--:|\n|0  | 19|  8|  0|  0|\n|1  |  3| 15|  1|  8|\n|2  |  2|  9| 15|  1|\n|3  |  4|  3|  0| 20|\n\n\n\nConfusion matrix of the best performing neural net model 42, trained using TF-IDF features. Here the factor levels are shown, where: (0) Mandela, (1) Mbeki, (2) Ramaphosa, (3) Zuma. The rows represent the original classes and the columns represent the fitted classes.\n:::\n:::\n\n\n# References\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}