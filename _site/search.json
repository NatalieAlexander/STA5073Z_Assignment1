[
  {
    "objectID": "STA5073Z_Assignment1.html",
    "href": "STA5073Z_Assignment1.html",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "",
    "text": "[[1]]\n[1] \"stringr\"   \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[7] \"methods\"   \"base\"     \n\n[[2]]\n[1] \"tidytext\"  \"stringr\"   \"stats\"     \"graphics\"  \"grDevices\" \"utils\"    \n[7] \"datasets\"  \"methods\"   \"base\"     \n\n[[3]]\n [1] \"tidyr\"     \"tidytext\"  \"stringr\"   \"stats\"     \"graphics\"  \"grDevices\"\n [7] \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[4]]\n [1] \"lubridate\" \"forcats\"   \"dplyr\"     \"purrr\"     \"readr\"     \"tibble\"   \n [7] \"ggplot2\"   \"tidyverse\" \"tidyr\"     \"tidytext\"  \"stringr\"   \"stats\"    \n[13] \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[5]]\n [1] \"lubridate\" \"forcats\"   \"dplyr\"     \"purrr\"     \"readr\"     \"tibble\"   \n [7] \"ggplot2\"   \"tidyverse\" \"tidyr\"     \"tidytext\"  \"stringr\"   \"stats\"    \n[13] \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[6]]\n [1] \"lubridate\" \"forcats\"   \"dplyr\"     \"purrr\"     \"readr\"     \"tibble\"   \n [7] \"ggplot2\"   \"tidyverse\" \"tidyr\"     \"tidytext\"  \"stringr\"   \"stats\"    \n[13] \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[7]]\n [1] \"lubridate\" \"forcats\"   \"dplyr\"     \"purrr\"     \"readr\"     \"tibble\"   \n [7] \"ggplot2\"   \"tidyverse\" \"tidyr\"     \"tidytext\"  \"stringr\"   \"stats\"    \n[13] \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[8]]\n [1] \"lubridate\" \"forcats\"   \"dplyr\"     \"purrr\"     \"readr\"     \"tibble\"   \n [7] \"ggplot2\"   \"tidyverse\" \"tidyr\"     \"tidytext\"  \"stringr\"   \"stats\"    \n[13] \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n[[9]]\n [1] \"caret\"     \"lattice\"   \"lubridate\" \"forcats\"   \"dplyr\"     \"purrr\"    \n [7] \"readr\"     \"tibble\"    \"ggplot2\"   \"tidyverse\" \"tidyr\"     \"tidytext\" \n[13] \"stringr\"   \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[19] \"methods\"   \"base\"     \n\n[[10]]\n [1] \"wordcloud\"    \"RColorBrewer\" \"caret\"        \"lattice\"      \"lubridate\"   \n [6] \"forcats\"      \"dplyr\"        \"purrr\"        \"readr\"        \"tibble\"      \n[11] \"ggplot2\"      \"tidyverse\"    \"tidyr\"        \"tidytext\"     \"stringr\"     \n[16] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[21] \"methods\"      \"base\"        \n\n[[11]]\n [1] \"rpart\"        \"wordcloud\"    \"RColorBrewer\" \"caret\"        \"lattice\"     \n [6] \"lubridate\"    \"forcats\"      \"dplyr\"        \"purrr\"        \"readr\"       \n[11] \"tibble\"       \"ggplot2\"      \"tidyverse\"    \"tidyr\"        \"tidytext\"    \n[16] \"stringr\"      \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"       \n[21] \"datasets\"     \"methods\"      \"base\"        \n\n[[12]]\n [1] \"tree\"         \"rpart\"        \"wordcloud\"    \"RColorBrewer\" \"caret\"       \n [6] \"lattice\"      \"lubridate\"    \"forcats\"      \"dplyr\"        \"purrr\"       \n[11] \"readr\"        \"tibble\"       \"ggplot2\"      \"tidyverse\"    \"tidyr\"       \n[16] \"tidytext\"     \"stringr\"      \"stats\"        \"graphics\"     \"grDevices\"   \n[21] \"utils\"        \"datasets\"     \"methods\"      \"base\"        \n\n[[13]]\n [1] \"rpart.plot\"   \"tree\"         \"rpart\"        \"wordcloud\"    \"RColorBrewer\"\n [6] \"caret\"        \"lattice\"      \"lubridate\"    \"forcats\"      \"dplyr\"       \n[11] \"purrr\"        \"readr\"        \"tibble\"       \"ggplot2\"      \"tidyverse\"   \n[16] \"tidyr\"        \"tidytext\"     \"stringr\"      \"stats\"        \"graphics\"    \n[21] \"grDevices\"    \"utils\"        \"datasets\"     \"methods\"      \"base\"        \n\n[[14]]\n [1] \"e1071\"        \"rpart.plot\"   \"tree\"         \"rpart\"        \"wordcloud\"   \n [6] \"RColorBrewer\" \"caret\"        \"lattice\"      \"lubridate\"    \"forcats\"     \n[11] \"dplyr\"        \"purrr\"        \"readr\"        \"tibble\"       \"ggplot2\"     \n[16] \"tidyverse\"    \"tidyr\"        \"tidytext\"     \"stringr\"      \"stats\"       \n[21] \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"     \"methods\"     \n[26] \"base\"        \n\n[[15]]\n [1] \"randomForest\" \"e1071\"        \"rpart.plot\"   \"tree\"         \"rpart\"       \n [6] \"wordcloud\"    \"RColorBrewer\" \"caret\"        \"lattice\"      \"lubridate\"   \n[11] \"forcats\"      \"dplyr\"        \"purrr\"        \"readr\"        \"tibble\"      \n[16] \"ggplot2\"      \"tidyverse\"    \"tidyr\"        \"tidytext\"     \"stringr\"     \n[21] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[26] \"methods\"      \"base\"        \n\n[[16]]\n [1] \"gbm\"          \"randomForest\" \"e1071\"        \"rpart.plot\"   \"tree\"        \n [6] \"rpart\"        \"wordcloud\"    \"RColorBrewer\" \"caret\"        \"lattice\"     \n[11] \"lubridate\"    \"forcats\"      \"dplyr\"        \"purrr\"        \"readr\"       \n[16] \"tibble\"       \"ggplot2\"      \"tidyverse\"    \"tidyr\"        \"tidytext\"    \n[21] \"stringr\"      \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"       \n[26] \"datasets\"     \"methods\"      \"base\"        \n\n[[17]]\n [1] \"MLmetrics\"    \"gbm\"          \"randomForest\" \"e1071\"        \"rpart.plot\"  \n [6] \"tree\"         \"rpart\"        \"wordcloud\"    \"RColorBrewer\" \"caret\"       \n[11] \"lattice\"      \"lubridate\"    \"forcats\"      \"dplyr\"        \"purrr\"       \n[16] \"readr\"        \"tibble\"       \"ggplot2\"      \"tidyverse\"    \"tidyr\"       \n[21] \"tidytext\"     \"stringr\"      \"stats\"        \"graphics\"     \"grDevices\"   \n[26] \"utils\"        \"datasets\"     \"methods\"      \"base\"        \n\n[[18]]\n [1] \"xgboost\"      \"MLmetrics\"    \"gbm\"          \"randomForest\" \"e1071\"       \n [6] \"rpart.plot\"   \"tree\"         \"rpart\"        \"wordcloud\"    \"RColorBrewer\"\n[11] \"caret\"        \"lattice\"      \"lubridate\"    \"forcats\"      \"dplyr\"       \n[16] \"purrr\"        \"readr\"        \"tibble\"       \"ggplot2\"      \"tidyverse\"   \n[21] \"tidyr\"        \"tidytext\"     \"stringr\"      \"stats\"        \"graphics\"    \n[26] \"grDevices\"    \"utils\"        \"datasets\"     \"methods\"      \"base\"        \n\n[[19]]\n [1] \"gridExtra\"    \"xgboost\"      \"MLmetrics\"    \"gbm\"          \"randomForest\"\n [6] \"e1071\"        \"rpart.plot\"   \"tree\"         \"rpart\"        \"wordcloud\"   \n[11] \"RColorBrewer\" \"caret\"        \"lattice\"      \"lubridate\"    \"forcats\"     \n[16] \"dplyr\"        \"purrr\"        \"readr\"        \"tibble\"       \"ggplot2\"     \n[21] \"tidyverse\"    \"tidyr\"        \"tidytext\"     \"stringr\"      \"stats\"       \n[26] \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"     \"methods\"     \n[31] \"base\"\n# A tibble: 6 × 5\n  filename                        speech                year  president_13 date \n  &lt;chr&gt;                           &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;\n1 1994_post_elections_Mandela.txt \"24 May 1994  Madame… 1994  Mandela      24-0…\n2 1994_pre_elections_deKlerk.txt  \"28 February 1994  M… 1994  deKlerk      28-0…\n3 1995_Mandela.txt                \"17 February 1995  H… 1995  Mandela      17-0…\n4 1996_Mandela.txt                \"9 February 1996  Ho… 1996  Mandela      9-02…\n5 1997_Mandela.txt                \"7 February 1997  Ma… 1997  Mandela      7-02…\n6 1998_Mandela.txt                \"6 February 1998  Ma… 1998  Mandela      6-02…\n\n\n# A tibble: 6 × 5\n  filename                          speech              year  president_13 date \n  &lt;chr&gt;                             &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;\n1 2019_post_elections_Ramaphosa.txt 20 Jun 2019 State … 2019  Ramaphosa    20-0…\n2 2019_pre_elections_Ramaphosa.txt  7 Feb 2019 Speaker… 2019  Ramaphosa    7-02…\n3 2020_Ramaphosa.txt                13 February 2020  … 2020  Ramaphosa    13-0…\n4 2021_Ramaphosa.txt                11 February 2021  … 2021  Ramaphosa    11-0…\n5 2022_Ramaphosa.txt                Thursday, 10 Febru… 2022  Ramaphosa    10-0…\n6 2023_Ramaphosa.txt                Thursday, 10 Febru… 2023  Ramaphosa    10-0…\n\n\ntibble [36 × 5] (S3: tbl_df/tbl/data.frame)\n $ filename    : chr [1:36] \"1994_post_elections_Mandela.txt\" \"1994_pre_elections_deKlerk.txt\" \"1995_Mandela.txt\" \"1996_Mandela.txt\" ...\n $ speech      : chr [1:36] \"24 May 1994  Madame Speaker and Deputy Speaker, President of the Senate and Deputy President, Deputy Presidents\"| __truncated__ \"28 February 1994  Mr Speaker  This Parliament has convened to adopt important amendments to the Constitution of\"| __truncated__ \"17 February 1995  Honourable President and Deputy President of the Senate, Honourable Speaker and Deputy Speake\"| __truncated__ \"9 February 1996  Honourable President and Deputy President of the Senate; Honourable Speaker and Deputy Speaker\"| __truncated__ ...\n $ year        : chr [1:36] \"1994\" \"1994\" \"1995\" \"1996\" ...\n $ president_13: chr [1:36] \"Mandela\" \"deKlerk\" \"Mandela\" \"Mandela\" ...\n $ date        : chr [1:36] \"24-05-1994\" \"28-02-1994\" \"17-02-1995\" \"9-02-1996\" ...\n\n\n[1] \"filename\"     \"speech\"       \"year\"         \"president_13\" \"date\"        \n\n\n# A tibble: 6 × 2\n# Groups:   president_label [6]\n  president_label     n\n  &lt;chr&gt;           &lt;int&gt;\n1 Mandela             7\n2 Mbeki              10\n3 Motlanthe           1\n4 Ramaphosa           7\n5 Zuma               10\n6 deKlerk             1"
  },
  {
    "objectID": "STA5073Z_Assignment1.html#data-source",
    "href": "STA5073Z_Assignment1.html#data-source",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "4.1 Data Source",
    "text": "4.1 Data Source\nThe data set contains the speeches delivered by South African presidents during the State of the Nation Address (SONA) from 1994 to 2023. The data is publicly available on the South African government website (www.gov.za, 2023)5."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#data-description",
    "href": "STA5073Z_Assignment1.html#data-description",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "4.2 Data Description",
    "text": "4.2 Data Description\nSeven speeches are available for former president Mandela (1994-1999). Ten speeches are available for former president Mbeki (2000-2008). Ten speeches are also available for former president Zuma (2009-2017). President Ramaphosa has a total of seven speeches to date (2018-2023). Two outliers exist, namely one speech for former president deKlerk (1994) and former president Motlanthe (2009).\nThese records cumulatively formed the “sona” data set with 36 records and 5 variables, namely: filename, speech, year, president and date of speech delivered."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#data-pre-processing",
    "href": "STA5073Z_Assignment1.html#data-pre-processing",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "4.3 Data Pre-processing",
    "text": "4.3 Data Pre-processing\nAll data was read into R version 4.3.1 , using R-Studio version 2023.9.1.494. The year of each speech was extracted from the first four characters in the “filename” column. The president names were extracted from the “filename” column using regular expressions, where alphabetical text ending in a “.txt” extension was matched as the presidents’ name. Subsequently, all other unnecessary text such as “http”-, fullstop-, ampersand-, greater-than-, and less-than characters were removed, in addition to trailing white spaces and new-line characters. Dates were then re-formatted into a dd-mm-yyyy format. Finally, the pre-processed data was saved as an RDS object for downstream analysis."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#data-processing",
    "href": "STA5073Z_Assignment1.html#data-processing",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.1 Data processing",
    "text": "5.1 Data processing\nThe pre-processed data was read into R and converted to tibble format. The data was then assessed by looking at the head and tail of the tibble, in addition to looking at the data types of each column. Dates which appeared before a president’s speech were removed using regular expressions. Trailing white spaces before and after a president’s speech were also removed. I also noticed that former president- Motlanthe and deKlerk only had one record in comparison to the other presidents with more than one record, and so these observations were removed from the data set.\nAs a result, the processed data had 34 rows and 5 columns."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#tokenization",
    "href": "STA5073Z_Assignment1.html#tokenization",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.2 Tokenization",
    "text": "5.2 Tokenization\nThe processed sona data set was then tokenized into sentences using unnest_tokens(), since the goal is to make predictions on sentence inputs. All text was then converted to lowercase to remove word redundancy, where each word is treated as its own unique feature, irrespective of letter case. I also removed all punctuation so that root-words are treated alike. I then included a sentence id column to track sentence membership.\nThe sentence tokens were then tokenized into word and bigram tokens respectively, where all stop words were then removed. I also ensured that “blank” tokens were removed. For the tokenization by bigram implementation, bigrams were split into individual words, where each word was assessed for stop words. If a stop word was detected, the entire bigram was removed, while the remaining bigrams were unified."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#exploratory-data-analysis",
    "href": "STA5073Z_Assignment1.html#exploratory-data-analysis",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.3 Exploratory Data Analysis",
    "text": "5.3 Exploratory Data Analysis\nI looked at the 20 most frequently used- words and bigrams: (1) for all presidents, and (2) for each president. I also looked at the average number of sentences per speech for a particular president. I then looked at the average number of words per sentence for a particular president."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#bag-of-words-model",
    "href": "STA5073Z_Assignment1.html#bag-of-words-model",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.4 Bag of Words Model",
    "text": "5.4 Bag of Words Model\nThe machine learning models to be discussed implement the bag-of-words model (BoW). The BoW model computes the frequency of occurrence of an unordered collection of words and uses these frequencies as features to train the classifier.\nA word bag was generated by taking the word tokens and grouping the unique sentence ID-president-word combinations and computing each grouping’s frequency, after which the top 200 words for each grouping was selected to create the final word bag. The choice of the top 200 most frequently occurring words was chosen due to its superior model performance relative to the top 100 and 500 words. The word bag consisted of 363 rows (words) and 3 columns (sentence ID, president, word).\nThe BoW was constructed by identifying all the words in a speech that overlap with the word bag. The frequency of a word within a sentence was then calculated. Finally, the BoW table was reformatted to a wide format (tidy format), where the column names are words (features), the rows are identified by the sentence ID and president name (observations) and the cell values are the frequency of a word within a sentence. All words not found in a sentence obtained a value of 0."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#term-frequency-inverse-document-frequency-tf-idf-model",
    "href": "STA5073Z_Assignment1.html#term-frequency-inverse-document-frequency-tf-idf-model",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.5 Term Frequency Inverse Document Frequency (TF-IDF) Model",
    "text": "5.5 Term Frequency Inverse Document Frequency (TF-IDF) Model\nTerm Frequency – Inverse Document Frequency (TF-IDF) refers to the metric that describes how important a word is in a document relative to other documents in the corpus. The TF-IDF is calculated by multiplying the Term Frequency (TF) by the Inverse Document Frequency (IDF), where TF is frequency of occurrence of a word in a document divided by the total number of words in the document, whereas IDF is the proportion of documents in the corpus that contain the word.\nThe bind_tf_idf() function was used to calculate the TF-IDF for each word in each sentence. It is important to note the “document” here refers to the sentence ID. The BoW table previously discussed was then manipulated to include the TF-IDF instead of the frequency of the word."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#class-imbalance-and-up-sampling",
    "href": "STA5073Z_Assignment1.html#class-imbalance-and-up-sampling",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.6 Class imbalance and up-sampling",
    "text": "5.6 Class imbalance and up-sampling\nI checked for class imbalance by comparing the frequency of records in each of the target variable classes, namely: Mandela, Mbeki, Zuma and Ramaphosa. I found that the classes were imbalanced, where Mbeki had the largest proportion of sentences (92), followed by Ramaphosa (51), Zuma (41) and Mandela (21). As a result, I used upSample() in the Caret package to oversample the minority classes so that the number of observations in each class match that of the majority class."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#split-balanced-data-into-training-validation-and-test-sets",
    "href": "STA5073Z_Assignment1.html#split-balanced-data-into-training-validation-and-test-sets",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.7 Split Balanced Data into Training, Validation and Test sets",
    "text": "5.7 Split Balanced Data into Training, Validation and Test sets\nI partitioned the data into 70% training and 30% test sets using createDataPartition(), which performs stratified sampling. The target variable, president was then converted to factors, where classes: Mandela, Mbeki, Ramaphosa and Zuma were categorized as levels 1 to 4, respectively. For the validation set, 5-fold cross-validation was applied during training, which is discussed below."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#models",
    "href": "STA5073Z_Assignment1.html#models",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.8 Models",
    "text": "5.8 Models\nEach model (a-e below) was implemented for both the BoW and TF-IDF methods discussed above. For each model, a grid search was performed to find the optimal hyperparameters, generating sub-models as a result. For each of these sub-models, 5-fold cross-validation was performed on the training set. The best sub-model was determined by the model performance on both the training and validation sets. The hyperparameters of the best sub-model was then used to rebuild the model on the full training set, after which predictions were made on the test set. The final models were compared based on their test set performance. Figure 1 below shows the general workflow.\n\na. Classification Tree\nClassification trees are decision trees that recursively partition the input space and assign a class label to each partitioned region based on the majority class of the training samples in that region. A grid search was performed using the rpart() function with  the following parameters:\n\nThe complexity parameter, a stopping criterion where tree splitting terminates once the reduction in relative error is less than a specified cp-threshold.\n\ncp = {0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889, 1.000}\n\nThe minimum number of observations at any terminal node.\nminbucket = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\nThe minimum number of observations that must exist in a node for a split to be attempted.\nminsplit= {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\n\n\nb. Random Forest\n\n\n\n\nflowchart TB\n  A&gt;1. Select Features] --&gt; B[a. Bag-of-Words]\n  A --&gt; D&gt;2. Select Training Model]\n  A --&gt; C[b. Term Frequency-Inverse Document Frequency]\n  D --&gt; E[a. Classification Tree]\n  D --&gt; F[b. Random Forest]\n  D --&gt; J&gt;3. Grid Search using 5-fold cross-validation]\n  D --&gt; G[c. Extreme Gradient Boosting]\n  D --&gt; H[d. Naïve Bayes]\n  D --&gt; I[e. Feed Forward Neural Network]\n  J --&gt; K&gt;4. Choose best sub-model based on train and validation performance]\n  K --&gt; L&gt;5. Rebuild model using the optimal hyperparameters on the full training set]\n  L --&gt; M&gt;6. Predictions on test set]\n  \n\n\nFigure 1: Flow-chart showing the work-flow of model construction, hyperparameter tuning and model selection and testing after splitting the data into 70% training and 30% test sets.\n\n\n\n:::{.callout-note}\nIn Figure 1 above, the ribbons are the steps in the process and the rectangles are the choices made at that step."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#performance-metrics",
    "href": "STA5073Z_Assignment1.html#performance-metrics",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.9 Performance Metrics",
    "text": "5.9 Performance Metrics\nFor all models, performance on the training(folds-1), validation(fold), full training and test sets were determined by the metrics discussed below. For 5-fold cross-validation, the best sub-model was determined which had the best validation and training macro-average F1-score. The overall best model was also chosen based on the model with the best test set macro-average F1-score. Other metrics were also considered (see below).\nWhen training a model with 5-fold cross-validation, each metric was determined at each fold for both the training and validation sets. At each fold, the metric is computed for every class, and the macro-average metric is obtained by averaging these class-specific metrics (Equation 1). After training is complete, the final metric is computed as the average of the macro-average metrics over all folds (Equation 2).\n\\[\n\\text{MacroAverageMetric}_{\\text{fold}} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{metric}_{i,\\text{fold}}\n\\]\n…equation 1, where metrici, fold is the metric for class i at a specific fold, and n is the number of classes.\n\\[\n\\text{FinalMetric} = \\frac{1}{k} \\sum_{j=1}^{k} \\text{MacroAverageMetric}_{j}\n\\]\n…equation 2, where MacroAverageMetricj is the macro-average metric at fold j, and k is the total number of folds.\n\na. Accuracy\nThis is the proportion of correct classifications among the total number of classifications. The formula is shown in Equation 3 below:\n\\[\n\\text{Accuracy} = \\frac{(\\text{TP} + \\text{TN})}{(\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN})}\n\\]\n…equation 3, where TP, TN, FP and FN are the number of true positives, true negatives, false positives, and false negatives, respectively found in the confusion matrix.\n\n\nb. Recall\nAlso known as the sensitivity or true positive rate, this is the proportion of actual positives that are correctly classified.\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{(\\text{TP} + \\text{FN})}\n\\]…equation 4\n\n\nc. Precision\nAlso known as the positive predictive value, this is the proportion of positive predictions that are actually correctly classified.\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{(\\text{TP} + \\text{FP})}\n\\]…equation 5\n\n\nd. F1-score\nThis is a measure that combines precision and recall using the harmonic mean, by obtaining a balance of precision and recall.\n\\[\nF1 = 2 \\frac{(\\text{Precision} \\times \\text{Recall})}{(\\text{Precision} + \\text{Recall})}\n\\]…equation 6"
  },
  {
    "objectID": "STA5073Z_Assignment1.html#a.-exploratory-data-analysis",
    "href": "STA5073Z_Assignment1.html#a.-exploratory-data-analysis",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "6.1 a. Exploratory Data Analysis",
    "text": "6.1 a. Exploratory Data Analysis\n\na. Top 20 most frequent words and bigrams\nFigure 2 below shows the top 20 most frequently used words and bigrams among all presidents. The bigrams provide more context relative to the unigrams, and we see that most president’s agenda is about economic growth with bigrams such as “economic growth”, “job creation” and “economic empowerment”. Figure 3 below shows the top 20 most frequently used words for each president and Figure 4 below shows the top 20 most frequently used bigrams for each president. In these figures we see that the agenda is more specific to that year or that president, where Mandela has bigrams such as “people-centered society” alluding to the end of Apartheid, Mbeki focuses on social justice with bigrams such as “social security” and “social partners”, Zuma focuses on the “world cup” which alludes to the 2010 soccer world cup hosted in South Africa and Ramaphosa’s focus is on crime prevention with bigrams such as “gender-based violence” and “law enforcement”.\n\n\n\n\nFigure 2: Bar plots showing the top 20 most frequently used word and bigrams among all presidents\n\n\n\n\n\n\n\n\n\nFigure 3: Bar plots showing the top 20 most frequently used words by each president\n\n\n\n\n\n\n\n\n\nFigure 4: Bar plots showing the top 20 most frequently used bigrams by each president\n\n\n\n\n\nb. Average speech and sentence length\nFigure 5 below shows the average speech length per president (a) and the average sentence length per president (b). In Figure 5 (a) we see that Ramaphosa has a relatively longer average speech length, in other words the average number of sentences within a speech is quite long for Ramaphosa. All other presidents have relatively the same average speech length. In Figure 5 (b) we see that Mbeki has the largest average sentence length (average number of words per sentence), followed by Mandela, Ramaphosa and Zuma.\n\n\n\nFigure 5: Bar plots showing the average speech length per president and the average sentence length per president\n\n\n\n\n\n\nc. Classification Trees\nIn Table 1 below, we see the results of the best performing classification trees for the BoW model, arranged in descending order of training and validation F1-score. We see that"
  },
  {
    "objectID": "templates/templates.html",
    "href": "templates/templates.html",
    "title": "Short Paper",
    "section": "",
    "text": "Please make sure that your manuscript follows the guidelines in the Guide for Authors of the relevant journal. It is not necessary to typeset your manuscript in exactly the same way as an article, unless you are submitting to a camera-ready copy (CRC) journal.\nFor detailed instructions regarding the elsevier article class, see https://www.elsevier.com/authors/policies-and-guidelines/latex-instructions"
  },
  {
    "objectID": "templates/templates.html#using-csl",
    "href": "templates/templates.html#using-csl",
    "title": "Short Paper",
    "section": "Using CSL",
    "text": "Using CSL\nIf cite-method is set to citeproc in elsevier_article(), then pandoc is used for citations instead of natbib. In this case, the csl option is used to format the references. By default, this template will provide an appropriate style, but alternative csl files are available from https://www.zotero.org/styles?q=elsevier. These can be downloaded and stored locally, or the url can be used as in the example header."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "today",
    "section": "",
    "text": "Welcome to my website, where I proudly present my Data Science for Industry course (STA5073Z) project 2023. The primary goal for this project was to construct predictive models that take a sentence of text as input and return a prediction of which South African president was the source of that sentence. ##"
  },
  {
    "objectID": "about.html#natalie-bianca-alexander",
    "href": "about.html#natalie-bianca-alexander",
    "title": "today",
    "section": "Natalie Bianca Alexander",
    "text": "Natalie Bianca Alexander"
  },
  {
    "objectID": "about.html#data",
    "href": "about.html#data",
    "title": "today",
    "section": "Data",
    "text": "Data\nThe data included a corpus of text documents containing the State of the Nation Address (SONA) delivered by the President of South Africa between the years 1994 to 2023."
  },
  {
    "objectID": "preprocessing_sona.html",
    "href": "preprocessing_sona.html",
    "title": "Processing the data",
    "section": "",
    "text": "#1. Libraries\n\n#load libraries\nlibrary(stringr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ purrr     1.0.2\n✔ ggplot2   3.4.3     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n2. Data pre_processing\n\n# read in text data files and organise these into a data frame\nfilenames &lt;- c('1994_post_elections_Mandela.txt', '1994_pre_elections_deKlerk.txt', '1995_Mandela.txt', '1996_Mandela.txt', '1997_Mandela.txt', '1998_Mandela.txt', \n               '1999_post_elections_Mandela.txt', '1999_pre_elections_Mandela.txt', '2000_Mbeki.txt', '2001_Mbeki.txt', '2002_Mbeki.txt', '2003_Mbeki.txt', \n               '2004_post_elections_Mbeki.txt', '2004_pre_elections_Mbeki.txt', '2005_Mbeki.txt', '2006_Mbeki.txt', '2007_Mbeki.txt', '2008_Mbeki.txt', \n               '2009_post_elections_Zuma.txt', '2009_pre_elections_ Motlanthe.txt', '2010_Zuma.txt', '2011_Zuma.txt', '2012_Zuma.txt', '2013_Zuma.txt', \n               '2014_post_elections_Zuma.txt', '2014_pre_elections_Zuma.txt', '2015_Zuma.txt', '2016_Zuma.txt', '2017_Zuma.txt', '2018_Ramaphosa.txt', \n               '2019_post_elections_Ramaphosa.txt', '2019_pre_elections_Ramaphosa.txt', '2020_Ramaphosa.txt', '2021_Ramaphosa.txt', '2022_Ramaphosa.txt', '2023_Ramaphosa.txt')\n\n#empty vector to append data\nthis_speech &lt;- c()\n\n#append speech data to \"this_speech\" vector\nthis_speech[1] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1994_post_elections_Mandela.txt', nchars = 27050)\nthis_speech[2] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1994_pre_elections_deKlerk.txt', nchars = 12786)\nthis_speech[3] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1995_Mandela.txt', nchars = 39019)\nthis_speech[4] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1996_Mandela.txt', nchars = 39524)\nthis_speech[5] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1997_Mandela.txt', nchars = 37489)\nthis_speech[6] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1998_Mandela.txt', nchars = 45247)\nthis_speech[7] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1999_post_elections_Mandela.txt', nchars = 34674)\nthis_speech[8] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/1999_pre_elections_Mandela.txt', nchars = 41225)\nthis_speech[9] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2000_Mbeki.txt', nchars = 37552)\nthis_speech[10] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2001_Mbeki.txt', nchars = 41719)\nthis_speech[11] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2002_Mbeki.txt', nchars = 50544)\nthis_speech[12] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2003_Mbeki.txt', nchars = 58284)\nthis_speech[13] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2004_post_elections_Mbeki.txt', nchars = 34590)\nthis_speech[14] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2004_pre_elections_Mbeki.txt', nchars = 39232)\nthis_speech[15] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2005_Mbeki.txt', nchars = 54635)\nthis_speech[16] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2006_Mbeki.txt', nchars = 48643)\nthis_speech[17] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2007_Mbeki.txt', nchars = 48641)\nthis_speech[18] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2008_Mbeki.txt', nchars = 44907)\nthis_speech[19] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2009_post_elections_Zuma.txt', nchars = 31101)\nthis_speech[20] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2009_pre_elections_Motlanthe.txt', nchars = 47157)\nthis_speech[21] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2010_Zuma.txt', nchars = 26384)\nthis_speech[22] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2011_Zuma.txt', nchars = 33281)\nthis_speech[23] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2012_Zuma.txt', nchars = 33376)\nthis_speech[24] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2013_Zuma.txt', nchars = 36006)\nthis_speech[25] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2014_post_elections_Zuma.txt', nchars = 29403)\nthis_speech[26] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2014_pre_elections_Zuma.txt', nchars = 36233)\nthis_speech[27] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2015_Zuma.txt', nchars = 32860)\nthis_speech[28] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2016_Zuma.txt', nchars = 32464)\nthis_speech[29] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2017_Zuma.txt', nchars = 35981)\nthis_speech[30] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2018_Ramaphosa.txt', nchars = 33290)\nthis_speech[31] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2019_post_elections_Ramaphosa.txt', nchars = 42112)\nthis_speech[32] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2019_pre_elections_Ramaphosa.txt', nchars = 56960)\nthis_speech[33] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2020_Ramaphosa.txt', nchars = 47910)\nthis_speech[34] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2021_Ramaphosa.txt', nchars = 43352)\nthis_speech[35] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2022_Ramaphosa.txt', nchars = 52972)\nthis_speech[36] &lt;- readChar('https://raw.githubusercontent.com/iandurbach/datasci-fi/master/data/sona/2022_Ramaphosa.txt', nchars = 52972)\n\n#convert vector to data frame\nsona &lt;- data.frame(filename = filenames, speech = this_speech, stringsAsFactors = FALSE)\n\n#extract year for each speech\nsona$year &lt;- str_sub(sona$filename, start = 1, end = 4)\n\n#extract president for each speech\nsona$president_13 &lt;- str_remove_all(str_extract(sona$filename, \"[dA-Z].*\\\\.\"), \"\\\\.\")\n\n# clean the sona dataset by adding the date and removing unnecessary text\nreplace_reg &lt;- '(http.*?(\\\\s|.$))|(www.*?(\\\\s|.$))|&amp;|&lt;|&gt;|\\n'\n\n#some processing\nsona &lt;-sona %&gt;%\n  mutate(speech = str_replace_all(speech, replace_reg , ' ')\n         ,date = str_sub(speech, start=1, end=30)\n         ,date = str_replace_all(date, \"February\", \"02\")\n         ,date = str_replace_all(date, \"June\", \"06\")\n         ,date = str_replace_all(date, \"Feb\", \"02\")\n         ,date = str_replace_all(date, \"May\", \"05\")\n         ,date = str_replace_all(date, \"Jun\", \"06\")\n         ,date = str_replace_all(date, \"Thursday, \",\"\")\n         ,date = str_replace_all(date, ' ', '-')        \n         ,date = str_replace_all(date, \"[A-z]\",'')\n         ,date = str_replace_all(date, '-----', '')\n         ,date = str_replace_all(date, '----', '')\n         ,date = str_replace_all(date, '---', '')\n         ,date = str_replace_all(date, '--', '')\n  )\n\n#save as rds object\nsaveRDS(sona, file = \"data//preprocessed_sona.rds\")"
  }
]