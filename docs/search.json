[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Data Science for Industry",
    "section": "",
    "text": "Welcome to my website, where I proudly present my Data Science for Industry course (STA5073Z) project 2023. The primary goal for this project was to construct classification models that can take a sentence of text as input and return a prediction of which South African president was the source of that sentence.  Click here to re-direct to the GitHub repository  Click here to re-direct to the website"
  },
  {
    "objectID": "about.html#natalie-bianca-alexander",
    "href": "about.html#natalie-bianca-alexander",
    "title": "Data Science for Industry",
    "section": "Natalie Bianca Alexander",
    "text": "Natalie Bianca Alexander"
  },
  {
    "objectID": "about.html#data",
    "href": "about.html#data",
    "title": "Data Science for Industry",
    "section": "Data",
    "text": "Data\nThe dataset includes a corpus of text documents containing the State of the Nation Address (SONA) speeches delivered by the President of South Africa between the years 1994 to 2023."
  },
  {
    "objectID": "alxnat003.html",
    "href": "alxnat003.html",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "",
    "text": "Text mining refers to the process of transforming unstructured text data into structured clusters of information. This project explores a specific aspect of text mining known as authorship attribution, which involves analyzing linguistic and stylistic features of text to predict its author. This project uses transcription data from South African State of the Nation Address (SONA) speeches, delivered by presidents between 1994 to 2023. The primary goal was to develop a classification model that takes a sentence from a SONA speech as input and correctly predicts the president who said it. Various models were trained, such as Classification Trees, and Random Forests, in addition to XGBoost- , Naïve Bayesian- and Feed Forward Neural Network models. I find that the neural net model, with TF-IDF features outperformed all other models with a test F1-score of 0.632 and a test accuracy of 0.639."
  },
  {
    "objectID": "alxnat003.html#data-source-and-description",
    "href": "alxnat003.html#data-source-and-description",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "4.1 Data Source and Description",
    "text": "4.1 Data Source and Description\nThe SONA dataset is publicly available on the South African government website [4]. The data contains the speeches delivered by South African presidents at the annual State of the Nation Address (SONA) from 1994 to 2023.\nThe following data is available:\n\nMandela (1994-1999) - 7 speeches\nMbeki (2000-2008) - 10 speeches\nZuma (2009-2017) - 10 speeches\nRamaphosa (2018-2023) - 7 speeches\nTwo outliers exist:\n\ndeKlerk (1994) - 1 speech\nMotlanthe (2009) - 1 speech\n\n\nThese records cumulatively formed the SONA dataset with 36 records and 5 variables, namely: filename, speech, year, president and date of speech delivered."
  },
  {
    "objectID": "alxnat003.html#data-pre-processing",
    "href": "alxnat003.html#data-pre-processing",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "4.2 Data Pre-processing",
    "text": "4.2 Data Pre-processing\nAll data was read into R version 4.3.1 , using R-Studio version 2023.9.1.494. The year of each speech was extracted from the first four characters in the filename column. The president names were extracted from the filename column using regular expressions, where alphabetical text ending in a “.txt” extension was matched as the presidents’ name. Subsequently, all other unnecessary text such as “http”-, fullstop-, ampersand-, greater-than-, and less-than characters were removed, in addition to trailing white spaces and new-line characters. Dates were then re-formatted into a dd-mm-yyyy format. Finally, the pre-processed data was saved as an RDS object for downstream analysis."
  },
  {
    "objectID": "alxnat003.html#data-processing",
    "href": "alxnat003.html#data-processing",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.1 Data processing",
    "text": "5.1 Data processing\nThe pre-processed data was read into R and converted to a tibble. The data was then assessed by looking at the head and tail of the tibble, in addition to looking at the data types of each column. Dates which appeared before a president’s speech were removed using regular expressions. Trailing white spaces before and after a speech were also removed. I also noticed that former president- Motlanthe and deKlerk only had one record and so these observations were removed from the dataset.\nAs a result, the processed data had 34 rows and 5 columns."
  },
  {
    "objectID": "alxnat003.html#tokenization",
    "href": "alxnat003.html#tokenization",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.2 Tokenization",
    "text": "5.2 Tokenization\nThe processed SONA dataset was then tokenized into sentences using unnest_tokens(), since the goal is to make predictions on sentence inputs. All text was then converted to lowercase to remove word redundancy. I also removed all punctuation so that root-words are treated alike. I then included a sentence ID column to track sentence membership.\nThe sentence tokens were then tokenized into word and bigram tokens respectively, where all stop words were removed. I also ensured that “blank” tokens were removed. For the tokenization by bigram implementation, bigrams were first split into individual words, where each word was assessed for stop words. If a stop word was detected, the entire bigram was removed, while the remaining bigrams were unified."
  },
  {
    "objectID": "alxnat003.html#exploratory-data-analysis",
    "href": "alxnat003.html#exploratory-data-analysis",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.3 Exploratory Data Analysis",
    "text": "5.3 Exploratory Data Analysis\nI looked at the 20 most frequently used- words and bigrams: (1) for all presidents, and (2) for each president. I also looked at the average number of sentences per speech and the average number of words per sentence for each president."
  },
  {
    "objectID": "alxnat003.html#features",
    "href": "alxnat003.html#features",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.4 Features",
    "text": "5.4 Features\n\n5.4.1 Bag-of-Words Model\nThe Bag-of-Words (BoW) model computes the frequency of occurrence of an unordered collection of words within a document and uses these frequencies as features to train the classifier.\nA word bag was generated by taking the word tokens and grouping the unique combinations of “sentence ID, president and word” and computing each grouping’s frequency, after which the top 200 words for each grouping was selected to create the final word bag. The choice of the top 200 words was chosen due to its superior model performance relative to the top 100- and 500 word models (tested informally). The word bag consisted of 363 rows (words) and 3 columns, namely, sentence ID, president, and word.\nThe BoW table was then constructed by identifying all the words in a sentence that overlap with the word bag. The frequency of each word within a sentence was then calculated. Finally, the BoW table was reformatted to a tidy format, where columns represent the features (149 words), the rows represent the observations (sentence ID) and the cell values are the frequency of a word within a sentence. All words not found in a sentence obtained a value of 0.\n\n\n5.4.2 Term Frequency-Inverse Document Frequency Model\nTerm Frequency – Inverse Document Frequency (TF-IDF) refers to the metric that describes how important a word is in a document relative to other documents in the corpus. TF-IDF is calculated by multiplying the Term Frequency (TF) by the Inverse Document Frequency (IDF), where TF is the frequency of a word within a document divided by the total number of words in that document, whereas IDF is the logarithmically scaled inverse fraction of the documents that contain the word. Bind_tf_idf() was used to calculate the TF-IDF for each word in each sentence. It is important to note that the “document” here refers to the sentence ID. The BoW table previously discussed was then manipulated to include TF-IDF values instead of word frequencies."
  },
  {
    "objectID": "alxnat003.html#class-imbalance-and-up-sampling",
    "href": "alxnat003.html#class-imbalance-and-up-sampling",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.5 Class Imbalance and Up-Sampling",
    "text": "5.5 Class Imbalance and Up-Sampling\nI checked for class imbalance by comparing the frequency of records in each of the target variable classes, namely: Mandela, Mbeki, Zuma and Ramaphosa. I found that the classes were imbalanced, where Mbeki had the largest proportion of sentences (92), followed by Ramaphosa (51), Zuma (41) and Mandela (21). As a result, I used upSample() in the Caret package to oversample the minority classes so that the number of observations in each class matched the majority class."
  },
  {
    "objectID": "alxnat003.html#split-balanced-data-into-training-validation-and-test-sets",
    "href": "alxnat003.html#split-balanced-data-into-training-validation-and-test-sets",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.6 Split Balanced Data into Training, Validation and Test sets",
    "text": "5.6 Split Balanced Data into Training, Validation and Test sets\nI partitioned the data into 70% training and 30% test sets using createDataPartition(), which performs stratified sampling. The target variable, president was then converted to factors, where classes: Mandela, Mbeki, Ramaphosa and Zuma were categorized as levels 1 to 4, respectively. For the validation set, 5-fold cross-validation was applied during training to determine the optimal model parameters by means of a grid-search (discussed below)."
  },
  {
    "objectID": "alxnat003.html#workflow",
    "href": "alxnat003.html#workflow",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.7 Workflow",
    "text": "5.7 Workflow\nEach classification model implemented both the BoW and TF-IDF features discussed above. For each model, a grid search was performed to find the optimal hyperparameters, generating sub-models as a result. For each sub-model, 5-fold cross-validation was performed on the training set. The best sub-model was determined by the model performance on both the training and validation sets. The hyperparameters of the best sub-model was then used to re-train the model on the full training set, after which predictions were made on the test set. The final models were compared based on their test set performance. Figure 1 below shows the general workflow.\n\n\n\n\n\nflowchart TB\n  A&gt;1. Select Features] --&gt; B[a. Bag-of-Words]\n  A --&gt; D&gt;2. Select Training Model]\n  A --&gt; C[b. Term Frequency-Inverse Document Frequency]\n  D --&gt; E[a. Classification Tree]\n  D --&gt; F[b. Random Forest]\n  D --&gt; J&gt;3. Grid Search using 5-fold cross-validation]\n  D --&gt; G[c. Extreme Gradient Boosting]\n  D --&gt; H[d. Naïve Bayes]\n  D --&gt; I[e. Feed Forward Neural Network]\n  J --&gt; K&gt;4. Choose best sub-model based on train and validation performance]\n  K --&gt; L&gt;5. Rebuild model using the optimal hyperparameters on the full training set]\n  L --&gt; M&gt;6. Predictions on test set and compare test set model performance]\n  \n\n\nFigure 1: Flow-chart showing the work-flow for model construction, hyperparameter tuning and model selection and testing after splitting the data into 70% training and 30% test sets.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Figure 1 above, the ribbons are the steps in the process and the rectangles are the choices made at each step."
  },
  {
    "objectID": "alxnat003.html#model-construction",
    "href": "alxnat003.html#model-construction",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.8 Model Construction",
    "text": "5.8 Model Construction\n\n5.8.1 Classification Tree\nClassification trees recursively partition the input space and assigns a class label to each partitioned region based on the majority observation classes in that region. A grid search was performed using the rpart() function with the following parameters:\n\nThe complexity parameter, a stopping criterion where tree splitting terminates once the reduction in relative error is less than a specified cp-threshold :\ncp = {0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889, 1.000}\nThe minimum number of observations at any terminal node:\nminbucket = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\nThe minimum number of observations that must exist in a node for a split to be attempted:\nminsplit = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\n\n\n5.8.2 Random Forest\nRandom Forests use bagging to train multiple decision trees in parallel, reaching a majority vote of class classification. This type of ensemble learning helps to overcome overfitting.\nThe randomForest() function was used, taking in the following parameters:\n\nThe number of trees to grow:\nntree = {100, 500, 1000}\nFeature importance:\nImportance = TRUE\nna.action = na.exclude\n\n\n\n5.8.3 Extreme Gradient Boosting\nExtreme Gradient Boosting (XGBoost) models aggregate the output of a sequential ensemble of tree models, where each subsequent model improves on the previous model. The xgboost() function was used, with the following important parameters:\n\nThe maximum depth of the tree, where increasing this value results in a more complex model:\nmax depth = {1, 2, 3, 4, 5, 6}\nThe step size shrinkage, which is used to prevent overfitting. This value of eta shrinks the feature weights to make the boosting process more conservative:\neta = {0.2, 0.4, 0.6, 1}\nThe minimum loss reduction, where the larger the value of gamma is, the more conservative the algorithm is:\ngamma = {0.5, 1}\nThe L2 regularization term on the weights:\nlambda = 1\nObjective function for multi-class classification:\nobjective = “multi:softmax”\n\nIt is important to note, that the labels of the input data were converted to factors then to integers and finally offset by -1 because xgboost() takes in numeric data where classes of the target variable should be indexed from 0. The features need to be converted to a DMatrix object using the xgb.DMatrix() function. Consequently, I offset the predictions by +1 when computing the confusion matrix, since R indexes from 1. I then had to convert the numeric predictions back to factors, where:\n\n0+1: Mandela\n1+1: Mbeki\n2+1: Ramaphosa\n3+1: Zuma\n\n\n\n5.8.4 Naïve Bayes\nNaïve Bayes is a generative model that seeks to model the distribution of inputs of a given class. The naiveBayes() function was used, while applying Laplace smoothing to handle zero probabilities in categorical data. Laplace values of 0, 0.1 and 1 were investigated.\n\n\n5.8.5 Feed Forward Neural Network\nA feed-forward neural network is a type of artificial neural network where the connections between nodes do not form a cycle. Information in this network moves only in one direction, forward, from the input layer through the hidden layers, to the output layer.\nA feed-forward neural network was built using Keras and TensorFlow within a Conda environment managed by Anaconda, all executed in RStudio. The target variable (president) was extracted from the training data, and converted to factors, after which the factors were converted to discrete integers from 0 to 3, where:\n\n0: Mandela\n1: Mbeki\n2: Ramaphosa\n3: Zuma.\n\nSubsequently, the target variable president was one hot-coded to a binary representation. The features were then extracted and converted to a matrix format. The following hyperparameters were used:\n\n  Input neurons = {50, 100, 200}\n  Learning rate = {0.001, 0.01, 0.1}\n  Drop-out rate = {0.01, 0.1}\n\nThe network was built as follows:\n\nLayer 1: a dense, fully connect layer with x input neurons specified above, and which uses either a Tanh or ReLU activation function. This layer expects 149 predictor variables (words).\nLayer 2: a dropout layer to prevent overfitting by randomly selecting a fraction (drop-out rate above) of the input units at each update and setting them to 0 during training time.\nLayer 3: a dense, fully connected layer with 4 output neurons (representing the 4 president classes). A softmax activation function was used to generate a vector of probabilities of class membership for each observation.\n\nThe categorical cross-entropy loss function was minimized which is invariant to shifting of the predicted probabilities. The n-Adam optimizer was used to update the model parameters (weights and biases). This choice of optimizer was based on its superior performance compared to other optimizers, such as SGD, RMSprop, and Adam (informally tested). Finally, the data was trained for 30 epochs, with a batch size of 5, while shuffling the training data at each epoch."
  },
  {
    "objectID": "alxnat003.html#model-performance",
    "href": "alxnat003.html#model-performance",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.9 Model Performance",
    "text": "5.9 Model Performance\nFor all models, performance on the training, validation and test sets were determined by the metrics discussed below. For 5-fold cross-validation, the best sub-model was determined which had the best mean cross-validation (CV) and training macro-average F1-score. The final model was chosen based on the model with the best test set macro-average F1-score. Other metrics were also considered (see below).\nAt each fold, the metric is computed for every class, and the macro-average metric is obtained by averaging these class-specific metrics (Equation 1). After training is complete, the final metric is computed as the average of the macro-average metrics over all folds (Equation 2).\n\\[\n\\text{MacroAverageMetric}_{\\text{j}} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{metric}_{i,\\text{j}}\n\\]\n…equation 1, where metrici, j is the metric for class i at a specific fold j , and n is the total number of classes. We see that j = [1,5], n = 4 and i [1,4].\n\\[\n\\text{FinalMetric} = \\frac{1}{k} \\sum_{j=1}^{k} \\text{MacroAverageMetric}_{j}\n\\]\n…equation 2, where MacroAverageMetricj is the macro-average metric at fold j, and k is the total number of folds. We see that k = 5 and j = [1, 5].\n\n5.9.1 Accuracy\nThe proportion of correct classifications among the total number of classifications.\n\\[\n\\text{Accuracy} = \\frac{(\\text{TP} + \\text{TN})}{(\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN})}\n\\]\n…equation 3, where TP, TN, FP and FN are the number of true positives, true negatives, false positives, and false negatives, respectively.\n\n\n5.9.2 Recall\nAlso known as the sensitivity, this is the proportion of actual positives that are correctly classified.\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{(\\text{TP} + \\text{FN})}\n\\]…equation 4\n\n\n5.9.3 Precision\nAlso known as the positive predictive value, this is the proportion of positive predictions that are correctly classified.\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{(\\text{TP} + \\text{FP})}\n\\]…equation 5\n\n\n5.9.4 F1-score\nI chose the F1-score as my main criterion for model performance because the F1-score provides information on the model’s ability to capture positive cases (recall) and be accurate with the cases it does capture (precision).\n\\[\nF1 = 2 \\frac{(\\text{Precision} \\times \\text{Recall})}{(\\text{Precision} + \\text{Recall})}\n\\]…equation 6"
  },
  {
    "objectID": "alxnat003.html#exploratory-data-analysis-1",
    "href": "alxnat003.html#exploratory-data-analysis-1",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "6.1 Exploratory Data Analysis",
    "text": "6.1 Exploratory Data Analysis\n\n6.1.1 Top 20 most frequently used words and bigrams\nIn Figure 2 below we see the top 20 most frequently used- (a) words and (b) bigrams, among all presidents. The bigrams provide more context relative to the unigrams, and we see that most president’s agenda is about the economy with bigrams such as “economic growth” and “job creation”. Figure 3 below shows the top 20 most frequently used words- and Figure 4 below shows the top 20 most frequently used bigrams- for each president. In these figures we see that the agenda is specific to a president or period of presidency, where Mandela has bigrams such as “people-centered society” alluding to the end of Apartheid, Mbeki focuses on social justice with bigrams such as “social security” and “social partners”, Zuma mentions the “world cup” which alludes to the 2010 soccer world cup hosted in South Africa and Ramaphosa’s focus is on crime prevention with bigrams such as “gender-based violence” and “law enforcement”.\n\n\n\n\n\nFigure 2: Bar plots showing the 20 most frequently used (a) words and (b) bigrams used by all presidents\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Bar plots showing the 20 most frequently used words by each president.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Bar plots showing the 20 most frequently used bigrams by each president.\n\n\n\n\n\n\n\n6.1.2 Average speech and Sentence Length\nFigure 5 below shows the (a) average speech length per president and (b) the average sentence length per president. In Figure 5 (a) we see that Ramaphosa has a relatively long average speech length (327 sentences per speech), followed by Zuma (266), Mbeki (242) and Mandela (238). In Figure 5 (b) we see that Mbeki has the largest average sentence length (30 words per sentence), followed by Mandela (25) , Ramaphosa (22) and Zuma (19).\n\n\n\n\n\nFigure 5: Bar plots showing (a) the average number of sentences per speech and (b) the average number of words per sentence for each president."
  },
  {
    "objectID": "alxnat003.html#bag-of-word-models",
    "href": "alxnat003.html#bag-of-word-models",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "6.2 Bag-of-Word Models",
    "text": "6.2 Bag-of-Word Models\n\n6.2.1 Classification Trees\nIn Table 1 below, we see the results of the best performing classification trees for the BoW features.\nWe see that model 1 and 2 both have the best mean CV F1-score of 0.546 and a training F1-score of 0.749. These models also have the best mean CV accuracy of 0.565 and a training accuracy of 0.751. These models perform better than the other models with regards to the other metrics as well. Model 1 parameters were chosen to re-train the model on the full training set. The results of training can be seen in the BoW tree diagram in Figure 6 .\n\n\n\n\nTable 1: Best Classification Trees, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncp\nminbucket\nminsplit\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW tree model 1\n0.001\n1\n1\n0.749\n0.546\n0.751\n0.565\n0.751\n0.565\n0.832\n0.649\n\n\nBoW tree model 2\n0.001\n1\n2\n0.749\n0.546\n0.751\n0.565\n0.751\n0.565\n0.832\n0.649\n\n\nBoW tree model 3\n0.001\n1\n3\n0.743\n0.540\n0.746\n0.561\n0.746\n0.561\n0.827\n0.643\n\n\nBoW tree model 4\n0.001\n1\n4\n0.741\n0.540\n0.744\n0.561\n0.744\n0.561\n0.824\n0.643\n\n\nBoW tree model 5\n0.001\n1\n5\n0.739\n0.540\n0.742\n0.561\n0.742\n0.561\n0.820\n0.643\n\n\n\n\n\n\n\n\n\n6.2.2 Random Forests\nIn Table 2 we see that all models performed equally well. All metrics have values above 0.6, suggesting good model fit. The mean CV-F1 score = 0.648 and the training F1-score = 0.641. The training accuracy = 0.639 and mean CV accuracy = 0.65. I chose RF sub-model 2 with ntrees = 500 to re-train the full training set, which strikes a balance between too many and too few trees.\n\n\n\n\nTable 2: Best Random Forest models, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nntree\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW RF model 1\n100\n0.641\n0.648\n0.639\n0.65\n0.639\n0.65\n0.682\n0.692\n\n\nBoW RF model 2\n500\n0.641\n0.648\n0.639\n0.65\n0.639\n0.65\n0.682\n0.692\n\n\nBoW RF model 3\n1000\n0.641\n0.648\n0.639\n0.65\n0.639\n0.65\n0.682\n0.692\n\n\n\n\n\n\n\n\n6.2.3 Extreme Gradient Boosting\nTable 3 below suggests that sub-model 47 had the best model performance, and so the model was re-trained on the full training dataset using max depth = 6, eta = 1 and gamma = 0.5. We see that the training F1-score (0.621) and training accuracy (0.62) is relatively good, however the mean CV F1-score (0.426) and mean CV accuracy (0.438) is relatively low . We see a similar trend for all other metrics, where the training performance is good, but the validation performance is poor, suggesting that the model overfits\n\n\n\n\nTable 3: Best XGBoost models, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmaxdepth\neta\ngamma\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW XGB model 47\n6\n1.0\n0.5\n0.621\n0.426\n0.620\n0.438\n0.620\n0.438\n0.721\n0.559\n\n\nBoW XGB model 48\n6\n1.0\n1.0\n0.615\n0.427\n0.613\n0.442\n0.613\n0.442\n0.724\n0.563\n\n\nBoW XGB model 39\n5\n1.0\n0.5\n0.614\n0.435\n0.612\n0.450\n0.612\n0.450\n0.714\n0.594\n\n\nBoW XGB model 45\n6\n0.6\n0.5\n0.609\n0.428\n0.606\n0.439\n0.606\n0.439\n0.727\n0.603\n\n\nBoW XGB model 40\n5\n1.0\n1.0\n0.609\n0.435\n0.606\n0.450\n0.606\n0.450\n0.722\n0.597\n\n\n\n\n\n\n\n\n6.2.4 Naïve Bayes\nAs seen in Table 4 below, all “top” performing models displayed poor performance. These models all displayed the same metric values, with a training F1-score of 0.106 and a mean CV F1-score of 0.1. The training accuracy of 0.253 and mean CV accuracy of 0.25 also suggests poor model performance. I used the simplest model, model 1 with no Laplace smoothing to re-train the model on the full training dataset.\n\n\n\n\nTable 4: Best Naïve Bayesian models, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaplace\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW NB model 1\n0.0\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\nBoW NB model 2\n0.1\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\nBoW NB model 3\n0.2\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\nBoW NB model 4\n0.3\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\nBoW NB model 5\n0.4\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\n\n\n\n\n\n\n6.2.5 Feed Forward Neural Network\nIn Table 5 below, we see the results of the feed forward neural network, where model 55 had the best training F1-score of 0.857 (as well as model 17) and the best mean CV F1-score of 0.593. We also see that model 55 had one of the best training accuracies (0.852) and the best mean CV accuracy (0.612). All other metrics for model 55 are also among the top performing models.\nFigure 7 in Addendum A shows the change in the Log Loss, training- recall, precision and accuracy over all 30 epochs.\n\n\n\n\nTable 5: Best Feed Forward Neural Network models, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput nodes\nDrop-out rate\nActivation function\nLearning rate\nL1 regularization\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW NN model 55\n100\n0.10\nrelu\n0.01\n0.01\n0.857\n0.593\n0.852\n0.612\n0.823\n0.531\n0.894\n0.677\n\n\nBoW NN model 17\n100\n0.01\nrelu\n0.01\n0.01\n0.857\n0.571\n0.856\n0.569\n0.819\n0.504\n0.901\n0.669\n\n\nBoW NN model 5\n50\n0.01\nrelu\n0.01\n0.01\n0.856\n0.597\n0.856\n0.623\n0.817\n0.523\n0.900\n0.702\n\n\nBoW NN model 65\n200\n0.01\nrelu\n0.01\n0.01\n0.856\n0.596\n0.853\n0.608\n0.813\n0.546\n0.904\n0.657\n\n\nBoW NN model 19\n100\n0.10\nrelu\n0.01\n0.01\n0.855\n0.586\n0.855\n0.619\n0.812\n0.512\n0.902\n0.693"
  },
  {
    "objectID": "alxnat003.html#term-frequency-inverse-document-frequency-models",
    "href": "alxnat003.html#term-frequency-inverse-document-frequency-models",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "6.3 Term Frequency-Inverse Document Frequency Models",
    "text": "6.3 Term Frequency-Inverse Document Frequency Models\n\n6.3.1 Classification Trees\nIn Table 6 below, we see that sub-model 1 and 2 had the best mean CV F1-score of 0.583, training F1-score of 0.742, mean CV accuracy of 0.596, and training accuracy of 0.746. In parallel, we see that that all other metrics for sub-models 1 and 2 are among the best performing models. Model 1 parameters were chosen to re-train the model on the full training set. We see interesting clusters in the TF-IDF tree diagram in Figure 8 .\n\n\n\n\nTable 6: Best Classification Trees, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncp\nminbucket\nminsplit\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF tree model 1\n0.001\n1\n1\n0.742\n0.583\n0.746\n0.596\n0.746\n0.596\n0.817\n0.677\n\n\nTFIDF tree model 2\n0.001\n1\n2\n0.742\n0.583\n0.746\n0.596\n0.746\n0.596\n0.817\n0.677\n\n\nTFIDF tree model 3\n0.001\n1\n3\n0.738\n0.573\n0.742\n0.589\n0.742\n0.589\n0.814\n0.662\n\n\nTFIDF tree model 4\n0.001\n1\n4\n0.738\n0.573\n0.742\n0.589\n0.742\n0.589\n0.814\n0.662\n\n\nTFIDF tree model 5\n0.001\n1\n5\n0.738\n0.573\n0.742\n0.589\n0.742\n0.589\n0.814\n0.662\n\n\n\n\n\n\n\n\n\n6.3.2 Random Forests\nIn Table 7 below we see that the grid-search performed equally well for all ntree values. All metrics are relatively good, with values above 0.6, suggesting good model fit. We see that the mean CV-F1 score (0.653) is similar to the training F1-score (0.647). We also see that the mean CV accuracy (0.654) is similar to the training accuracy (0.647). I chose RF sub-model 2 with ntrees = 500 to re-train the full training set, which strikes a balance between too many and too few trees.\n\n\n\n\nTable 7: Best Random Forest models, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nntree\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF RF model 1\n100\n0.647\n0.653\n0.647\n0.654\n0.647\n0.654\n0.698\n0.699\n\n\nTFIDF RF model 2\n500\n0.647\n0.653\n0.647\n0.654\n0.647\n0.654\n0.698\n0.699\n\n\nTFIDF RF model 3\n1000\n0.647\n0.653\n0.647\n0.654\n0.647\n0.654\n0.698\n0.699\n\n\n\n\n\n\n\n\n\n6.3.3 Extreme Gradient Boosting\nTable 8 below suggests that sub-model 39 has the best model performance, and so this model was re-trained on the full training dataset. We see that the training F1-score (0.637) and training accuracy (0.637) is relatively good, however the mean CV F1-score (0.465) and mean CV accuracy (0.481) is relatively low . We see a similar trend for all other metrics, where the training performance is good, but the validation performance is poor, suggestive of model overfitting.\n\n\n\n\nTable 8: Best XGBoost models, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmaxdepth\neta\ngamma\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF XGB model 39\n5\n1.0\n0.5\n0.637\n0.465\n0.637\n0.481\n0.637\n0.481\n0.738\n0.593\n\n\nTFIDF XGB model 47\n6\n1.0\n0.5\n0.636\n0.460\n0.637\n0.477\n0.637\n0.477\n0.737\n0.594\n\n\nTFIDF XGB model 48\n6\n1.0\n1.0\n0.633\n0.462\n0.635\n0.477\n0.635\n0.477\n0.735\n0.592\n\n\nTFIDF XGB model 40\n5\n1.0\n1.0\n0.630\n0.457\n0.630\n0.473\n0.630\n0.473\n0.731\n0.579\n\n\nTFIDF XGB model 45\n6\n0.6\n0.5\n0.626\n0.460\n0.625\n0.473\n0.625\n0.473\n0.733\n0.587\n\n\n\n\n\n\n\n\n\n6.3.4 Naïve Bayes\nAs seen in Table 9 below, all “top” performing models displayed poor performance. These models all displayed the same metric values, with a training F1-score of 0.1 equivalent to the mean CV F1-score of 0.1. The training accuracy and mean CV accuracy are both 0.25 suggesting poor model performance. I used the simplest model, model 1 with no Laplace smoothing to re-train the model on the full training dataset.\n\n\n\n\nTable 9: Best Naïve Bayesian models, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaplace\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF NB model 1\n0.0\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\nTFIDF NB model 2\n0.1\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\nTFIDF NB model 3\n0.2\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\nTFIDF NB model 4\n0.3\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\nTFIDF NB model 5\n0.4\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\n\n\n\n\n\n\n\n6.3.5 Feed Forward Neural Network\nIn Table 10 below, we see that model 42 had the best training F1-score of 0.849 and one of the best mean CV F1-scores of 0.637. We also see that model 42 had among the best training accuracies (0.851) and the best validation accuracy (0.650). All other metrics for model 42 are also among the top performing models.\nFigure 9 in Addendum A shows the change in the Log Loss, training- recall, precision and accuracy over all 30 epochs.\n\n\n\n\nTable 10: Best Feed Forward Neural Network models, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput nodes\nDrop-out rate\nActivation function\nLearning rate\nL1 regularization\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF NN model 42\n50\n0.01\ntanh\n0.01\n0.01\n0.849\n0.637\n0.851\n0.650\n0.806\n0.596\n0.897\n0.686\n\n\nTFIDF NN model 5\n50\n0.01\nrelu\n0.01\n0.01\n0.848\n0.637\n0.853\n0.646\n0.802\n0.600\n0.900\n0.680\n\n\nTFIDF NN model 41\n50\n0.01\nrelu\n0.01\n0.01\n0.848\n0.632\n0.849\n0.654\n0.805\n0.581\n0.898\n0.704\n\n\nTFIDF NN model 6\n50\n0.01\ntanh\n0.01\n0.01\n0.846\n0.639\n0.850\n0.646\n0.804\n0.600\n0.894\n0.686\n\n\nTFIDF NN model 44\n50\n0.10\ntanh\n0.01\n0.01\n0.846\n0.635\n0.855\n0.673\n0.803\n0.596\n0.896\n0.680"
  },
  {
    "objectID": "alxnat003.html#test-set-model-performance",
    "href": "alxnat003.html#test-set-model-performance",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "6.4 Test Set Model Performance",
    "text": "6.4 Test Set Model Performance\nIn Table 11 below we see the model performance on the test set. The neural net model 42, using the TF-IDF fetaures outperformed all other models. Figure 10 in Addendum A shows the confusion matrix of TF-IDF NN model 42.\n\n\n\n\nTable 11: Model performance on the test set when training the best performing models on the full training set\n\n\n\n\n\n\n\n\n\n\nModel ID\nParameters\nTest Accuracy\nTest-F1\nTest Precision\nTest Recall\n\n\n\n\nBoW tree model 1\ncp = 0.001, minbucket = 1, minsplit = 1\n0.500\n0.495\n0.664\n0.500\n\n\nBoW RF model 2\nntree=500\n0.602\n0.613\n0.686\n0.602\n\n\nBoW XGB model 47\nmaxdepth = 6, eta = 1, gamma = 0.5\n0.370\n0.378\n0.596\n0.370\n\n\nBoW NB model 1\nLaplace0\n0.250\n0.100\n0.062\n0.250\n\n\nBoW NN model 55\nInput nodes = 50, activation function = 0.01, Learning rate = relu, L1 regularization = 0.001\n0.593\n0.629\n0.709\n0.565\n\n\nTFIDF tree model 1\ncp = 0.001, minbucket = 1, minsplit = 1\n0.463\n0.460\n0.615\n0.463\n\n\nTFIDF RF model 2\nntree=500\n0.620\n0.628\n0.696\n0.620\n\n\nTFIDF XGB model 39\nmaxdepth = 5, eta = 1, gamma = 0.5\n0.417\n0.422\n0.625\n0.417\n\n\nTFIDF NB model 1\nLaplace=0\n0.250\n0.100\n0.062\n0.250\n\n\nTFIDF NN model 42\nInput nodes = 50, Learning rate = 0.01, activation function = tanh, L1 regularization = 0.01\n0.639\n0.632\n0.653\n0.611"
  },
  {
    "objectID": "alxnat003.html#training-and-validation-model-performance",
    "href": "alxnat003.html#training-and-validation-model-performance",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "7.1 Training and Validation Model Performance",
    "text": "7.1 Training and Validation Model Performance\nFor the BoW Table 1 and TF-IDF Table 6 classification trees we see that the training performance is generally better than the validation performance, suggestive of model overfitting, where the model does not generalize well to the validation data. We see a similar trend for the XGBoost models ( BoW Table 3 and TF-IDF Table 8 ).\nWe see two interesting clusters in the tree diagrams in Figure 6 (BoW) and Figure 8 (TF-IDF) of Addendum A, which suggests that the unigram “health” is an important feature in the sentences of Ramaphosa (2018-present), which is in accordance with the Covid-19 pandemic. In addition, “peace” is an important feature in the sentences of Mandela (1994-1999), which is in accordance with the end of the Apartheid era (1994).\nFor the BoW (Table 2) and TF-IDF (Table 7) Random Forest models, we see a trend where the validation metrics are similar to the training metrics, where all metrics have a value above 0.6 suggesting good model fit. These models generalize well to the validation data.\nThe Naive Bayesian models (BoW, Table 4 and TF-IDF, Table 9 ) have poor model fit on the training data, and does not generalize well to the validation data.\nFor the BoW ( Table 5 and Figure 7 in Addendum A ) and TF-IDF (Table 10 and Figure 9 in Addendum A) Neural Network models, we see the log loss decreases to 0, as the number of epochs increase, whereas the training- recall, precision and accuracy values increase towards a maximum value. This suggests that the loss has been minimized and the performance, maximized."
  },
  {
    "objectID": "alxnat003.html#test-set-model-performance-1",
    "href": "alxnat003.html#test-set-model-performance-1",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "7.2 Test Set Model Performance",
    "text": "7.2 Test Set Model Performance\nThe overall best model (Neural Net model 42, using the TF-IDF features) (Table 11), outperformed all other models with a test F1-score of 0.632. This model also had the best test set accuracy (0.639). However, the model’s test precision score of 0.653 felt short of the neural net model 55, using BoW features (0.709). In addition, NN model 42 had a test recall of 0.611, which felt short of the random forest model 2 using TF-IDF features, with a recall of 0.620.\nThe neural net models and the random forest models for both BoW and TF-IDF generalized well to the unseen, test data, with performance metrics above 0.6 in most cases. This suggests that in most cases these models can correctly predict the president who said a sentence.\nThe classification trees and XGBoost models had sub-optimal performance, which suggests that these models do not generalize as well to the unseen, test data. In addition, the Naïve Bayesian models have very poor model fit and do not generalize well to the unseen, test data. These sub-optimal and poorly fit models may be over-fitting the training data. As a result, the model does not “learn” but memorizes the characteristics of the training data.\nIn general, we also see a trend where in most cases the TF-IDF models outperform the BoW models, which may indicate that word importance is a better feature choice than word frequency."
  },
  {
    "objectID": "STA5073Z_Assignment1.html",
    "href": "STA5073Z_Assignment1.html",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "",
    "text": "Text mining refers to the process of transforming unstructured text data into structured clusters of information. This project explores a specific aspect of text mining known as authorship attribution, which involves analyzing linguistic and stylistic features of text to predict its author. This project uses transcription data from South African State of the Nation Address (SONA) speeches, delivered by presidents between 1994 to 2023. The primary goal was to develop a classification model that takes a sentence from a SONA speech as input and correctly predicts the president who said it. Various models were trained, such as Classification Trees, and Random Forests, in addition to XGBoost- , Naïve Bayesian- and Feed Forward Neural Network models. I find that the Neural Net model, with TF-IDF features outperformed all other models with a test F1-score of 0.632 and a test accuracy of 0.639."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#data-source-and-description",
    "href": "STA5073Z_Assignment1.html#data-source-and-description",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "4.1 Data Source and Description",
    "text": "4.1 Data Source and Description\nThe SONA dataset is publicly available on the South African government website [4]. The data contains the speeches delivered by South African presidents at the annual State of the Nation Address (SONA) from 1994 to 2023.\nThe following data is available:\n\nMandela (1994-1999) - 7 speeches\nMbeki (2000-2008) - 10 speeches\nZuma (2009-2017) - 10 speeches\nRamaphosa (2018-2023) - 7 speeches\nTwo outliers exist:\n\ndeKlerk (1994) - 1 speech\nMotlanthe (2009) - 1 speech\n\n\nThese records cumulatively formed the SONA dataset with 36 records and 5 variables, namely: filename, speech, year, president and date of speech delivered."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#data-pre-processing",
    "href": "STA5073Z_Assignment1.html#data-pre-processing",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "4.2 Data Pre-processing",
    "text": "4.2 Data Pre-processing\nAll data was read into R version 4.3.1 , using R-Studio version 2023.9.1.494. The year of each speech was extracted from the first four characters in the filename column. The president names were extracted from the filename column using regular expressions, where alphabetical text ending in a “.txt” extension was matched as the presidents’ name. Subsequently, all other unnecessary text such as “http”-, fullstop-, ampersand-, greater-than-, and less-than characters were removed, in addition to trailing white spaces and new-line characters. Dates were then re-formatted into a dd-mm-yyyy format. Finally, the pre-processed data was saved as an RDS object for downstream analysis."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#data-processing",
    "href": "STA5073Z_Assignment1.html#data-processing",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.1 Data processing",
    "text": "5.1 Data processing\nThe pre-processed data was read into R and converted to a tibble. The data was then assessed by looking at the head and tail of the tibble, in addition to looking at the data types of each column. Dates which appeared before a president’s speech were removed using regular expressions. Trailing white spaces before and after a speech were also removed. I also noticed that former president- Motlanthe and deKlerk only had one record and so these observations were removed from the dataset.\nAs a result, the processed data had 34 rows and 5 columns."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#tokenization",
    "href": "STA5073Z_Assignment1.html#tokenization",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.2 Tokenization",
    "text": "5.2 Tokenization\nThe processed SONA dataset was then tokenized into sentences using unnest_tokens(), since the goal is to make predictions on sentence inputs. All text was then converted to lowercase to remove word redundancy. I also removed all punctuation so that root-words are treated alike. I then included a sentence ID column to track sentence membership.\nThe sentence tokens were then tokenized into word and bigram tokens respectively, where all stop words were removed. I also ensured that “blank” tokens were removed. For the tokenization by bigram implementation, bigrams were first split into individual words, where each word was assessed for stop words. If a stop word was detected, the entire bigram was removed, while the remaining bigrams were unified."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#exploratory-data-analysis",
    "href": "STA5073Z_Assignment1.html#exploratory-data-analysis",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.3 Exploratory Data Analysis",
    "text": "5.3 Exploratory Data Analysis\nI looked at the 20 most frequently used- words and bigrams: (1) for all presidents, and (2) for each president. I also looked at the average number of sentences per speech and the average number of words per sentence for each president."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#features",
    "href": "STA5073Z_Assignment1.html#features",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.4 Features",
    "text": "5.4 Features\n\n5.4.1 Bag-of-Words Model\nThe Bag-of-Words (BoW) model computes the frequency of occurrence of an unordered collection of words within a document and uses these frequencies as features to train the classifier.\nA word bag was generated by taking the word tokens and grouping the unique combinations of “sentence ID, president and word” and computing each grouping’s frequency, after which the top 200 words for each grouping was selected to create the final word bag. The choice of the top 200 words was chosen due to its superior model performance relative to the top 100- and 500 word models (tested informally). The word bag consisted of 363 rows (words) and 3 columns, namely, sentence ID, president, and word.\nThe BoW table was then constructed by identifying all the words in a sentence that overlap with the word bag. The frequency of each word within a sentence was then calculated. Finally, the BoW table was reformatted to a tidy format, where columns represent the features (149 words), the rows represent the observations (sentence ID) and the cell values are the frequency of a word within a sentence. All words not found in a sentence obtained a value of 0.\n\n\n5.4.2 Term Frequency-Inverse Document Frequency Model\nTerm Frequency – Inverse Document Frequency (TF-IDF) refers to the metric that describes how important a word is in a document relative to other documents in the corpus. TF-IDF is calculated by multiplying the Term Frequency (TF) by the Inverse Document Frequency (IDF), where TF is the frequency of a word within a document divided by the total number of words in that document, whereas IDF is the logarithmically scaled inverse fraction of the documents that contain the word. Bind_tf_idf() was used to calculate the TF-IDF for each word in each sentence. It is important to note that the “document” here refers to the sentence ID. The BoW table previously discussed was then manipulated to include TF-IDF values instead of word frequencies."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#class-imbalance-and-up-sampling",
    "href": "STA5073Z_Assignment1.html#class-imbalance-and-up-sampling",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.5 Class Imbalance and Up-Sampling",
    "text": "5.5 Class Imbalance and Up-Sampling\nI checked for class imbalance by comparing the frequency of records in each of the target variable classes, namely: Mandela, Mbeki, Zuma and Ramaphosa. I found that the classes were imbalanced, where Mbeki had the largest proportion of sentences (92), followed by Ramaphosa (51), Zuma (41) and Mandela (21). As a result, I used upSample() in the Caret package to oversample the minority classes so that the number of observations in each class matched the majority class."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#split-balanced-data-into-training-validation-and-test-sets",
    "href": "STA5073Z_Assignment1.html#split-balanced-data-into-training-validation-and-test-sets",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.6 Split Balanced Data into Training, Validation and Test sets",
    "text": "5.6 Split Balanced Data into Training, Validation and Test sets\nI partitioned the data into 70% training and 30% test sets using createDataPartition(), which performs stratified sampling. The target variable, president was then converted to factors, where classes: Mandela, Mbeki, Ramaphosa and Zuma were categorized as levels 1 to 4, respectively. For the validation set, 5-fold cross-validation was applied during training to determine the optimal model parameters by means of a grid-search (discussed below)."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#workflow",
    "href": "STA5073Z_Assignment1.html#workflow",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.7 Workflow",
    "text": "5.7 Workflow\nEach classification model implemented both the BoW and TF-IDF features discussed above. For each model, a grid search was performed to find the optimal hyperparameters, generating sub-models as a result. For each sub-model, 5-fold cross-validation was performed on the training set. The best sub-model was determined by the model performance on both the training and validation sets. The hyperparameters of the best sub-model was then used to re-train the model on the full training set, after which predictions were made on the test set. The final models were compared based on their test set performance. Figure 1 below shows the general workflow.\n\n\n\n\n\nflowchart TB\n  A&gt;1. Select Features] --&gt; B[a. Bag-of-Words]\n  A --&gt; D&gt;2. Select Training Model]\n  A --&gt; C[b. Term Frequency-Inverse Document Frequency]\n  D --&gt; E[a. Classification Tree]\n  D --&gt; F[b. Random Forest]\n  D --&gt; J&gt;3. Grid Search using 5-fold cross-validation]\n  D --&gt; G[c. Extreme Gradient Boosting]\n  D --&gt; H[d. Naïve Bayes]\n  D --&gt; I[e. Feed Forward Neural Network]\n  J --&gt; K&gt;4. Choose best sub-model based on train and validation performance]\n  K --&gt; L&gt;5. Rebuild model using the optimal hyperparameters on the full training set]\n  L --&gt; M&gt;6. Predictions on test set and compare test set model performance]\n  \n\n\nFigure 1: Flow-chart showing the work-flow for model construction, hyperparameter tuning and model selection and testing after splitting the data into 70% training and 30% test sets.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Figure 1 above, the ribbons are the steps in the process and the rectangles are the choices made at each step."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#model-construction",
    "href": "STA5073Z_Assignment1.html#model-construction",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.8 Model Construction",
    "text": "5.8 Model Construction\n\n5.8.1 Classification Tree\nClassification trees recursively partition the input space and assigns a class label to each partitioned region based on the majority observation classes in that region. A grid search was performed using the rpart() function with the following parameters:\n\nThe complexity parameter, a stopping criterion where tree splitting terminates once the reduction in relative error is less than a specified cp-threshold :\ncp = {0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889, 1.000}\nThe minimum number of observations at any terminal node:\nminbucket = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\nThe minimum number of observations that must exist in a node for a split to be attempted:\nminsplit = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\n\n\n5.8.2 Random Forest\nRandom Forests use bagging to train multiple decision trees in parallel, reaching a majority vote of class classification. This type of ensemble learning helps to overcome overfitting.\nThe randomForest() function was used, taking in the following parameters:\n\nThe number of trees to grow:\nntree = {100, 500, 1000}\nFeature importance:\nImportance = TRUE\nna.action = na.exclude\n\n\n\n5.8.3 Extreme Gradient Boosting\nExtreme Gradient Boosting (XGBoost) models aggregate the output of a sequential ensemble of tree models, where each subsequent model improves on the previous model. The xgboost() function was used, with the following important parameters:\n\nThe maximum depth of the tree, where increasing this value results in a more complex model:\nmax depth = {1, 2, 3, 4, 5, 6}\nThe step size shrinkage, which is used to prevent overfitting. This value of eta shrinks the feature weights to make the boosting process more conservative:\neta = {0.2, 0.4, 0.6, 1}\nThe minimum loss reduction, where the larger the value of gamma is, the more conservative the algorithm is:\ngamma = {0.5, 1}\nThe L2 regularization term on the weights:\nlambda = 1\nObjective function for multi-class classification:\nobjective = “multi:softmax”\n\nIt is important to note, that the labels of the input data were converted to factors then to integers and finally offset by -1 because xgboost() takes in numeric data where classes of the target variable should be indexed from 0. The features need to be converted to a DMatrix object using the xgb.DMatrix() function. Consequently, I offset the predictions by +1 when computing the confusion matrix, since R indexes from 1. I then had to convert the numeric predictions back to factors, where:\n\n0+1: Mandela\n1+1: Mbeki\n2+1: Ramaphosa\n3+1: Zuma\n\n\n\n5.8.4 Naïve Bayes\nNaïve Bayes is a generative model that seeks to model the distribution of inputs of a given class. The naiveBayes() function was used, while applying Laplace smoothing to handle zero probabilities in categorical data. Laplace values of 0, 0.1 and 1 were investigated.\n\n\n5.8.5 Feed Forward Neural Network\nA feed-forward neural network is a type of artificial neural network where the connections between nodes do not form a cycle. Information in this network moves only in one direction, forward, from the input layer through the hidden layers, to the output layer.\nA feed-forward neural network was built using Keras and TensorFlow within a Conda environment managed by Anaconda, all executed in RStudio. The target variable (president) was extracted from the training data, and converted to factors, after which the factors were converted to discrete integers from 0 to 3, where:\n\n0: Mandela\n1: Mbeki\n2: Ramaphosa\n3: Zuma.\n\nSubsequently, the target variable president was one hot-coded to a binary representation. The features were then extracted and converted to a matrix format. The following hyperparameters were used:\n\n  Input neurons = {50, 100, 200}\n  Learning rate = {0.001, 0.01, 0.1}\n  Drop-out rate = {0.01, 0.1}\n\nThe network was built as follows:\n\nLayer 1: a dense, fully connect layer with x input neurons specified above, and which uses either a Tanh or ReLU activation function. This layer expects 149 predictor variables (words).\nLayer 2: a dropout layer to prevent overfitting by randomly selecting a fraction (drop-out rate above) of the input units at each update and setting them to 0 during training time.\nLayer 3: a dense, fully connected layer with 4 output neurons (representing the 4 president classes). A softmax activation function was used to generate a vector of probabilities of class membership for each observation.\n\nThe categorical cross-entropy loss function was minimized which is invariant to shifting of the predicted probabilities. The n-Adam optimizer was used to update the model parameters (weights and biases). This choice of optimizer was based on its superior performance compared to other optimizers, such as SGD, RMSprop, and Adam (informally tested). Finally, the data was trained for 30 epochs, with a batch size of 5, while shuffling the training data at each epoch."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#model-performance",
    "href": "STA5073Z_Assignment1.html#model-performance",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "5.9 Model Performance",
    "text": "5.9 Model Performance\nFor all models, performance on the training, validation and test sets were determined by the metrics discussed below. For 5-fold cross-validation, the best sub-model was determined which had the best mean cross-validation (CV) and training macro-average F1-score. The final model was chosen based on the model with the best test set macro-average F1-score. Other metrics were also considered (see below).\nAt each fold, the metric is computed for every class, and the macro-average metric is obtained by averaging these class-specific metrics (Equation 1). After training is complete, the final metric is computed as the average of the macro-average metrics over all folds (Equation 2).\n\\[\n\\text{MacroAverageMetric}_{\\text{j}} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{metric}_{i,\\text{j}}\n\\]\n…equation 1, where metrici, j is the metric for class i at a specific fold j , and n is the total number of classes. We see that j = [1,5], n = 4 and i [1,4].\n\\[\n\\text{FinalMetric} = \\frac{1}{k} \\sum_{j=1}^{k} \\text{MacroAverageMetric}_{j}\n\\]\n…equation 2, where MacroAverageMetricj is the macro-average metric at fold j, and k is the total number of folds. We see that k = 5 and j = [1, 5].\n\n5.9.1 Accuracy\nThe proportion of correct classifications among the total number of classifications.\n\\[\n\\text{Accuracy} = \\frac{(\\text{TP} + \\text{TN})}{(\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN})}\n\\]\n…equation 3, where TP, TN, FP and FN are the number of true positives, true negatives, false positives, and false negatives, respectively.\n\n\n5.9.2 Recall\nAlso known as the sensitivity, this is the proportion of actual positives that are correctly classified.\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{(\\text{TP} + \\text{FN})}\n\\]…equation 4\n\n\n5.9.3 Precision\nAlso known as the positive predictive value, this is the proportion of positive predictions that are correctly classified.\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{(\\text{TP} + \\text{FP})}\n\\]…equation 5\n\n\n5.9.4 F1-score\nI chose the F1-score as my main criterion for model performance because the F1-score provides information on the model’s ability to capture positive cases (recall) and be accurate with the cases it does capture (precision).\n\\[\nF1 = 2 \\frac{(\\text{Precision} \\times \\text{Recall})}{(\\text{Precision} + \\text{Recall})}\n\\]…equation 6"
  },
  {
    "objectID": "STA5073Z_Assignment1.html#exploratory-data-analysis-1",
    "href": "STA5073Z_Assignment1.html#exploratory-data-analysis-1",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "6.1 Exploratory Data Analysis",
    "text": "6.1 Exploratory Data Analysis\n\n6.1.1 Top 20 most frequently used words and bigrams\nIn Figure 2 below we see the top 20 most frequently used- (a) words and (b) bigrams, among all presidents. The bigrams provide more context relative to the unigrams, and we see that most president’s agenda is about the economy with bigrams such as “economic growth” and “job creation”. Figure 3 below shows the top 20 most frequently used words- and Figure 4 below shows the top 20 most frequently used bigrams- for each president. In these figures we see that the agenda is specific to a president or period of presidency, where Mandela has bigrams such as “people-centered society” alluding to the end of Apartheid, Mbeki focuses on social justice with bigrams such as “social security” and “social partners”, Zuma mentions the “world cup” which alludes to the 2010 soccer world cup hosted in South Africa and Ramaphosa’s focus is on crime prevention with bigrams such as “gender-based violence” and “law enforcement”.\n\n\n\n\n\nFigure 2: Bar plots showing the 20 most frequently used (a) words and (b) bigrams used by all presidents\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Bar plots showing the 20 most frequently used words by each president.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Bar plots showing the 20 most frequently used bigrams by each president.\n\n\n\n\n\n\n\n6.1.2 Average speech and Sentence Length\nFigure 5 below shows the (a) average speech length per president and (b) the average sentence length per president. In Figure 5 (a) we see that Ramaphosa has a relatively long average speech length (327 sentences per speech), followed by Zuma (266), Mbeki (242) and Mandela (238). In Figure 5 (b) we see that Mbeki has the longest average sentence length (30 words per sentence), followed by Mandela (25) , Ramaphosa (22) and Zuma (19).\n\n\n\n\n\nFigure 5: Bar plots showing (a) the average number of sentences per speech and (b) the average number of words per sentence for each president."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#bag-of-word-models",
    "href": "STA5073Z_Assignment1.html#bag-of-word-models",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "6.2 Bag-of-Word Models",
    "text": "6.2 Bag-of-Word Models\n\n6.2.1 Classification Trees\nIn Table 1 below, we see the results of the best performing classification trees for the BoW features.\nWe see that model 1 and 2 both have the best mean CV F1-score of 0.546 and a training F1-score of 0.749. These models also have the best mean CV accuracy of 0.565 and a training accuracy of 0.751. These models perform better than the other models with regards to the other metrics as well. Model 1 parameters were chosen to re-train the model on the full training set. The results of training can be seen in the BoW tree diagram in Figure 6 .\n\n\n\n\nTable 1: Best Classification Trees, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncp\nminbucket\nminsplit\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW tree model 1\n0.001\n1\n1\n0.749\n0.546\n0.751\n0.565\n0.751\n0.565\n0.832\n0.649\n\n\nBoW tree model 2\n0.001\n1\n2\n0.749\n0.546\n0.751\n0.565\n0.751\n0.565\n0.832\n0.649\n\n\nBoW tree model 3\n0.001\n1\n3\n0.743\n0.540\n0.746\n0.561\n0.746\n0.561\n0.827\n0.643\n\n\nBoW tree model 4\n0.001\n1\n4\n0.741\n0.540\n0.744\n0.561\n0.744\n0.561\n0.824\n0.643\n\n\nBoW tree model 5\n0.001\n1\n5\n0.739\n0.540\n0.742\n0.561\n0.742\n0.561\n0.820\n0.643\n\n\n\n\n\n\n\n\n\n6.2.2 Random Forests\nIn Table 2 we see that all models performed equally well. All metrics have values above 0.6, suggesting good model fit. The mean CV-F1 score = 0.648 and the training F1-score = 0.641. The training accuracy = 0.639 and mean CV accuracy = 0.65. I chose RF sub-model 2 with ntrees = 500 to re-train the full training set, which strikes a balance between too many and too few trees.\n\n\n\n\nTable 2: Best Random Forest models, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nntree\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW RF model 1\n100\n0.641\n0.648\n0.639\n0.65\n0.639\n0.65\n0.682\n0.692\n\n\nBoW RF model 2\n500\n0.641\n0.648\n0.639\n0.65\n0.639\n0.65\n0.682\n0.692\n\n\nBoW RF model 3\n1000\n0.641\n0.648\n0.639\n0.65\n0.639\n0.65\n0.682\n0.692\n\n\n\n\n\n\n\n\n6.2.3 Extreme Gradient Boosting\nTable 3 below suggests that sub-model 47 had the best model performance, and so the model was re-trained on the full training dataset using max depth = 6, eta = 1 and gamma = 0.5. We see that the training F1-score (0.621) and training accuracy (0.62) is relatively good, however the mean CV F1-score (0.426) and mean CV accuracy (0.438) is relatively low . We see a similar trend for all other metrics, where the training performance is good, but the validation performance is poor, suggesting that the model overfits\n\n\n\n\nTable 3: Best XGBoost models, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmaxdepth\neta\ngamma\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW XGB model 47\n6\n1.0\n0.5\n0.621\n0.426\n0.620\n0.438\n0.620\n0.438\n0.721\n0.559\n\n\nBoW XGB model 48\n6\n1.0\n1.0\n0.615\n0.427\n0.613\n0.442\n0.613\n0.442\n0.724\n0.563\n\n\nBoW XGB model 39\n5\n1.0\n0.5\n0.614\n0.435\n0.612\n0.450\n0.612\n0.450\n0.714\n0.594\n\n\nBoW XGB model 45\n6\n0.6\n0.5\n0.609\n0.428\n0.606\n0.439\n0.606\n0.439\n0.727\n0.603\n\n\nBoW XGB model 40\n5\n1.0\n1.0\n0.609\n0.435\n0.606\n0.450\n0.606\n0.450\n0.722\n0.597\n\n\n\n\n\n\n\n\n6.2.4 Naïve Bayes\nAs seen in Table 4 below, all “top” performing models displayed poor performance. These models all displayed the same metric values, with a training F1-score of 0.106 and a mean CV F1-score of 0.1. The training accuracy of 0.253 and mean CV accuracy of 0.25 also suggests poor model performance. I used the simplest model, model 1 with no Laplace smoothing to re-train the model on the full training dataset.\n\n\n\n\nTable 4: Best Naïve Bayesian models, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaplace\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW NB model 1\n0.0\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\nBoW NB model 2\n0.1\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\nBoW NB model 3\n0.2\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\nBoW NB model 4\n0.3\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\nBoW NB model 5\n0.4\n0.106\n0.1\n0.253\n0.25\n0.253\n0.25\n0.112\n0.062\n\n\n\n\n\n\n\n\n6.2.5 Feed Forward Neural Network\nIn Table 5 below, we see the results of the feed forward neural network, where model 55 had the best training F1-score of 0.857 (as well as model 17) and the best mean CV F1-score of 0.593. We also see that model 55 had one of the best training accuracies (0.852) and the best mean CV accuracy (0.612). All other metrics for model 55 are also among the top performing models.\nFigure 7 in Addendum A shows the change in the Log Loss, training- recall, precision and accuracy over all 30 epochs.\n\n\n\n\nTable 5: Best Feed Forward Neural Network models, using bag-of-words features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput nodes\nDrop-out rate\nActivation function\nLearning rate\nL1 regularization\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nBoW NN model 55\n100\n0.10\nrelu\n0.01\n0.01\n0.857\n0.593\n0.852\n0.612\n0.823\n0.531\n0.894\n0.677\n\n\nBoW NN model 17\n100\n0.01\nrelu\n0.01\n0.01\n0.857\n0.571\n0.856\n0.569\n0.819\n0.504\n0.901\n0.669\n\n\nBoW NN model 5\n50\n0.01\nrelu\n0.01\n0.01\n0.856\n0.597\n0.856\n0.623\n0.817\n0.523\n0.900\n0.702\n\n\nBoW NN model 65\n200\n0.01\nrelu\n0.01\n0.01\n0.856\n0.596\n0.853\n0.608\n0.813\n0.546\n0.904\n0.657\n\n\nBoW NN model 19\n100\n0.10\nrelu\n0.01\n0.01\n0.855\n0.586\n0.855\n0.619\n0.812\n0.512\n0.902\n0.693"
  },
  {
    "objectID": "STA5073Z_Assignment1.html#term-frequency-inverse-document-frequency-models",
    "href": "STA5073Z_Assignment1.html#term-frequency-inverse-document-frequency-models",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "6.3 Term Frequency-Inverse Document Frequency Models",
    "text": "6.3 Term Frequency-Inverse Document Frequency Models\n\n6.3.1 Classification Trees\nIn Table 6 below, we see that sub-model 1 and 2 had the best mean CV F1-score of 0.583, training F1-score of 0.742, mean CV accuracy of 0.596, and training accuracy of 0.746. In parallel, we see that that all other metrics for sub-models 1 and 2 are among the best performing models. Model 1 parameters were chosen to re-train the model on the full training set. We see interesting clusters in the TF-IDF tree diagram in Figure 8 .\n\n\n\n\nTable 6: Best Classification Trees, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncp\nminbucket\nminsplit\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF tree model 1\n0.001\n1\n1\n0.742\n0.583\n0.746\n0.596\n0.746\n0.596\n0.817\n0.677\n\n\nTFIDF tree model 2\n0.001\n1\n2\n0.742\n0.583\n0.746\n0.596\n0.746\n0.596\n0.817\n0.677\n\n\nTFIDF tree model 3\n0.001\n1\n3\n0.738\n0.573\n0.742\n0.589\n0.742\n0.589\n0.814\n0.662\n\n\nTFIDF tree model 4\n0.001\n1\n4\n0.738\n0.573\n0.742\n0.589\n0.742\n0.589\n0.814\n0.662\n\n\nTFIDF tree model 5\n0.001\n1\n5\n0.738\n0.573\n0.742\n0.589\n0.742\n0.589\n0.814\n0.662\n\n\n\n\n\n\n\n\n\n6.3.2 Random Forests\nIn Table 7 below we see that the grid-search performed equally well for all ntree values. All metrics are relatively good, with values above 0.6, suggesting good model fit. We see that the mean CV-F1 score (0.653) is similar to the training F1-score (0.647). We also see that the mean CV accuracy (0.654) is similar to the training accuracy (0.647). I chose RF sub-model 2 with ntrees = 500 to re-train the full training set, which strikes a balance between too many and too few trees.\n\n\n\n\nTable 7: Best Random Forest models, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nntree\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF RF model 1\n100\n0.647\n0.653\n0.647\n0.654\n0.647\n0.654\n0.698\n0.699\n\n\nTFIDF RF model 2\n500\n0.647\n0.653\n0.647\n0.654\n0.647\n0.654\n0.698\n0.699\n\n\nTFIDF RF model 3\n1000\n0.647\n0.653\n0.647\n0.654\n0.647\n0.654\n0.698\n0.699\n\n\n\n\n\n\n\n\n\n6.3.3 Extreme Gradient Boosting\nTable 8 below suggests that sub-model 39 has the best model performance, and so this model was re-trained on the full training dataset. We see that the training F1-score (0.637) and training accuracy (0.637) is relatively good, however the mean CV F1-score (0.465) and mean CV accuracy (0.481) is relatively low . We see a similar trend for all other metrics, where the training performance is good, but the validation performance is poor, suggestive of model overfitting.\n\n\n\n\nTable 8: Best XGBoost models, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmaxdepth\neta\ngamma\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF XGB model 39\n5\n1.0\n0.5\n0.637\n0.465\n0.637\n0.481\n0.637\n0.481\n0.738\n0.593\n\n\nTFIDF XGB model 47\n6\n1.0\n0.5\n0.636\n0.460\n0.637\n0.477\n0.637\n0.477\n0.737\n0.594\n\n\nTFIDF XGB model 48\n6\n1.0\n1.0\n0.633\n0.462\n0.635\n0.477\n0.635\n0.477\n0.735\n0.592\n\n\nTFIDF XGB model 40\n5\n1.0\n1.0\n0.630\n0.457\n0.630\n0.473\n0.630\n0.473\n0.731\n0.579\n\n\nTFIDF XGB model 45\n6\n0.6\n0.5\n0.626\n0.460\n0.625\n0.473\n0.625\n0.473\n0.733\n0.587\n\n\n\n\n\n\n\n\n\n6.3.4 Naïve Bayes\nAs seen in Table 9 below, all “top” performing models displayed poor performance. These models all displayed the same metric values, with a training F1-score of 0.1 equivalent to the mean CV F1-score of 0.1. The training accuracy and mean CV accuracy are both 0.25 suggesting poor model performance. I used the simplest model, model 1 with no Laplace smoothing to re-train the model on the full training dataset.\n\n\n\n\nTable 9: Best Naïve Bayesian models, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaplace\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF NB model 1\n0.0\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\nTFIDF NB model 2\n0.1\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\nTFIDF NB model 3\n0.2\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\nTFIDF NB model 4\n0.3\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\nTFIDF NB model 5\n0.4\n0.1\n0.1\n0.25\n0.25\n0.25\n0.25\n0.063\n0.062\n\n\n\n\n\n\n\n\n\n6.3.5 Feed Forward Neural Network\nIn Table 10 below, we see that model 42 had the best training F1-score of 0.849 and one of the best mean CV F1-scores of 0.637. We also see that model 42 had among the best training accuracies (0.851) and the best validation accuracy (0.650). All other metrics for model 42 are also among the top performing models.\nFigure 9 in Addendum A shows the change in the Log Loss, training- recall, precision and accuracy over all 30 epochs.\n\n\n\n\nTable 10: Best Feed Forward Neural Network models, using TF-IDF features and sorted in descending order of training and validation F1-score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput nodes\nDrop-out rate\nActivation function\nLearning rate\nL1 regularization\nTrain F1-score\nValidation F1-score\nTrain accuracy\nValidation accuracy\nTrain recall\nValidation recall\nTrain precision\nValidation precision\n\n\n\n\nTFIDF NN model 42\n50\n0.01\ntanh\n0.01\n0.01\n0.849\n0.637\n0.851\n0.650\n0.806\n0.596\n0.897\n0.686\n\n\nTFIDF NN model 5\n50\n0.01\nrelu\n0.01\n0.01\n0.848\n0.637\n0.853\n0.646\n0.802\n0.600\n0.900\n0.680\n\n\nTFIDF NN model 41\n50\n0.01\nrelu\n0.01\n0.01\n0.848\n0.632\n0.849\n0.654\n0.805\n0.581\n0.898\n0.704\n\n\nTFIDF NN model 6\n50\n0.01\ntanh\n0.01\n0.01\n0.846\n0.639\n0.850\n0.646\n0.804\n0.600\n0.894\n0.686\n\n\nTFIDF NN model 44\n50\n0.10\ntanh\n0.01\n0.01\n0.846\n0.635\n0.855\n0.673\n0.803\n0.596\n0.896\n0.680"
  },
  {
    "objectID": "STA5073Z_Assignment1.html#test-set-model-performance",
    "href": "STA5073Z_Assignment1.html#test-set-model-performance",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "6.4 Test Set Model Performance",
    "text": "6.4 Test Set Model Performance\nIn Table 11 below we see the model performance on the test set. The neural net model 42, using the TF-IDF fetaures outperformed all other models. Figure 10 in Addendum A shows the confusion matrix of TF-IDF NN model 42.\n\n\n\n\nTable 11: Model performance on the test set when training the best performing models on the full training set\n\n\n\n\n\n\n\n\n\n\nModel ID\nParameters\nTest Accuracy\nTest-F1\nTest Precision\nTest Recall\n\n\n\n\nBoW tree model 1\ncp = 0.001, minbucket = 1, minsplit = 1\n0.500\n0.495\n0.664\n0.500\n\n\nBoW RF model 2\nntree=500\n0.602\n0.613\n0.686\n0.602\n\n\nBoW XGB model 47\nmaxdepth = 6, eta = 1, gamma = 0.5\n0.370\n0.378\n0.596\n0.370\n\n\nBoW NB model 1\nLaplace0\n0.250\n0.100\n0.062\n0.250\n\n\nBoW NN model 55\nInput nodes = 50, activation function = 0.01, Learning rate = relu, L1 regularization = 0.001\n0.593\n0.629\n0.709\n0.565\n\n\nTFIDF tree model 1\ncp = 0.001, minbucket = 1, minsplit = 1\n0.463\n0.460\n0.615\n0.463\n\n\nTFIDF RF model 2\nntree=500\n0.620\n0.628\n0.696\n0.620\n\n\nTFIDF XGB model 39\nmaxdepth = 5, eta = 1, gamma = 0.5\n0.417\n0.422\n0.625\n0.417\n\n\nTFIDF NB model 1\nLaplace=0\n0.250\n0.100\n0.062\n0.250\n\n\nTFIDF NN model 42\nInput nodes = 50, Learning rate = 0.01, activation function = tanh, L1 regularization = 0.01\n0.639\n0.632\n0.653\n0.611"
  },
  {
    "objectID": "STA5073Z_Assignment1.html#training-and-validation-model-performance",
    "href": "STA5073Z_Assignment1.html#training-and-validation-model-performance",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "7.1 Training and Validation Model Performance",
    "text": "7.1 Training and Validation Model Performance\nFor the BoW Table 1 and TF-IDF Table 6 classification trees we see that the training performance is generally better than the validation performance, suggestive of model overfitting, where the model does not generalize well to the validation data. We see a similar trend for the XGBoost models ( BoW Table 3 and TF-IDF Table 8 ).\nWe see two interesting clusters in the tree diagrams in Figure 6 (BoW) and Figure 8 (TF-IDF) of Addendum A, which suggests that the unigram “health” is an important feature in the sentences of Ramaphosa (2018-present), which is in accordance with the Covid-19 pandemic. In addition, “peace” is an important feature in the sentences of Mandela (1994-1999), which is in accordance with the end of the Apartheid era (1994).\nFor the BoW (Table 2) and TF-IDF (Table 7) Random Forest models, we see a trend where the validation metrics are similar to the training metrics, where all metrics have a value above 0.6 suggesting good model fit. These models generalize well to the validation data.\nThe Naive Bayesian models (BoW, Table 4 and TF-IDF, Table 9 ) have poor model fit on the training data, and does not generalize well to the validation data.\nFor the BoW ( Table 5 and Figure 7 in Addendum A ) and TF-IDF (Table 10 and Figure 9 in Addendum A) Neural Network models, we see the log loss decreases to 0, as the number of epochs increase, whereas the training- recall, precision and accuracy values increase towards a maximum value. This suggests that the loss has been minimized and the performance, maximized."
  },
  {
    "objectID": "STA5073Z_Assignment1.html#test-set-model-performance-1",
    "href": "STA5073Z_Assignment1.html#test-set-model-performance-1",
    "title": "Data Science for Industry (STA5073Z)",
    "section": "7.2 Test Set Model Performance",
    "text": "7.2 Test Set Model Performance\nThe overall best model (Neural Net model 42, using the TF-IDF features) (Table 11), outperformed all other models with a test F1-score of 0.632. This model also had the best test set accuracy (0.639). However, the model’s test precision score of 0.653 felt short of the neural net model 55, using BoW features (0.709). In addition, NN model 42 had a test recall of 0.611, which felt short of the random forest model 2 using TF-IDF features, with a recall of 0.620.\nThe neural net models and the random forest models for both BoW and TF-IDF generalized well to the unseen, test data, with performance metrics above 0.6 in most cases. This suggests that in most cases these models can correctly predict the president who said a sentence.\nThe classification trees and XGBoost models had sub-optimal performance, which suggests that these models do not generalize as well to the unseen, test data. In addition, the Naïve Bayesian models have very poor model fit and do not generalize well to the unseen, test data. These sub-optimal and poorly fit models may be over-fitting the training data. As a result, the model does not “learn” but memorizes the characteristics of the training data.\nIn general, we also see a trend where in most cases the TF-IDF models outperform the BoW models, which may indicate that word importance is a better feature choice than word frequency."
  },
  {
    "objectID": "templates/templates.html",
    "href": "templates/templates.html",
    "title": "Short Paper",
    "section": "",
    "text": "Please make sure that your manuscript follows the guidelines in the Guide for Authors of the relevant journal. It is not necessary to typeset your manuscript in exactly the same way as an article, unless you are submitting to a camera-ready copy (CRC) journal.\nFor detailed instructions regarding the elsevier article class, see https://www.elsevier.com/authors/policies-and-guidelines/latex-instructions"
  },
  {
    "objectID": "templates/templates.html#using-csl",
    "href": "templates/templates.html#using-csl",
    "title": "Short Paper",
    "section": "Using CSL",
    "text": "Using CSL\nIf cite-method is set to citeproc in elsevier_article(), then pandoc is used for citations instead of natbib. In this case, the csl option is used to format the references. By default, this template will provide an appropriate style, but alternative csl files are available from https://www.zotero.org/styles?q=elsevier. These can be downloaded and stored locally, or the url can be used as in the example header."
  }
]